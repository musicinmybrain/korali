#include "problem/evaluation/directWithGradient/directWithGradient.hpp"
#include "conduit/conduit.hpp"
#include "auxiliar/math.hpp"

bool korali::problem::evaluation::DirectWithGradient::isSampleFeasible(korali::Sample& sample)
{
  for (size_t i = 0; i < sample["Parameters"].size(); i++)
  {
    double par = sample["Parameters"][i];
    if (std::isfinite(par) == false) return false;
    if (par < _k->_variables[i]->_lowerBound) return false;
    if (par > _k->_variables[i]->_upperBound) return false;
  }
  return true;
}

void korali::problem::evaluation::DirectWithGradient::initialize()
{
  if (_k->_variables.size() == 0) korali::logError("Direct with Gradient Evaluation problems require at least one variable.\n");
}

void korali::problem::evaluation::DirectWithGradient::basicEvaluation(korali::Sample& sample)
{
  sample.run(_objectiveFunction);
  int N = sample["Parameters"].size();

  double evaluation = sample["Evaluation"];
  std::vector<double> gradient(N,0.);
  for(size_t i=0; i<N; i++) gradient[i] = sample["Gradient"][i]["Value"];

  double evaluationSign = _objective == "Maximize" ? 1.0 : -1.0;

  // If result is not a finite number, objective function evaluates to -Infinity
  if(std::isnan(evaluation)) sample["Evaluation"] = -korali::Inf;
  // TODO check also for derivatives
  else{
    sample["Evaluation"] = evaluationSign * evaluation;
    for(size_t i=0; i<N; i++)
      sample["Gradient"][i]["Value"] = evaluationSign * gradient[i];
  }

}

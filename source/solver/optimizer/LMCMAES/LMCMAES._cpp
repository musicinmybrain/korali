#include "solver/optimizer/LMCMAES/LMCMAES.hpp"
#include "engine/sample/sample.hpp"
#include "conduit/conduit.hpp"

#include <stdio.h>
#include <unistd.h>
#include <chrono>
#include <numeric>   // std::iota
#include <algorithm> // std::sort

void korali::solver::optimizer::LMCMAES::initialize()
{
 korali::solver::Optimizer::initialize();

 // Initializing RNGs
 auto jsNormal = nlohmann::json();
 jsNormal["Name"] = "Normal Generator";
 jsNormal["Type"] = "Univariate/Normal";
 jsNormal["Mean"] = 0.0;
 jsNormal["Standard Deviation"] = 1.0;
 _normalGenerator = dynamic_cast<korali::distribution::univariate::Normal*>(korali::Module::getModule(jsNormal));
 _normalGenerator->initialize();

 auto jsUniform = nlohmann::json();
 jsUniform["Name"] = "Uniform Generator";
 jsUniform["Type"] = "Univariate/Uniform";
 jsUniform["Minimum"] = 0.0;
 jsUniform["Maximum"] = 1.0;
 _uniformGenerator = dynamic_cast<korali::distribution::univariate::Uniform*>(korali::Module::getModule(jsUniform));
 _uniformGenerator->initialize();

 _directProblem = dynamic_cast<korali::problem::evaluation::Direct*>(_k->_problem);
 bool isDirectProblem = _directProblem != NULL;

 _evaluationProblem = dynamic_cast<korali::problem::Evaluation*>(_k->_problem);
 bool isEvaluationProblem = _evaluationProblem != NULL;

 if (isEvaluationProblem == false)
  korali::logError("LMCMAES can only optimize problems of type 'Evaluation' or derived.\n");

 N = _k->_variables.size();

 if (_currentGeneration > 1) return;

 _chiSquareNumber = sqrt((double) N) * (1. - 1./(4.*N) + 1./(21.*N*N));

 // Allocating Memory
 _samplePopulation.resize(_populationSize);
 for(size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(N);

 _evolutionPath.resize(N);
 _conjugateEvolutionPath.resize(N);
 _auxiliarBDZMatrix.resize(N);
 _meanUpdate.resize(N);
 _currentMean.resize(N);
 _previousMean.resize(N);
 _bestEverVariables.resize(N);
 _axisLengths.resize(N);
 _auxiliarAxisLengths.resize(N);
 _currentBestVariables.resize(N);

 _sortingIndex.resize(_populationSize);
 _valueVector.resize(_populationSize);

 _covarianceMatrix.resize(N*N);
 _auxiliarCovarianceMatrix.resize(N*N);
 _covarianceEigenvectorMatrix.resize(N*N);
 _auxiliarCovarianceEigenvectorMatrix.resize(N*N);
 _bDZMatrix.resize(_populationSize*N);

 _muWeights.resize(_muValue);

 // Initializing variable defaults
  for (size_t i = 0; i < N; i++)
  {
    if(std::isfinite(_k->_variables[i]->_initialMean) == false)
    {
     if(std::isfinite(_k->_variables[i]->_lowerBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
     if(std::isfinite(_k->_variables[i]->_upperBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
     _k->_variables[i]->_initialMean = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound)*0.5;
    }

    if(std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
     if(std::isfinite(_k->_variables[i]->_lowerBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
     if(std::isfinite(_k->_variables[i]->_upperBound) == false) korali::logError("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
     _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound)*0.3;
    }

    if(_k->_variables[i]->_initialMean < _k->_variables[i]->_lowerBound || _k->_variables[i]->_initialMean > _k->_variables[i]->_upperBound)
    korali::logError("Initial Mean (%.4f) of variable \'%s\' is out of bounds (%.4f-%.4f).\n",
             _k->_variables[i]->_initialMean,
             _k->_variables[i]->_name.c_str(),
             _k->_variables[i]->_lowerBound,
             _k->_variables[i]->_upperBound);
  }

 _globalSuccessRate = -1.0;
 _covarianceMatrixAdaptionFactor = -1.0;
 _bestValidSample = 0;

 _covarianceMatrixAdaptationCount = 0;

 // Setting algorithm internal variables
 initMuWeights(_muValue);

 initCovariance();

 _infeasibleSampleCount = 0;

 _bestEverValue = -std::numeric_limits<double>::infinity();
 _conjugateEvolutionPathL2Norm = 0.0;

 for (size_t i = 0; i < N; i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialMean;

 _currentMinStandardDeviation = +std::numeric_limits<double>::infinity();
 _currentMaxStandardDeviation = -std::numeric_limits<double>::infinity();

}

void korali::solver::optimizer::LMCMAES::runGeneration()
{

 prepareGeneration();

 // Initializing Sample Evaluation
 std::vector<korali::Sample> samples(_populationSize);
 for (size_t i = 0; i < _populationSize; i++)
 {
  samples[i]["Operation"] = "Basic Evaluation";
  samples[i]["Parameters"] = _samplePopulation[i];
  samples[i]["Sample Id"] = i;
  _k->_conduit->start(samples[i]);
 }

 // Waiting for samples to finish
 _k->_conduit->waitAll(samples);

 // Processing results
 for (size_t i = 0; i < _populationSize; i++)
  _valueVector[i] = samples[i]["Evaluation"];

 updateDistribution();
}

void korali::solver::optimizer::LMCMAES::initMuWeights(size_t numsamplesmu)
{
 // Initializing Mu Weights
 if      (_muType == "Linear")       for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = numsamplesmu - i;
 else if (_muType == "Equal")        for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = 1.;
 else if (_muType == "Logarithmic")  for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = log(std::max( (double)numsamplesmu, 0.5*_populationSize)+0.5)-log(i+1.);
 else  korali::logError("Invalid setting of Mu Type (%s) (Linear, Equal, or Logarithmic accepted).",  _muType.c_str());

 // Normalize weights vector and set mueff
 double s1 = 0.0;
 double s2 = 0.0;

 for (size_t i = 0; i < numsamplesmu; i++)
 {
  s1 += _muWeights[i];
  s2 += _muWeights[i]*_muWeights[i];
 }
 _effectiveMu = s1*s1/s2;

 for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] /= s1;

 // Setting Cumulative Covariancea
 if( (_initialCumulativeCovariance <= 0) || (_initialCumulativeCovariance > 1) ) _cumulativeCovariance = (4.0 + _effectiveMu/(1.0*N)) / (N+4.0 + 2.0*_effectiveMu/(1.0*N));
 else _cumulativeCovariance = _initialCumulativeCovariance;

 // Setting Sigma Cumulation Factor
 _sigmaCumulationFactor = _initialSigmaCumulationFactor;
 if (_sigmaCumulationFactor <= 0 || _sigmaCumulationFactor >= 1) _sigmaCumulationFactor = (_effectiveMu + 2.0) / (N + _effectiveMu + 3.0);

 // Setting Damping Factor
 _dampFactor = _initialDampFactor;
 if (_dampFactor <= 0.0)
     _dampFactor = (1.0 + 2*std::max(0.0, sqrt((_effectiveMu-1.0)/(N+1.0)) - 1)) + _sigmaCumulationFactor;

}


void korali::solver::optimizer::LMCMAES::initCovariance()
{

 // Setting Sigma
 _trace = 0.0;
 for (size_t i = 0; i < N; ++i) _trace += _k->_variables[i]->_initialStandardDeviation*_k->_variables[i]->_initialStandardDeviation;
 _sigma = sqrt(_trace/N);

 // Setting B, C and _axisD
 for (size_t i = 0; i < N; ++i)
 {
  _covarianceEigenvectorMatrix[i*N+i] = 1.0;
  _covarianceMatrix[i*N+i] = _axisLengths[i] = _k->_variables[i]->_initialStandardDeviation * sqrt(N / _trace);
  _covarianceMatrix[i*N+i] *= _covarianceMatrix[i*N+i];
 }

 _minimumCovarianceEigenvalue = *std::min_element(std::begin(_axisLengths), std::end(_axisLengths));
 _maximumCovarianceEigenvalue = *std::max_element(std::begin(_axisLengths), std::end(_axisLengths));

 _minimumCovarianceEigenvalue = _minimumCovarianceEigenvalue * _minimumCovarianceEigenvalue;
 _maximumCovarianceEigenvalue = _maximumCovarianceEigenvalue * _maximumCovarianceEigenvalue;

 _maximumDiagonalCovarianceMatrixElement=_covarianceMatrix[0]; for(size_t i=1;i<N;++i) if(_maximumDiagonalCovarianceMatrixElement<_covarianceMatrix[i*N+i]) _maximumDiagonalCovarianceMatrixElement=_covarianceMatrix[i*N+i];
 _minimumDiagonalCovarianceMatrixElement=_covarianceMatrix[0]; for(size_t i=1;i<N;++i) if(_minimumDiagonalCovarianceMatrixElement>_covarianceMatrix[i*N+i]) _minimumDiagonalCovarianceMatrixElement=_covarianceMatrix[i*N+i];
}


void korali::solver::optimizer::LMCMAES::prepareGeneration()
{
 for (size_t d = 0; d < N; ++d) _auxiliarCovarianceMatrix = _covarianceMatrix;
 updateEigensystem(_auxiliarCovarianceMatrix);
 for (size_t i = 0; i < _populationSize; ++i)
 {
   size_t initial_infeasible = _infeasibleSampleCount;
   sampleSingle(i);

   korali::Sample sample;
   sample["Parameters"] = _samplePopulation[i];
   while(_evaluationProblem->isSampleFeasible(sample) == false )
   {
     _infeasibleSampleCount++;
     sampleSingle(i);
     sample["Parameters"] = _samplePopulation[i];
   }
 }
}

void korali::solver::optimizer::LMCMAES::sampleSingle(size_t sampleIdx)
{
  for (size_t d = 0; d < N; ++d)
  {
   double randomNumber = _normalGenerator->getRandomNumber();

   if (_isDiagonal)
   {
     _bDZMatrix[sampleIdx*N+d] = _axisLengths[d] * randomNumber;
     _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma * _bDZMatrix[sampleIdx*N+d];
   }
   else _auxiliarBDZMatrix[d] = _axisLengths[d] * randomNumber;
  }

  if (!_isDiagonal) for (size_t d = 0; d < N; ++d)
   {
    _bDZMatrix[sampleIdx*N+d] = 0.0;
    for (size_t e = 0; e < N; ++e) _bDZMatrix[sampleIdx*N+d] += _covarianceEigenvectorMatrix[d*N+e] * _auxiliarBDZMatrix[e];
    _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma * _bDZMatrix[sampleIdx*N+d];
  }

}


void korali::solver::optimizer::LMCMAES::updateDistribution()
{

 /* Generate _sortingIndex */
 sort_index(_valueVector, _sortingIndex, _populationSize);

 _bestValidSample = 0;

 /* update function value history */
 _previousBestValue = _currentBestValue;

 /* update current best */
 _currentBestValue = _valueVector[_bestValidSample];
 for (size_t d = 0; d < N; ++d) _currentBestVariables[d] = _samplePopulation[_bestValidSample][d];

 /* update xbestever */
 if ( _currentBestValue > _bestEverValue )
 {
  _previousBestEverValue = _bestEverValue;
  _bestEverValue = _currentBestValue;

  for (size_t d = 0; d < N; ++d)
   _bestEverVariables[d] = _currentBestVariables[d];

 }

 /* set weights */
 for (size_t d = 0; d < N; ++d) {
   _previousMean[d] = _currentMean[d];
   _currentMean[d] = 0.;
   for (size_t i = 0; i < _muValue; ++i)
     _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];

   _meanUpdate[d] = (_currentMean[d] - _previousMean[d])/_sigma;
 }

 /* calculate z := D^(-1) * B^(T) * _meanUpdate into _auxiliarBDZMatrix */
 for (size_t d = 0; d < N; ++d) {
  double sum = 0.0;
  if (_isDiagonal) sum = _meanUpdate[d];
  else for (size_t e = 0; e < N; ++e) sum += _covarianceEigenvectorMatrix[e*N+d] * _meanUpdate[e]; /* B^(T) * _meanUpdate ( iterating B[e][d] = B^(T) ) */

  _auxiliarBDZMatrix[d] = sum / _axisLengths[d]; /* D^(-1) * B^(T) * _meanUpdate */
 }

 _conjugateEvolutionPathL2Norm = 0.0;

 /* cumulation for _sigma (ps) using B*z */
 for (size_t d = 0; d < N; ++d) {
    double sum = 0.0;
    if (_isDiagonal) sum = _auxiliarBDZMatrix[d];
    else for (size_t e = 0; e < N; ++e) sum += _covarianceEigenvectorMatrix[d*N+e] * _auxiliarBDZMatrix[e];

    _conjugateEvolutionPath[d] = (1. - _sigmaCumulationFactor) * _conjugateEvolutionPath[d] + sqrt(_sigmaCumulationFactor * (2. - _sigmaCumulationFactor) * _effectiveMu) * sum;

    /* calculate norm(ps)^2 */
    _conjugateEvolutionPathL2Norm += _conjugateEvolutionPath[d] * _conjugateEvolutionPath[d];
 }

 int hsig = (1.4 + 2.0/(N+1) > sqrt(_conjugateEvolutionPathL2Norm) / sqrt(1. - pow(1.-_sigmaCumulationFactor, 2.0*(1.0+_currentGeneration))) / _chiSquareNumber);

 /* cumulation for covariance matrix (pc) using B*D*z~N(0,C) */
 for (size_t d = 0; d < N; ++d)
    _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + hsig * sqrt( _cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu ) * _meanUpdate[d];

 /* update covariance matrix  */
 adaptC(hsig);

 /* update sigma */
 updateSigma();

 /* numerical error management */
 numericalErrorTreatment();

 _currentMinStandardDeviation = std::numeric_limits<double>::infinity();
 _currentMaxStandardDeviation = -std::numeric_limits<double>::infinity();

 // Calculating current Minimum and Maximum STD Devs
 for(size_t i = 0; i <N; ++i )
 {
  _currentMinStandardDeviation = std::min(_currentMinStandardDeviation, _sigma * sqrt(_covarianceMatrix[i*N+i]));
  _currentMaxStandardDeviation = std::max(_currentMaxStandardDeviation, _sigma * sqrt(_covarianceMatrix[i*N+i]));
 }

}

void korali::solver::optimizer::LMCMAES::adaptC(int hsig)
{
  /* definitions for speeding up inner-most loop */
  //double ccov1  = std::min(_covarianceMatrixLearningRate * (1./_muCovariance) * (_isDiagonal ? (N+1.5) / 3. : 1.), 1.); (orig, alternative)
  //double ccovmu = std::min(_covarianceMatrixLearningRate * (1-1./_muCovariance) * (_isDiagonal ? (N+1.5) / 3. : 1.), 1.-ccov1); (orig, alternative)
  double ccov1  = 2.0 / (std::pow(N+1.3,2)+_effectiveMu);
  double ccovmu = std::min(1.0-ccov1,  2.0 * (_effectiveMu-2+1/_effectiveMu) / (std::pow(N+2.0,2)+_effectiveMu));
  double sigmasquare = _sigma * _sigma;

  /* update covariance matrix */
  for (size_t d = 0; d < N; ++d)
   for (size_t e = _isDiagonal ? d : 0; e <= d; ++e)
   {
     _covarianceMatrix[d*N+e] = (1 - ccov1 - ccovmu) * _covarianceMatrix[d*N+e]
                        + ccov1 * (_evolutionPath[d] * _evolutionPath[e]
                        + (1-hsig)*ccov1*_cumulativeCovariance*(2.-_cumulativeCovariance) * _covarianceMatrix[d*N+e]);

     for (size_t k = 0; k < _muValue; ++k)
         _covarianceMatrix[d*N+e] += ccovmu * _muWeights[k] * (_samplePopulation[_sortingIndex[k]][d]
                            - _previousMean[d]) * (_samplePopulation[_sortingIndex[k]][e] - _previousMean[e]) / sigmasquare;

     if (e < d) _covarianceMatrix[e*N+d] = _covarianceMatrix[d*N+e];
   }

  /* update maximal and minimal diagonal value */
  _maximumDiagonalCovarianceMatrixElement = _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[0];
  for (size_t d = 1; d < N; ++d) {
  if (_maximumDiagonalCovarianceMatrixElement < _covarianceMatrix[d*N+d]) _maximumDiagonalCovarianceMatrixElement = _covarianceMatrix[d*N+d];
  else if (_minimumDiagonalCovarianceMatrixElement > _covarianceMatrix[d*N+d])  _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[d*N+d];
  }
}


void korali::solver::optimizer::LMCMAES::updateSigma()
{
 // _sigma *= exp(min(1.0, _sigmaCumulationFactor/_dampFactor*((sqrt(_conjugateEvolutionPathL2Norm)/_chiSquareNumber)-1.))); (alternative)
 _sigma *= exp(_sigmaCumulationFactor/_dampFactor*(sqrt(_conjugateEvolutionPathL2Norm)/_chiSquareNumber-1.));

 /* escape flat evaluation */
 if (_currentBestValue == _valueVector[_sortingIndex[(int)_muValue]]) {
   _sigma *= exp(0.2+_sigmaCumulationFactor/_dampFactor);
   korali::logWarning("Detailed", "Sigma increased due to equal function values.\n");
 }

 /* upper bound check for _sigma */
 double _upperBound = sqrt(_trace/N);

 if(_sigma > _upperBound)
 {
  korali::logInfo("Detailed", "Sigma exceeding inital value of _sigma (%f > %f), increase Initial Standard Deviation of variables.\n", _sigma, _upperBound);
  if( _isSigmaBounded )
  {
    _sigma = _upperBound;
    korali::logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
  }
 }

}


void korali::solver::optimizer::LMCMAES::numericalErrorTreatment()
{
 //treat maximal standard deviations
 //TODO

 //treat minimal standard deviations
 for (size_t d = 0; d < N; ++d) if (_sigma * sqrt(_covarianceMatrix[d*N+d]) < _k->_variables[d]->_minimumStandardDeviationUpdate)
 {
   _sigma = (_k->_variables[d]->_minimumStandardDeviationUpdate)/sqrt(_covarianceMatrix[d*N+d]) * exp(0.05+_sigmaCumulationFactor/_dampFactor);
   korali::logWarning("Detailed", "Sigma increased due to minimal standard deviation.\n");
 }

 //too low coordinate axis deviations
 //TODO

 //treat numerical precision provblems
 //TODO
}


void korali::solver::optimizer::LMCMAES::updateEigensystem(std::vector<double>& M)
{
 eigen(N, M, _auxiliarAxisLengths, _auxiliarCovarianceEigenvectorMatrix);

 /* find largest and smallest eigenvalue, they are supposed to be sorted anyway */
 double minEWtmp = *std::min_element(std::begin(_auxiliarAxisLengths), std::end(_auxiliarAxisLengths));
 double maxEWtmp = *std::max_element(std::begin(_auxiliarAxisLengths), std::end(_auxiliarAxisLengths));

 if (minEWtmp <= 0.0)
 { korali::logWarning("Detailed", "Min Eigenvalue smaller or equal 0.0 (%+6.3e) after Eigen decomp (no update possible).\n", minEWtmp ); return; }

 for (size_t d = 0; d < N; ++d)
 {
     _auxiliarAxisLengths[d] = sqrt(_auxiliarAxisLengths[d]);
     if (std::isfinite(_auxiliarAxisLengths[d]) == false)
     {
       korali::logWarning("Detailed", "Could not calculate root of Eigenvalue (%+6.3e) after Eigen decomp (no update possible).\n", _auxiliarAxisLengths[d] );
       return;
     }
    for (size_t e = 0; e < N; ++e) if (std::isfinite(_covarianceEigenvectorMatrix[d*N+e]) == false)
    {
      korali::logWarning("Detailed", "Non finite value detected in B (no update possible).\n");
       return;
    }
 }

 /* write back */
 for (size_t d = 0; d < N; ++d) _axisLengths[d] = _auxiliarAxisLengths[d];
 _covarianceEigenvectorMatrix.assign(std::begin(_auxiliarCovarianceEigenvectorMatrix), std::end(_auxiliarCovarianceEigenvectorMatrix));

 _minimumCovarianceEigenvalue = minEWtmp;
 _maximumCovarianceEigenvalue = maxEWtmp;
}


/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/


void korali::solver::optimizer::LMCMAES::eigen(size_t size, std::vector<double>& M,  std::vector<double>& diag, std::vector<double>& Q) const
{
 std::vector<double> data(size * size);

 for (size_t i = 0; i <  size; i++)
 for (size_t j = 0; j <= i; j++)
 {
  data[i*size + j] = M[i*N+j];
  data[j*size + i] = M[i*N+j];
 }

 // GSL Workspace
 gsl_vector* gsl_eval = gsl_vector_alloc(N);
 gsl_matrix* gsl_evec = gsl_matrix_alloc(N, N);
 gsl_eigen_symmv_workspace* gsl_work =  gsl_eigen_symmv_alloc(N);

 gsl_matrix_view m = gsl_matrix_view_array (data.data(), size, size);

 gsl_eigen_symmv (&m.matrix, gsl_eval, gsl_evec, gsl_work);
 gsl_eigen_symmv_sort (gsl_eval, gsl_evec, GSL_EIGEN_SORT_ABS_ASC);

 for (size_t i = 0; i < size; i++)
 {
  gsl_vector_view gsl_evec_i = gsl_matrix_column (gsl_evec, i);
  for (size_t j = 0; j < size; j++) Q[j*N+i] = gsl_vector_get (&gsl_evec_i.vector, j);
 }

 for (size_t i = 0; i < size; i++) diag[i] = gsl_vector_get (gsl_eval, i);

 gsl_vector_free(gsl_eval);
 gsl_matrix_free(gsl_evec);
 gsl_eigen_symmv_free(gsl_work);
}


void korali::solver::optimizer::LMCMAES::sort_index(const std::vector<double>& vec, std::vector<size_t>& _sortingIndex, size_t n) const
{
  // initialize original _sortingIndex locations
  std::iota(std::begin(_sortingIndex), std::begin(_sortingIndex)+n, (size_t) 0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(_sortingIndex), std::begin(_sortingIndex)+n, [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; } );

}


void korali::solver::optimizer::LMCMAES::printGenerationBefore() { return; }

void korali::solver::optimizer::LMCMAES::printGenerationAfter()
{

 korali::logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
 korali::logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);
 korali::logInfo("Normal", "Diagonal Covariance:    Min = %+6.3e -  Max = %+6.3e\n", _minimumDiagonalCovarianceMatrixElement, _maximumDiagonalCovarianceMatrixElement);
 korali::logInfo("Normal", "Covariance Eigenvalues: Min = %+6.3e -  Max = %+6.3e\n", _minimumCovarianceEigenvalue, _maximumCovarianceEigenvalue);

  korali::logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < N; d++) korali::logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  korali::logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < N; d++)
  {
   for (size_t e = 0; e <= d; e++) korali::logData("Detailed", "   %+6.3e  ",_covarianceMatrix[d*N+e]);
   korali::logInfo("Detailed", "\n");
  }

  korali::logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void korali::solver::optimizer::LMCMAES::finalize()
{
  korali::logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  korali::logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < N; ++d) korali::logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  korali::logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

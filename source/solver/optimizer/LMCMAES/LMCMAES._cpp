#include "solver/optimizer/LMCMAES/LMCMAES.hpp"
#include "engine/sample/sample.hpp"
#include "conduit/conduit.hpp"

#include <stdio.h>
#include <unistd.h>
#include <chrono>
#include <numeric>   // std::iota
#include <algorithm> // std::sort

void korali::solver::optimizer::LMCMAES::initialize()
{
 korali::solver::Optimizer::initialize();

 // Initializing RNGs
 auto jsNormal = nlohmann::json();
 jsNormal["Name"] = "Normal Generator";
 jsNormal["Type"] = "Univariate/Normal";
 jsNormal["Mean"] = 0.0;
 jsNormal["Standard Deviation"] = 1.0;
 _normalGenerator = dynamic_cast<korali::distribution::univariate::Normal*>(korali::Module::getModule(jsNormal));
 _normalGenerator->initialize();

 auto jsUniform = nlohmann::json();
 jsUniform["Name"] = "Uniform Generator";
 jsUniform["Type"] = "Univariate/Uniform";
 jsUniform["Minimum"] = 0.0;
 jsUniform["Maximum"] = 1.0;
 _uniformGenerator = dynamic_cast<korali::distribution::univariate::Uniform*>(korali::Module::getModule(jsUniform));
 _uniformGenerator->initialize();

 _directProblem = dynamic_cast<korali::problem::evaluation::Direct*>(_k->_problem);
 bool isDirectProblem = _directProblem != NULL;

 _evaluationProblem = dynamic_cast<korali::problem::Evaluation*>(_k->_problem);
 bool isEvaluationProblem = _evaluationProblem != NULL;

 if (isEvaluationProblem == false)
  korali::logError("LMCMAES can only optimize problems of type 'Evaluation' or derived.\n");

 N = _k->_variables.size();

 if (_currentGeneration > 1) return;

 _sigmaExponentFactor = 0.0;

 // Allocating Memory
 _samplePopulation.resize(_populationSize);
 for(size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(N);

 _evolutionPath.resize(N);
 _meanUpdate.resize(N);
 _currentMean.resize(N);
 _previousMean.resize(N);
 _bestEverVariables.resize(N);
 _currentBestVariables.resize(N);

 _sortingIndex.resize(_populationSize);
 _previousSortingIndex.resize(_populationSize);
 _valueVector.resize(_populationSize);
 _previousValueVector.resize(_populationSize);

 _muWeights.resize(_muValue);

 _subsetHistory.resize(_subsetSize);
 _subsetUpdateTimes.resize(_subsetSize);
 _evolutionPathMatrix.resize(N*_subsetSize);
 _inverseVectors.resize(N*_subsetSize);
 _choleskyFactorVectorProduct.resize(N);

 // Initializing variable defaults
  for (size_t i = 0; i < N; i++)
  {
    if(std::isfinite(_k->_variables[i]->_initialMean) == false)
    {
     if(std::isfinite(_k->_variables[i]->_lowerBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
     if(std::isfinite(_k->_variables[i]->_upperBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
     _k->_variables[i]->_initialMean = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound)*0.5;
    }

    if(std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
     if(std::isfinite(_k->_variables[i]->_lowerBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
     if(std::isfinite(_k->_variables[i]->_upperBound) == false) korali::logError("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
     _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound)*0.3;
    }

    if(_k->_variables[i]->_initialMean < _k->_variables[i]->_lowerBound || _k->_variables[i]->_initialMean > _k->_variables[i]->_upperBound)
    korali::logError("Initial Mean (%.4f) of variable \'%s\' is out of bounds (%.4f-%.4f).\n",
             _k->_variables[i]->_initialMean,
             _k->_variables[i]->_name.c_str(),
             _k->_variables[i]->_lowerBound,
             _k->_variables[i]->_upperBound);
  }

 // Setting algorithm internal variables
 initMuWeights(_muValue);

 _infeasibleSampleCount = 0;

 _bestEverValue = -std::numeric_limits<double>::infinity();

 for (size_t i = 0; i < N; i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialMean;

}

void korali::solver::optimizer::LMCMAES::runGeneration()
{

 prepareGeneration();

 // Initializing Sample Evaluation
 std::vector<korali::Sample> samples(_populationSize);
 for (size_t i = 0; i < _populationSize; i++)
 {
  samples[i]["Operation"] = "Basic Evaluation";
  samples[i]["Parameters"] = _samplePopulation[i];
  samples[i]["Sample Id"] = i;
  _k->_conduit->start(samples[i]);
 }

 // Waiting for samples to finish
 _k->_conduit->waitAll(samples);

 // Processing results
 for (size_t i = 0; i < _populationSize; i++)
  _valueVector[i] = samples[i]["Evaluation"];

 updateDistribution();
}

void korali::solver::optimizer::LMCMAES::initMuWeights(size_t numsamplesmu)
{
 // Initializing Mu Weights
 if      (_muType == "Linear")       for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = numsamplesmu - i;
 else if (_muType == "Equal")        for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = 1.;
 else if (_muType == "Logarithmic")  for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = log(std::max( (double)numsamplesmu, 0.5*_populationSize)+0.5)-log(i+1.);
 else  korali::logError("Invalid setting of Mu Type (%s) (Linear, Equal, or Logarithmic accepted).",  _muType.c_str());

 // Normalize weights vector and set mueff
 double s1 = 0.0;
 double s2 = 0.0;

 for (size_t i = 0; i < numsamplesmu; i++)
 {
  s1 += _muWeights[i];
  s2 += _muWeights[i]*_muWeights[i];
 }
 _effectiveMu = s1*s1/s2;

 for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] /= s1;

 if (_cumulativeCovariance <= 0) korali::logError("Invalid Initial Cumulative Covariance (must be greater 0.0");
 if (_sigmaCumulationFactor <= 0) korali::logError("Invalid Sigma Cumulative Covariance (must be greater 0.0");
 if (_dampFactor <= 0) korali::logError("Invalid Damp Factor (must be greater 0.0");

}


void korali::solver::optimizer::LMCMAES::prepareGeneration()
{
 for (size_t i = 0; i < _populationSize; ++i)
 {
   size_t initial_infeasible = _infeasibleSampleCount;
   sampleSingle(i);

   korali::Sample sample;
   sample["Parameters"] = _samplePopulation[i];
   while(_evaluationProblem->isSampleFeasible(sample) == false )
   {
     _infeasibleSampleCount++;
     sampleSingle(i);
     sample["Parameters"] = _samplePopulation[i];
   }
 }
}

void korali::solver::optimizer::LMCMAES::sampleSingle(size_t sampleIdx)
{
  if (sampleIdx % 2 == 1)
  {
    for (size_t d = 0; d < N; ++d) _randomVector[d] = (_uniformGenerator->getRandomNumber() > 0.5) ? 1 : -1.0;
    selectSubset(sampleIdx);
    choleskyFactorUpdate();
    for (size_t d = 0; d < N; ++d)
     _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma*_choleskyFactorVectorProduct[d];
  }
  else
  {
    for (size_t d = 0; d < N; ++d)
     _samplePopulation[sampleIdx][d] = 2.0*_currentMean[d] - _samplePopulation[sampleIdx-1][d];
  }

}


void korali::solver::optimizer::LMCMAES::selectSubset(size_t sampleIdx)
{
  double ms = 4.0;
  if (sampleIdx = 0) ms *= 10;
  
  _subsetStartIndex = _subsetSize - std::min( (size_t) std::floor(ms*std::abs(_normalGenerator->getRandomNumber())), _subsetSize);
}


void korali::solver::optimizer::LMCMAES::updateSet()
{
  size_t t = std::floor(_currentGeneration/_setUpdateInterval);
  if (t < _subsetSize)
  {
   _subsetHistory[t] = t;
  }
  else
  {
    int tmparg, imin;
    int minarg = std::numeric_limits<int>::max();
    for(size_t i = 1; i < _subsetSize; ++i) 
    { 
      tmparg = _subsetUpdateTimes[_subsetHistory[i]] - _subsetUpdateTimes[_subsetHistory[i-1]] - _targetUpdateDistance;
      if(tmparg < minarg) { minarg = tmparg; imin = i; }
    }
    if (tmparg > 0) imin = 0;
    size_t jtmp = _subsetHistory[imin];
    for(size_t i = imin; i < _subsetSize-1; ++i) _subsetHistory[i] = _subsetHistory[i+1];
    _subsetHistory[_subsetSize-1] = jtmp;
  }
  
  size_t jcur = _subsetHistory[std::min(t,_subsetSize-1)];
  _subsetUpdateTimes[jcur] = t * _setUpdateInterval;

  std::copy(_evolutionPath.begin(), _evolutionPath.end(), _evolutionPathMatrix.begin()+jcur);
}


void korali::solver::optimizer::LMCMAES::updateDistribution()
{
 /* Copy to previous */
 std::copy(_sortingIndex.begin(), _sortingIndex.end(), _previousSortingIndex.begin());
 std::copy(_valueVector.begin(), _valueVector.end(), _previousValueVector.begin());
 
 /* Generate _sortingIndex */
 sort_index(_valueVector, _sortingIndex, _populationSize);

 /* update function value history */
 _previousBestValue = _currentBestValue;

 /* update current best */
 _currentBestValue = _valueVector[0];
 for (size_t d = 0; d < N; ++d) _currentBestVariables[d] = _samplePopulation[0][d];

 /* update xbestever */
 if ( _currentBestValue > _bestEverValue )
 {
  _previousBestEverValue = _bestEverValue;
  _bestEverValue = _currentBestValue;

  for (size_t d = 0; d < N; ++d)
   _bestEverVariables[d] = _currentBestVariables[d];

 }

 /* set weights */
 for (size_t d = 0; d < N; ++d) {
   _previousMean[d] = _currentMean[d];
   _currentMean[d] = 0.;
   for (size_t i = 0; i < _muValue; ++i)
     _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];

   _meanUpdate[d] = (_currentMean[d] - _previousMean[d])/_sigma;
 }

 for (size_t d = 0; d < N; ++d)
   _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + sqrt( _cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu ) * _meanUpdate[d];

 if (_currentGeneration % _setUpdateInterval == 0)
 {
    updateSet();
    updateInverseVectors();
 }

 /* update sigma */
 updateSigma();

 /* numerical error management */
 numericalErrorTreatment();

}

void korali::solver::optimizer::LMCMAES::choleskyFactorUpdate()
{
  double fac = std::sqrt(1.0-_choleskyMatrixLearningRate);
  
  for(size_t i = _subsetStartIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double v2L2 = 0.0;
    for(size_t d = 0; d < N; ++d) v2L2 += _inverseVectors[idx*N+d] * _inverseVectors[idx*N+d];
    double bjt = fac/v2L2 * (std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate)*v2L2) - 1.0);

    double k = 0.0;
    for(size_t d = 0; d < N; ++d) k += _inverseVectors[idx*N+d] * _randomVector[d];
    k *= bjt;
    
    for(size_t d = 0; d < N; ++d) _choleskyFactorVectorProduct[d] = fac * _choleskyFactorVectorProduct[d] + k * _subsetHistory[idx*d];
  }
}


void korali::solver::optimizer::LMCMAES::updateInverseVectors()
{
  double fac1 = 1.0/std::sqrt(1.0-_choleskyMatrixLearningRate);
  double fac2 = std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate));
  
  for(size_t i = _subsetStartIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double v2L2 = 0.0;
    for(size_t d = 0; d < N; ++d) v2L2 += _inverseVectors[idx*N+d] * _inverseVectors[idx*N+d];
    
    double djt = fac1/v2L2 * (1.0 - 1.0/(fac2*std::sqrt(v2L2)));
    
    double k = 0.0;
    for(size_t d = 0; d < N; ++d) k += _inverseVectors[idx*N+d] * _evolutionPathMatrix[idx*N+d];
    k *= djt;

    for(size_t d = 0; d < N; ++d) _inverseVectors[idx*N+d] = fac1 * _evolutionPathMatrix[idx*N+d] - k * _inverseVectors[idx*d];
  }
}


void korali::solver::optimizer::LMCMAES::updateSigma()
{

 double rank = 0;
 for(size_t i = 0; i < _populationSize-1; ++i)
 {
  if(_valueVector[_sortingIndex[i]] > _previousValueVector[_previousSortingIndex[i]])
  {
   rank++;
   for(size_t j = i; (j < _populationSize) && (_valueVector[_sortingIndex[j]] <= _previousValueVector[_previousSortingIndex[i]]); ++j) rank++;
  }
  else
  {
   rank--;
   for(size_t j = i; (j < _populationSize) && (_previousValueVector[_previousSortingIndex[j]] <= _valueVector[_sortingIndex[i]]); ++j) rank--;
  }
 }

 rank = rank/(_populationSize*_populationSize) - _targetSuccessRate;
 _sigmaExponentFactor = (1.0 - _sigmaCumulationFactor) * _sigmaExponentFactor + _sigmaCumulationFactor * rank;
 
 /* update sigma */
 _sigma *= std::exp(_sigmaExponentFactor/_dampFactor); 

 /* escape flat evaluation */
 if (_currentBestValue == _valueVector[_sortingIndex[(int)_muValue]]) {
   _sigma *= exp(0.2+_sigmaCumulationFactor/_dampFactor);
   korali::logWarning("Detailed", "Sigma increased due to equal function values.\n");
 }

 /* upper bound check for _sigma */
 double _upperBound = sqrt(_trace/N);

 if(_sigma > _upperBound)
 {
  korali::logInfo("Detailed", "Sigma exceeding inital value of _sigma (%f > %f), increase Initial Standard Deviation of variables.\n", _sigma, _upperBound);
  if( _isSigmaBounded )
  {
    _sigma = _upperBound;
    korali::logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
  }
 }

}


void korali::solver::optimizer::LMCMAES::numericalErrorTreatment()
{
 //treat numerical precision provblems
 //TODO
}


/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/


void korali::solver::optimizer::LMCMAES::sort_index(const std::vector<double>& vec, std::vector<size_t>& _sortingIndex, size_t n) const
{
  // initialize original _sortingIndex locations
  std::iota(std::begin(_sortingIndex), std::begin(_sortingIndex)+n, (size_t) 0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(_sortingIndex), std::begin(_sortingIndex)+n, [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; } );

}


void korali::solver::optimizer::LMCMAES::printGenerationBefore() { return; }

void korali::solver::optimizer::LMCMAES::printGenerationAfter()
{

 korali::logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
 korali::logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);

  korali::logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < N; d++) korali::logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  korali::logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < N; d++)
  {
   //for (size_t e = 0; e <= d; e++) korali::logData("Detailed", "   %+6.3e  ",_covarianceMatrix[d*N+e]);
   korali::logInfo("Detailed", "\n");
  }

  korali::logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void korali::solver::optimizer::LMCMAES::finalize()
{
  korali::logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  korali::logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < N; ++d) korali::logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  korali::logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

#include "solver/optimizer/LMCMAES/LMCMAES.hpp"
#include "engine/sample/sample.hpp"
#include "conduit/conduit.hpp"

#include <stdio.h>
#include <unistd.h>
#include <chrono>
#include <numeric>   // std::iota
#include <algorithm> // std::sort

void korali::solver::optimizer::LMCMAES::initialize()
{
 korali::solver::Optimizer::initialize();

 // Initializing RNGs
 auto jsNormal = nlohmann::json();
 jsNormal["Name"] = "Normal Generator";
 jsNormal["Type"] = "Univariate/Normal";
 jsNormal["Mean"] = 0.0;
 jsNormal["Standard Deviation"] = 1.0;
 _normalGenerator = dynamic_cast<korali::distribution::univariate::Normal*>(korali::Module::getModule(jsNormal));
 _normalGenerator->initialize();

 auto jsUniform = nlohmann::json();
 jsUniform["Name"] = "Uniform Generator";
 jsUniform["Type"] = "Univariate/Uniform";
 jsUniform["Minimum"] = 0.0;
 jsUniform["Maximum"] = 1.0;
 _uniformGenerator = dynamic_cast<korali::distribution::univariate::Uniform*>(korali::Module::getModule(jsUniform));
 _uniformGenerator->initialize();

 _directProblem       = dynamic_cast<korali::problem::evaluation::Direct*>(_k->_problem);
 bool isDirectProblem = _directProblem != NULL;

 _evaluationProblem       = dynamic_cast<korali::problem::Evaluation*>(_k->_problem);
 bool isEvaluationProblem = _evaluationProblem != NULL;

 if (isEvaluationProblem == false)
  korali::logError("LMCMAES can only optimize problems of type 'Evaluation' or derived.\n");

 N = _k->_variables.size();

 if(_targetDistanceCoefficients.size() != 3)
  korali::logError("LMCMAES requires 3 parameters for 'Target Distance Coefficients' (%zu provided).\n", _targetDistanceCoefficients.size());

 if (_currentGeneration > 1) return;

 _chiSquareNumber = sqrt((double) N) * (1. - 1./(4.*N) + 1./(21.*N*N));
 _sigmaExponentFactor = 0.0;
 _conjugateEvolutionPathL2Norm = 0.0;

 // Allocating Memory
 _samplePopulation.resize(_populationSize);
 for(size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(N);

 _evolutionPath.resize(N);
 _meanUpdate.resize(N);
 _currentMean.resize(N);
 _previousMean.resize(N);
 _bestEverVariables.resize(N);
 _currentBestVariables.resize(N);
 _randomVector.resize(N);
 _choleskyFactorVectorProduct.resize(N);
 _standardDeviation.resize(N);
 
 _muWeights.resize(_muValue);
 
 _sortingIndex.resize(_populationSize);
 _previousSortingIndex.resize(_populationSize);
 _valueVector.resize(_populationSize);
 _previousValueVector.resize(_populationSize);

 _evolutionPathWeights.resize(_subsetSize);
 _subsetHistory.resize(_subsetSize);
 _subsetUpdateTimes.resize(_subsetSize);
   
 std::fill(_evolutionPathWeights.begin(), _evolutionPathWeights.end(), 0.0);
 std::fill(_subsetHistory.begin(), _subsetHistory.end(), 0);
 std::fill(_subsetUpdateTimes.begin(), _subsetUpdateTimes.end(), 0);
 
 _inverseVectors.resize(_subsetSize);
 _evolutionPathHistory.resize(_subsetSize);
 for(size_t i = 0 ; i < _subsetSize; ++i)
 {
   _inverseVectors[i].resize(N);
   _evolutionPathHistory[i].resize(N);
   std::fill(_inverseVectors[i].begin(), _inverseVectors[i].end(), 0.0);
   std::fill(_evolutionPathHistory[i].begin(), _evolutionPathHistory[i].end(), 0.0);
 }

 if (isDirectProblem == false)
 {
  for (size_t i = 0; i < N; i++)
  {
    if( std::isfinite(_k->_variables[i]->_initialMean) == false )
      korali::logError("Initial Mean of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());
    if( std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
      korali::logError("Initial Standard Deviation of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());
  }
 }

 // Initializing variable defaults
 for (size_t i = 0; i < N; i++)
 {
   /* confirm lower & upper bound */
   if( (std::isfinite(_k->_variables[i]->_lowerBound) && std::isfinite(_k->_variables[i]->_upperBound) ) == false ) 
        korali::logError("Lower and/or upper bound of variable \'%s\' is not defined.\n", _k->_variables[i]->_name.c_str());
    
   /* init mean if not defined */
   if(std::isfinite(_k->_variables[i]->_initialMean) == false)
   {
    if(std::isfinite(_k->_variables[i]->_lowerBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
    if(std::isfinite(_k->_variables[i]->_upperBound) == false) korali::logError("Initial Mean of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
    _k->_variables[i]->_initialMean = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound)*0.5;
   }

   /* validate mean */
   if(isDirectProblem == true)
   if(_k->_variables[i]->_initialMean < _k->_variables[i]->_lowerBound || _k->_variables[i]->_initialMean > _k->_variables[i]->_upperBound)
   korali::logError("Initial Mean (%.4f) of variable \'%s\' is out of bounds (%.4f-%.4f).\n",
             _k->_variables[i]->_initialMean,
             _k->_variables[i]->_name.c_str(),
             _k->_variables[i]->_lowerBound,
             _k->_variables[i]->_upperBound);

   /* calculate stddevs */
   if( std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    _standardDeviation[i] = 0.3 * (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound);
   else
    _standardDeviation[i] = _k->_variables[i]->_initialStandardDeviation;

 }
 
 _sigma = _initialSigma;
 
 if ( (_sigmaUpdateRule == "CMAES" || _sigmaUpdateRule == "LMCMA") == false)
   korali::logError("Invalid setting of Sigma Update Rule (%s) (choose CMAES, or LMCMA).",  _sigmaUpdateRule.c_str());

 if      (_muType == "Linear")       for (size_t i = 0; i < _muValue; i++) _muWeights[i] = _muValue - i;
 else if (_muType == "Equal")        for (size_t i = 0; i < _muValue; i++) _muWeights[i] = 1.;
 else if (_muType == "Logarithmic")  for (size_t i = 0; i < _muValue; i++) _muWeights[i] = log(std::max( (double)_muValue, 0.5*_populationSize)+0.5)-log(i+1.);
 else  korali::logError("Invalid setting of Mu Type (%s) (Linear, Equal, and Logarithmic accepted).",  _muType.c_str());

 if ((_randomNumberDistribution != "Normal") && (_randomNumberDistribution != "Uniform")) korali::logError("Invalid setting of Random Number Distribution (%s) (Normal and Uniform accepted).",  _randomNumberDistribution.c_str());

 // Normalize weights vector and set mueff
 double s1 = 0.0;
 double s2 = 0.0;

 for (size_t i = 0; i < _muValue; i++)
 {
  s1 += _muWeights[i];
  s2 += _muWeights[i]*_muWeights[i];
 }
 _effectiveMu = s1*s1/s2;

 for (size_t i = 0; i < _muValue; i++) _muWeights[i] /= s1;

 if (_initialSigma <= 0) 
     korali::logError("Invalid Initial Sigma (must be greater 0.0, is %lf).", _initialSigma);
 if (_cumulativeCovariance <= 0) 
     korali::logError("Invalid Initial Cumulative Covariance (must be greater 0.0).");
 if (_sigmaCumulationFactor <= 0) 
     korali::logError("Invalid Sigma Cumulative Covariance (must be greater 0.0).");
 if (_dampFactor <= 0) 
     korali::logError("Invalid Damp Factor (must be greater 0.0).");
 if (_choleskyMatrixLearningRate <= 0 || _choleskyMatrixLearningRate > 1) 
     korali::logError("Invalid Cholesky Matrix Learning Rate (must be in (0, 1], is %lf).", _choleskyMatrixLearningRate);
 if (_setUpdateInterval <= 0 ) 
     korali::logError("Invalid Set Update Interval(must be greater 0, is %zu).", _setUpdateInterval);


 _infeasibleSampleCount   = 0;
 _bestEverValue           = -std::numeric_limits<double>::infinity();
 _sqrtInverseCholeskyRate = std::sqrt(1.0-_choleskyMatrixLearningRate);

 for (size_t i = 0; i < N; i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialMean;

}

void korali::solver::optimizer::LMCMAES::runGeneration()
{

 prepareGeneration();

 // Initializing Sample Evaluation
 std::vector<korali::Sample> samples(_populationSize);
 for (size_t i = 0; i < _populationSize; i++)
 {
  samples[i]["Operation"]  = "Basic Evaluation";
  samples[i]["Parameters"] = _samplePopulation[i];
  samples[i]["Sample Id"]  = i;
  _modelEvaluationCount++;
  _k->_conduit->start(samples[i]);
 }

 // Waiting for samples to finish
 _k->_conduit->waitAll(samples);

 // Processing results
 for (size_t i = 0; i < _populationSize; i++)
  _valueVector[i] = samples[i]["Evaluation"];

 updateDistribution();
}


void korali::solver::optimizer::LMCMAES::prepareGeneration()
{
 for (size_t i = 0; i < _populationSize; ++i)
 {
   size_t initialInfeasible = _infeasibleSampleCount;
   sampleSingle(i);

   korali::Sample sample;
   sample["Parameters"] = _samplePopulation[i];
   
   while(_evaluationProblem->isSampleFeasible(sample) == false )
   {
     _infeasibleSampleCount++;
     sampleSingle(i);
     sample["Parameters"] = _samplePopulation[i];
   }
 }
}

void korali::solver::optimizer::LMCMAES::sampleSingle(size_t sampleIdx)
{
  if ( _symmetricSampling || (sampleIdx % 2) == 0)
  {
    choleskyFactorUpdate(sampleIdx);
    for (size_t d = 0; d < N; ++d) {
     _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma*_standardDeviation[d]*_choleskyFactorVectorProduct[d];
    }
  }
  else
  {
    for (size_t d = 0; d < N; ++d)
     //_samplePopulation[sampleIdx][d] = 2.0*_currentMean[d] - _samplePopulation[sampleIdx-1][d]; // version from [Loshchilov2015]
     _samplePopulation[sampleIdx][d] = _currentMean[d] - _sigma*_standardDeviation[d]*_choleskyFactorVectorProduct[d];  // version from Loshcholov's code
  }

}


void korali::solver::optimizer::LMCMAES::updateDistribution()
{
 /* Copy to previous */
 std::copy(_sortingIndex.begin(), _sortingIndex.end(), _previousSortingIndex.begin());
 std::copy(_valueVector.begin(), _valueVector.end(), _previousValueVector.begin());
 
 /* Generate _sortingIndex */
 sort_index(_valueVector, _sortingIndex, _populationSize);

 /* update function value history */
 _previousBestValue = _currentBestValue;

 /* update current best */
 _currentBestValue = _valueVector[0];
 for (size_t d = 0; d < N; ++d) _currentBestVariables[d] = _samplePopulation[_sortingIndex[0]][d];

 /* update xbestever */
 if ( _currentBestValue > _bestEverValue )
 {
  _previousBestEverValue = _bestEverValue;
  _bestEverValue         = _currentBestValue;

  for (size_t d = 0; d < N; ++d) _bestEverVariables[d] = _currentBestVariables[d];

 }

 /* set weights */
 for (size_t d = 0; d < N; ++d) {
   _previousMean[d] = _currentMean[d];
   _currentMean[d]  = 0.;
   for (size_t i = 0; i < _muValue; ++i)
     _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];

   _meanUpdate[d] = (_currentMean[d] - _previousMean[d])/(_sigma*_standardDeviation[d]);
 }

 /* update evolution path */
 _conjugateEvolutionPathL2Norm = 0.0;
 for (size_t d = 0; d < N; ++d)
 {
   _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + sqrt( _cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu ) * _meanUpdate[d];
   _conjugateEvolutionPathL2Norm += std::pow(_evolutionPath[d],2);
 }
 _conjugateEvolutionPathL2Norm = std::sqrt(_conjugateEvolutionPathL2Norm);

 /* update stored paths */
 if ( (_currentGeneration-1) % _setUpdateInterval == 0)
 {
    updateSet();
    updateInverseVectors();
 }

 /* update sigma */
 updateSigma();

 /* numerical error management */
 numericalErrorTreatment();

}


void korali::solver::optimizer::LMCMAES::choleskyFactorUpdate(size_t sampleIdx)
{
 
  /* randomly select subsetStartIndex */
  double ms = 4.0;
  if (sampleIdx == 0) ms *= 10;
  size_t subsetStartIndex = _subsetSize - std::min( (size_t) std::floor(ms*std::abs(_normalGenerator->getRandomNumber()))+1, _subsetSize);

  //for (size_t d = 0; d < N; ++d) _randomVector[d] = (_uniformGenerator->getRandomNumber() > 0.5) ? 1 : -1.0; Rademacher
  if ( _randomNumberDistribution == "Normal") 
      for (size_t d = 0; d < N; ++d) _randomVector[d] = _normalGenerator->getRandomNumber();
  else if ( _randomNumberDistribution == "Uniform") 
      for (size_t d = 0; d < N; ++d) _randomVector[d] = 2*_uniformGenerator->getRandomNumber()-1.0;

  for (size_t d = 0; d < N; ++d) _choleskyFactorVectorProduct[d] = _randomVector[d];
  
  for(size_t i = subsetStartIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double k = 0.0;
    for(size_t d = 0; d < N; ++d) k += _inverseVectors[idx][d] * _randomVector[d]; 
    k *= _evolutionPathWeights[idx];
    
    for(size_t d = 0; d < N; ++d) _choleskyFactorVectorProduct[d] = _sqrtInverseCholeskyRate * _choleskyFactorVectorProduct[d] + k * _evolutionPathHistory[idx][d];
  }
}


void korali::solver::optimizer::LMCMAES::updateSet()
{
  size_t t = std::floor( double(_currentGeneration-1.0)/ double(_setUpdateInterval) );

  if (t < _subsetSize)
  {
    _replacementIndex     = t;
    _subsetHistory[t]     = t;
    _subsetUpdateTimes[t] = t * _setUpdateInterval + 1;
  }
  else
  {
    double tmparg, minarg, target;
    minarg = std::numeric_limits<double>::max();
    for(size_t i = 1; i < _subsetSize; ++i) 
    { 
      /* `target` by default equals  N */
      target = _targetDistanceCoefficients[0] + _targetDistanceCoefficients[1] * std::pow(double(i+1.)/double(_subsetSize), _targetDistanceCoefficients[2]);
      tmparg = _subsetUpdateTimes[_subsetHistory[i]] - _subsetUpdateTimes[_subsetHistory[i-1]] - target;
      if(tmparg < minarg) { minarg = tmparg; _replacementIndex = i; }
    }
    if (tmparg > 0) _replacementIndex = 0; /* if all evolution paths at a distance of `target` or larger, update oldest */
    size_t jtmp = _subsetHistory[_replacementIndex];
    for(size_t i = _replacementIndex; i < _subsetSize-1; ++i) _subsetHistory[i] = _subsetHistory[i+1];
    
    _subsetHistory[_subsetSize-1] = jtmp;
    _subsetUpdateTimes[jtmp]      = t * _setUpdateInterval + 1;
  }
  
  /* insert new evolution path */
  std::copy(_evolutionPath.begin(), _evolutionPath.end(), _evolutionPathHistory[_subsetHistory[_replacementIndex]].begin()); 
  
}


void korali::solver::optimizer::LMCMAES::updateInverseVectors()
{
  double djt, k;
  double fac = std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate));
  
  /* update all inverse vectors and evolution path weights onwards from replacement index */
  for(size_t i = _replacementIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];
    
    double v2L2 = 0.0;
    for(size_t d = 0; d < N; ++d) v2L2 += _inverseVectors[idx][d] * _inverseVectors[idx][d];
    
    k = 0.0;
    if (v2L2 > 0.0)
    {
      djt = _sqrtInverseCholeskyRate/v2L2 * (1.0 - 1.0/(fac*std::sqrt(v2L2)));
    
      k = 0.0;
      for(size_t d = 0; d < N; ++d) k += _inverseVectors[idx][d] * _evolutionPathHistory[idx][d];
      k *= djt;

      _evolutionPathWeights[idx] = _sqrtInverseCholeskyRate/v2L2 * (std::sqrt(1.0+_choleskyMatrixLearningRate/(1.0-_choleskyMatrixLearningRate)*v2L2) - 1.0);
 
    }
    
    for(size_t d = 0; d < N; ++d) 
      _inverseVectors[idx][d] = _sqrtInverseCholeskyRate * _evolutionPathHistory[idx][d] - k * _inverseVectors[idx][d];

 }

}


void korali::solver::optimizer::LMCMAES::updateSigma()
{

 /* update sigma */
 if(_sigmaUpdateRule == "LMCMA")
 {
 double rank = 0;
 for(size_t i = 0; i < _populationSize; ++i)
 {
  if(_valueVector[_sortingIndex[i]] > _previousValueVector[_previousSortingIndex[i]])
  {
   rank++;
   for(size_t j = i+1; (j < _populationSize) && (_valueVector[_sortingIndex[j]] > _previousValueVector[_previousSortingIndex[i]]); ++j) rank++;
  }
  else
  {
   rank--;
   for(size_t j = i+1; (j < _populationSize) && (_previousValueVector[_previousSortingIndex[j]] > _valueVector[_sortingIndex[i]]); ++j) rank--;
  }
 }

 rank = rank/(_populationSize*_populationSize) - _targetSuccessRate;
 _sigmaExponentFactor = (1.0 - _sigmaCumulationFactor) * _sigmaExponentFactor + _sigmaCumulationFactor * rank;
  
   _sigma *= std::exp(_sigmaExponentFactor/_dampFactor); 
 }
 else /* CMA-ES update rule */
 {
   _sigma *= exp(_sigmaCumulationFactor/_dampFactor*(_conjugateEvolutionPathL2Norm/_chiSquareNumber-1.));
 }

 /* escape flat evaluation */
 if (_currentBestValue == _valueVector[_sortingIndex[(int)_muValue]]) {
   _sigma *= exp(0.2+_sigmaCumulationFactor/_dampFactor);
   korali::logWarning("Detailed", "Sigma increased due to equal function values.\n");
 }

 /* upper bound check for _sigma */
 if(_sigma > 2.0*_initialSigma)
 {
  korali::logInfo("Detailed", "Sigma exceeding initial sigma by a factor of two (%f > %f), increase value of Initial Sigma.\n", _sigma, 2.0*_initialSigma);
  if( _isSigmaBounded )
  {
    _sigma = 2.0*_initialSigma;
    korali::logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
  }
 }

}


void korali::solver::optimizer::LMCMAES::numericalErrorTreatment()
{
 //treat numerical precision provblems
 //TODO
}


/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/


void korali::solver::optimizer::LMCMAES::sort_index(const std::vector<double>& vec, std::vector<size_t>& sortingIndex, size_t n) const
{
  // initialize original _sortingIndex locations
  std::iota(std::begin(sortingIndex), std::begin(sortingIndex)+n, (size_t) 0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(sortingIndex), std::begin(sortingIndex)+n, [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; } );

}


void korali::solver::optimizer::LMCMAES::printGenerationBefore() { return; }

void korali::solver::optimizer::LMCMAES::printGenerationAfter()
{

 korali::logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
 korali::logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);

  korali::logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < N; d++) korali::logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  korali::logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < N; d++)
  {
   //for (size_t e = 0; e <= d; e++) korali::logData("Detailed", "   %+6.3e  ",_covarianceMatrix[d*N+e]);
   korali::logInfo("Detailed", "\n");
  }

  korali::logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void korali::solver::optimizer::LMCMAES::finalize()
{
  korali::logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  korali::logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < N; ++d) korali::logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  korali::logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

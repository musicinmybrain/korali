#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

/********************************************
 * Pending Performance Improvements
 * - Make sure all operations are made on memory objects with format_tag::any, reordered by oneDNN during creation.
 *   This guarantees that the best memory landscape is selected for all these operations
 * - Do not reserve memory nor create/execute primitives for differentiation when just inferring
 * - Use prop_kind::forward_inference always when inferring
 * - Some memory structures can be re-used (instead of having a separate diff vector, reuse the data vector)
 ********************************************/

namespace korali
{
NeuralNetwork::NeuralNetwork()
{
  _isInitialized = false;
}

void NeuralNetwork::initialize()
{
#ifndef _KORALI_USE_ONEDNN

  KORALI_LOG_ERROR("OneDNN has not been properly installed to support NN operations.\n");

#endif

  if (_isInitialized)
    KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

#ifdef _KORALI_USE_ONEDNN

  // Initializing Engine and stream

  if (_engineKind == "CPU") _engine = engine(engine::kind::cpu, 0);
  if (_engineKind == "GPU") _engine = engine(engine::kind::gpu, 0);

  _stream = stream(_engine);

#endif

  // Initializing weight, bias, shift and scale values
  for (size_t i = 1; i < _layers.size(); i++)
  {
    // Setting value for this layer's xavier constant
    double xavierConstant = sqrt(6.0) / sqrt(_layers[i]->_nodeCount + _layers[i - 1]->_nodeCount);

    _layers[i]->_weights.resize(_layers[i]->_nodeCount);
    _layers[i]->_bias.resize(_layers[i]->_nodeCount);
    _layers[i]->_batchNormalizationShift.resize(_layers[i]->_nodeCount);
    _layers[i]->_batchNormalizationScale.resize(_layers[i]->_nodeCount);
    _layers[i]->_batchNormalizationMeans.resize(_layers[i]->_nodeCount);
    _layers[i]->_batchNormalizationVariances.resize(_layers[i]->_nodeCount);

    // Adding layer's weights
    for (size_t j = 0; j < _layers[i]->_nodeCount; j++)
    {
      _layers[i]->_weights[j].resize(_layers[i - 1]->_nodeCount);
      for (size_t k = 0; k < _layers[i - 1]->_nodeCount; k++)
        _layers[i]->_weights[j][k] = xavierConstant * _xavierGenerator->getRandomNumber();
    }

    for (size_t j = 0; j < _layers[i]->_nodeCount; j++)
    {
      _layers[i]->_bias[j] = xavierConstant * _xavierGenerator->getRandomNumber();
      _layers[i]->_batchNormalizationShift[j] = xavierConstant * _xavierGenerator->getRandomNumber();
      _layers[i]->_batchNormalizationMeans[j] = 0.0;
      _layers[i]->_batchNormalizationVariances[j] = 1.0;
    }
  }

  // Setting batch size to 1 as default.
  _batchSize = 1;
  _isInitialized = true;
}

void NeuralNetwork::createForwardPipeline(size_t batchSize)
{
  if (_isInitialized == false) KORALI_LOG_ERROR("Neural Network not initialized.\n");

  if (batchSize == 0) KORALI_LOG_ERROR("You should provide an input batch size larger than zero.\n");

  // Updating NN's batch size
  _batchSize = batchSize;

#ifdef _KORALI_USE_ONEDNN

  /****************************************************************************
    * Checking input/output configuration
    ****************************************************************************/

  // Obtaining NN dimensions
  size_t layerCount = _layers.size();
  memory::dim N = _batchSize;

  if (_layers[0]->_type != "Input") KORALI_LOG_ERROR("The first layer must be of an input type.\n");
  for (size_t i = 1; i < layerCount - 1; i++)
  {
    if (_layers[i]->_type == "Input") KORALI_LOG_ERROR("Hidden layers cannot be input type.\n");
    if (_layers[i]->_type == "Output") KORALI_LOG_ERROR("Hidden layers cannot be output type.\n");
  }
  if (_layers[layerCount - 1]->_type != "Output") KORALI_LOG_ERROR("The last layer must be of an output type.\n");

  // Checking Layer sizes
  for (size_t i = 0; i < layerCount; i++)
    if (_layers[i]->_nodeCount == 0)
      KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", i);

  /*********************************************************************************
  *  Initializing memory objects and primitives for FORWARD propagation
  *********************************************************************************/

  // Creating layer's data memory storage and activation function
  for (size_t i = 0; i < layerCount; i++)
  {
    const memory::dim OC = _layers[i]->_nodeCount;
    const memory::dims layerDims = {N, OC};
    auto dataMemDesc = memory::desc(layerDims, memory::data_type::f32, memory::format_tag::ab);
    _layers[i]->_nodeMem = memory(dataMemDesc, _engine);
    _layers[i]->_activationMem = memory(dataMemDesc, _engine);
    if (_batchNormalizationEnabled) _layers[i]->_batchNormalizationMem = memory(dataMemDesc, _engine);

    /*****************************************
    * Creating Activation Function primitive and memory
    * ***************************************/

    _layers[i]->_activationAlgorithm = algorithm::eltwise_linear;

    if (_layers[i]->_activationFunctionType == "Identity") _layers[i]->_activationFunctionAlpha = 1.0f;
    if (_layers[i]->_activationFunctionType == "Clip") _layers[i]->_activationAlgorithm = algorithm::eltwise_clip;
    if (_layers[i]->_activationFunctionType == "Linear") _layers[i]->_activationAlgorithm = algorithm::eltwise_linear;
    if (_layers[i]->_activationFunctionType == "Log") _layers[i]->_activationAlgorithm = algorithm::eltwise_log;
    if (_layers[i]->_activationFunctionType == "ReLU") _layers[i]->_activationAlgorithm = algorithm::eltwise_relu;
    if (_layers[i]->_activationFunctionType == "Tanh") _layers[i]->_activationAlgorithm = algorithm::eltwise_tanh;
    if (_layers[i]->_activationFunctionType == "Logistic") _layers[i]->_activationAlgorithm = algorithm::eltwise_logistic;

    float alpha = _layers[i]->_activationFunctionAlpha;
    float beta = _layers[i]->_activationFunctionBeta;

    // Creating descriptor
    auto activationDesc = eltwise_forward::desc(prop_kind::forward_training, _layers[i]->_activationAlgorithm, _layers[i]->_nodeMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    _layers[i]->_forwardActivationPrimitiveDesc = eltwise_forward::primitive_desc(activationDesc, _engine);

    // Create the primitive.
    _layers[i]->_forwardActivationPrimitive = eltwise_forward(_layers[i]->_forwardActivationPrimitiveDesc);

    // Primitive arguments.
    if (_batchNormalizationEnabled)
      _layers[i]->_forwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_batchNormalizationMem});
    else
      _layers[i]->_forwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_nodeMem});

    _layers[i]->_forwardActivationArgs.insert({DNNL_ARG_DST, _layers[i]->_activationMem});
  }

  // Initializing Wx+b Operation (also called inner product in oneDNN)
  for (size_t i = 1; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;

    memory::dims weightDims = {OC, IC};

    // Allocating weight memory
    auto weightMemDesc = memory::desc(weightDims, memory::data_type::f32, memory::format_tag::ba);
    _layers[i]->_weightsMem = memory(weightMemDesc, _engine);
    _layers[i]->_weightsDiffMem = memory(weightMemDesc, _engine);

    // Allocating bias memory
    auto biasMemDesc = memory::desc({OC}, memory::data_type::f32, memory::format_tag::a);
    _layers[i]->_biasMem = memory(biasMemDesc, _engine);
    _layers[i]->_biasDiffMem = memory(biasMemDesc, _engine);

    // Create memory descriptor for weights with format_tag::any. This enables
    // the inner product primitive to choose the memory layout for an optimized
    // primitive implementation, and this format may differ from the one
    // provided by the user.
    auto weightsWorkMemDesc = memory::desc(weightDims, memory::data_type::f32, memory::format_tag::any);

    // Create operation descriptor.
    auto inner_product_d = inner_product_forward::desc(prop_kind::forward_training, _layers[i - 1]->_activationMem.get_desc(), weightsWorkMemDesc, biasMemDesc, _layers[i]->_nodeMem.get_desc());

    // Create inner product primitive descriptor.
    dnnl::primitive_attr forwardPrimitiveAttributes;
    _layers[i]->_forwardInnerProductPrimitiveDesc = inner_product_forward::primitive_desc(inner_product_d, forwardPrimitiveAttributes, _engine);

    // For now, assume that the weights memory layout generated by the primitive
    // and the one provided by the user are identical.
    _layers[i]->_weightsWorkMem = _layers[i]->_weightsMem;

    // Reorder the data in case the weights memory layout generated by the
    // primitive and the one provided by the user are different. In this case,
    // we create additional memory objects with internal buffers that will
    // contain the reordered data.
    if (_layers[i]->_weightsWorkMem.get_desc() != _layers[i]->_forwardInnerProductPrimitiveDesc.weights_desc())
      _layers[i]->_weightsWorkMem = memory(_layers[i]->_forwardInnerProductPrimitiveDesc.weights_desc(), _engine);

    // Create the weights+bias primitive.
    _layers[i]->_forwardInnerProductPrimitive = inner_product_forward(_layers[i]->_forwardInnerProductPrimitiveDesc);

    // Configuring inner product arguments
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_SRC, _layers[i - 1]->_activationMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_WEIGHTS, _layers[i]->_weightsWorkMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_BIAS, _layers[i]->_biasMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_DST, _layers[i]->_nodeMem});

    // Initializing forward layer normalization operations
    if (_batchNormalizationEnabled)
    {
      memory::dims scale_shift_dims = {2, OC};

      // Allocating normalization scale/shift memory structure
      auto normalizationMemDesc = memory::desc(scale_shift_dims, memory::data_type::f32, memory::format_tag::nc);
      _layers[i]->_batchNormalizationScaleShiftMem = memory(normalizationMemDesc, _engine);

      auto normalizationFlags = normalization_flags::use_scale_shift;

      // If mean and variances are defined, then normalize with respect to them
      if (_batchNormalizationUseGlobalStats)
        normalizationFlags |= normalization_flags::use_global_stats;

      // Create operation descriptor.
      auto forwardNormalizationDesc = batch_normalization_forward::desc(prop_kind::forward_training, _layers[i]->_nodeMem.get_desc(), _batchNormalizationEpsilon, normalizationFlags);

      // Create inner product primitive descriptor.
      _layers[i]->_forwardNormalizationPrimitiveDesc = batch_normalization_forward::primitive_desc(forwardNormalizationDesc, _engine);

      // Creating mean and variance mem structures
      _layers[i]->_batchNormalizationMeanMem = memory(_layers[i]->_forwardNormalizationPrimitiveDesc.mean_desc(), _engine);
      _layers[i]->_batchNormalizationVarianceMem = memory(_layers[i]->_forwardNormalizationPrimitiveDesc.variance_desc(), _engine);
      _layers[i]->_batchNormalizationWorkMem = memory(_layers[i]->_forwardNormalizationPrimitiveDesc.workspace_desc(), _engine);

      // Create the normalization primitive.
      _layers[i]->_forwardNormalizationPrimitive = batch_normalization_forward(_layers[i]->_forwardNormalizationPrimitiveDesc);

      // Configuring inner product arguments
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_SRC, _layers[i]->_nodeMem});                                 // Input
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_SCALE_SHIFT, _layers[i]->_batchNormalizationScaleShiftMem}); // Input
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_MEAN, _layers[i]->_batchNormalizationMeanMem});              // Output
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_VARIANCE, _layers[i]->_batchNormalizationVarianceMem});      // Output
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_WORKSPACE, _layers[i]->_batchNormalizationWorkMem});         // Output
      _layers[i]->_forwardNormalizationArgs.insert({DNNL_ARG_DST, _layers[i]->_batchNormalizationMem});                   // Output
    }
  }

#endif
}

void NeuralNetwork::createBackwardPipeline()
{
  /*********************************************************************************
  *  Initializing memory objects and primitives for BACKWARD propagation
  *********************************************************************************/

  if (_isInitialized == false) KORALI_LOG_ERROR("Neural Network not initialized.\n");
  if (_batchSize == 0) KORALI_LOG_ERROR("You should provide an input batch size larger than zero. Have you forgotten to create the forward pipeline first?\n");

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  const memory::dim N = _batchSize;

  // Creating layer's data diff memory storage
  for (size_t i = 0; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i]->_nodeCount;
    const memory::dims layerDims = {N, IC};
    auto dataDiffMemDesc = memory::desc(layerDims, memory::data_type::f32, memory::format_tag::ab);
    _layers[i]->_nodeDiffMem = memory(dataDiffMemDesc, _engine);
    _layers[i]->_batchNormalizationDiffMem = memory(dataDiffMemDesc, _engine);
    _layers[i]->_activationDiffMem = memory(dataDiffMemDesc, _engine);
  }

  for (ssize_t i = layerCount - 1; i >= 0; i--)
  {
    const memory::dim IC = _layers[i]->_nodeCount;

    // Creating backward propagation primitives for activation functions
    float alpha = _layers[i]->_activationFunctionAlpha;
    float beta = _layers[i]->_activationFunctionBeta;

    // Creating descriptor
    auto activationDesc = eltwise_backward::desc(_layers[i]->_activationAlgorithm, _layers[i]->_nodeMem.get_desc(), _layers[i]->_activationMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    auto backwardActivationPrimitiveDesc = eltwise_backward::primitive_desc(activationDesc, _engine, _layers[i]->_forwardActivationPrimitiveDesc);

    // Create the primitive.
    _layers[i]->_backwardActivationPrimitive = eltwise_backward(backwardActivationPrimitiveDesc);

    // Primitive arguments.
    if (_batchNormalizationEnabled)
      _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_batchNormalizationMem}); // Input
    else
      _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_nodeMem}); // Input

    _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_activationDiffMem}); // Input

    if (_batchNormalizationEnabled)
      _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i]->_batchNormalizationDiffMem}); // Output
    else
      _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i]->_nodeDiffMem}); // Output

    // Do not defined the following primitives for the first layer
    if (i > 0)
    {
      auto backwardDataDesc = inner_product_backward_data::desc(
        _layers[i - 1]->_nodeDiffMem.get_desc(),
        _layers[i]->_weightsWorkMem.get_desc(),
        _layers[i]->_nodeMem.get_desc());

      // Create the primitive.
      auto backwardDataPrimitiveDesc = inner_product_backward_data::primitive_desc(backwardDataDesc, _engine, _layers[i]->_forwardInnerProductPrimitiveDesc);
      _layers[i]->_backwardDataPrimitive = inner_product_backward_data(backwardDataPrimitiveDesc);

      _layers[i]->_backwardDataArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_nodeDiffMem});           // Input
      _layers[i]->_backwardDataArgs.insert({DNNL_ARG_WEIGHTS, _layers[i]->_weightsWorkMem});         // Input
      _layers[i]->_backwardDataArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i - 1]->_activationDiffMem}); // Output

      auto backwardWeightsDesc = inner_product_backward_weights::desc(
        _layers[i - 1]->_nodeMem.get_desc(),
        _layers[i]->_weightsMem.get_desc(),
        _layers[i]->_biasMem.get_desc(),
        _layers[i]->_nodeDiffMem.get_desc());

      // Create the primitive.
      auto backwardWeightsPrimitiveDesc = inner_product_backward_weights::primitive_desc(backwardWeightsDesc, _engine, _layers[i]->_forwardInnerProductPrimitiveDesc);
      _layers[i]->_backwardWeightsPrimitive = inner_product_backward_weights(backwardWeightsPrimitiveDesc);

      _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_SRC, _layers[i - 1]->_activationMem});       // Input
      _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_nodeDiffMem});        // Input
      _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_WEIGHTS, _layers[i]->_weightsDiffMem}); // Output
      _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_BIAS, _layers[i]->_biasDiffMem});       // Output

      // Initializing backward layer normalization operations
      if (_batchNormalizationEnabled)
      {
        const memory::dims normalizationDims = {2, IC};
        auto dataDiffMemDesc = memory::desc(normalizationDims, memory::data_type::f32, memory::format_tag::ab);
        _layers[i]->_batchNormalizationScaleShiftDiffMem = memory(dataDiffMemDesc, _engine);

        auto backwardNormalizationDesc = batch_normalization_backward::desc(
          prop_kind::backward,
          _layers[i]->_nodeDiffMem.get_desc(),
          _layers[i]->_nodeMem.get_desc(),
          _batchNormalizationEpsilon,
          normalization_flags::use_scale_shift);

        // Create the primitive.
        auto normalizationPrimitiveDesc = batch_normalization_backward::primitive_desc(backwardNormalizationDesc, _engine, _layers[i]->_forwardNormalizationPrimitiveDesc);
        _layers[i]->_backwardNormalizationPrimitive = batch_normalization_backward(normalizationPrimitiveDesc);

        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_SRC, _layers[i]->_nodeMem});                                          // Input
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_batchNormalizationDiffMem});                   // Input
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_MEAN, _layers[i]->_batchNormalizationMeanMem});                       // Input
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_VARIANCE, _layers[i]->_batchNormalizationVarianceMem});               // Input
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_SCALE_SHIFT, _layers[i]->_batchNormalizationScaleShiftMem});          // Input
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i]->_nodeDiffMem});                                 // Output
        _layers[i]->_backwardNormalizationArgs.insert({DNNL_ARG_DIFF_SCALE_SHIFT, _layers[i]->_batchNormalizationScaleShiftDiffMem}); // Output
      }
    }
  }

#endif
}

void NeuralNetwork::setInput(const std::vector<std::vector<double>> &input)
{
  size_t inputSize = _layers[0]->_nodeCount;

  // Checking input
  if (input.size() != _batchSize)
    KORALI_LOG_ERROR("Input data set has a different batch size (%lu) than the one for which the NN was created for (%lu).\n", input.size(), _batchSize);

  for (size_t i = 0; i < input.size(); i++)
    if (input[i].size() != inputSize)
      KORALI_LOG_ERROR("Input data set %lu has a different number of elements (%lu) than the input layer node count (%lu).\n", i, input[i].size(), inputSize);

  // Copying input data to first layer
  std::vector<float> batchInput(_batchSize * inputSize);
  for (size_t i = 0; i < _batchSize; i++)
    for (size_t j = 0; j < inputSize; j++)
      batchInput[i * inputSize + j] = input[i][j];

  // If normalization is defined, the inputs are put into the normalization mem, otherwise node mem
  if (_batchNormalizationEnabled)
    write_to_dnnl_memory(batchInput.data(), _layers[0]->_batchNormalizationMem);
  else
    write_to_dnnl_memory(batchInput.data(), _layers[0]->_nodeMem);
}

void NeuralNetwork::updateWeightsBiasAndNormalization()
{
#ifdef _KORALI_USE_ONEDNN

  // Initialize solver's configuration here
  size_t layerCount = _layers.size();

  // Initializing weight matrix memories and values
  for (size_t i = 1; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;
    memory::dims weightDims = {OC, IC};
    size_t weightCount = product(weightDims);
    std::vector<float> weightsData(weightCount);
    std::vector<float> biasData(OC);

    if (_layers[i]->_weights.size() != (size_t)OC)
      KORALI_LOG_ERROR("Layer %lu weights were either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu weight sets. \n", i, OC, _layers[i]->_weights.size());

    for (size_t j = 0; j < (size_t)OC; j++)
      if (_layers[i]->_weights[j].size() != (size_t)IC)
        KORALI_LOG_ERROR("Layer %lu weight set %lu was either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu weight sets. \n", i, j, IC, _layers[i]->_weights[j].size());

    if (_layers[i]->_bias.size() != (size_t)OC)
      KORALI_LOG_ERROR("Layer %lu biases were either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu biases. \n", i, OC, _layers[i]->_bias.size());

    for (size_t j = 0; j < (size_t)OC; j++)
      for (size_t k = 0; k < (size_t)IC; k++)
        weightsData[j * IC + k] = _layers[i]->_weights[j][k];

    // If layer normalization is enabled, we don't need biases
    if (_batchNormalizationEnabled)
      for (size_t j = 0; j < (size_t)OC; j++) biasData[j] = 0.0;
    else
      for (size_t j = 0; j < (size_t)OC; j++) biasData[j] = _layers[i]->_bias[j];

    // Setting weight and bias data to oneDNN format
    write_to_dnnl_memory(weightsData.data(), _layers[i]->_weightsMem);
    write_to_dnnl_memory(biasData.data(), _layers[i]->_biasMem);

    // For now, assume that the weights memory layout generated by the primitive
    // and the one provided by the user are identical.
    reorder(_layers[i]->_weightsMem, _layers[i]->_weightsWorkMem).execute(_stream, _layers[i]->_weightsMem, _layers[i]->_weightsWorkMem);

    // If layer normalization is enabled, we fill in scale/shift data
    if (_batchNormalizationEnabled)
    {
      // Copying normalization scale/shift
      std::vector<float> normalizationScaleShiftData(2 * OC);

      if (_layers[i]->_batchNormalizationScale.size() != (size_t)OC)
        KORALI_LOG_ERROR("Layer %lu normalization scale values differ from the expected size. Expected: %lu, provided: %lu.\n", i, OC, _layers[i]->_batchNormalizationScale.size());

      if (_layers[i]->_batchNormalizationShift.size() != (size_t)OC)
        KORALI_LOG_ERROR("Layer %lu normalization shift values differ from the expected size. Expected: %lu, provided: %lu.\n", i, OC, _layers[i]->_batchNormalizationShift.size());

      for (size_t j = 0; j < (size_t)OC; j++) normalizationScaleShiftData[j] = _layers[i]->_batchNormalizationScale[j];
      for (size_t j = 0; j < (size_t)OC; j++) normalizationScaleShiftData[(size_t)OC + j] = _layers[i]->_batchNormalizationShift[j];

      write_to_dnnl_memory(normalizationScaleShiftData.data(), _layers[i]->_batchNormalizationScaleShiftMem);

      // Copying normalization means, if given
      if (_layers[i]->_batchNormalizationMeans.empty() == false)
      {
        std::vector<float> normalizationMeanData(OC);

        if (_layers[i]->_batchNormalizationMeans.size() != (size_t)OC)
          KORALI_LOG_ERROR("Layer %lu normalization mean values differ from the expected size. Expected: %lu, provided: %lu.\n", i, OC, _layers[i]->_batchNormalizationMeans.size());

        for (size_t j = 0; j < (size_t)OC; j++) normalizationMeanData[j] = _layers[i]->_batchNormalizationMeans[j];
        write_to_dnnl_memory(normalizationMeanData.data(), _layers[i]->_batchNormalizationMeanMem);
      }

      // Copying normalization variances, if given
      if (_layers[i]->_batchNormalizationMeans.empty() == false)
      {
        std::vector<float> normalizationVarianceData(OC);

        if (_layers[i]->_batchNormalizationVariances.size() != (size_t)OC)
          KORALI_LOG_ERROR("Layer %lu normalization variance values differ from the expected size. Expected: %lu, provided: %lu.\n", i, OC, _layers[i]->_batchNormalizationVariances.size());

        for (size_t j = 0; j < (size_t)OC; j++) normalizationVarianceData[j] = _layers[i]->_batchNormalizationVariances[j];
        write_to_dnnl_memory(normalizationVarianceData.data(), _layers[i]->_batchNormalizationVarianceMem);
      }

      if (_layers[i]->_batchNormalizationMeans.size() != _layers[i]->_batchNormalizationVariances.size()) KORALI_LOG_ERROR("Layer %lu inconsistent normalization variance and mean values: %lu != %lu.\n", i, _layers[i]->_batchNormalizationVariances.size(), _layers[i]->_batchNormalizationMeans.size());
    }
  }

#endif

  // Lifting flag to make sure we do not backpropagate without running forward first
  _hasPerformedForwardPropagation = false;
}

void NeuralNetwork::forward()
{
#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();

  // forward propagating neural network
  _layers[0]->_forwardActivationPrimitive.execute(_stream, _layers[0]->_forwardActivationArgs);
  for (size_t i = 1; i < layerCount; i++)
  {
    _layers[i]->_forwardInnerProductPrimitive.execute(_stream, _layers[i]->_forwardInnerProductArgs);
    if (_batchNormalizationEnabled) _layers[i]->_forwardNormalizationPrimitive.execute(_stream, _layers[i]->_forwardNormalizationArgs);
    _layers[i]->_forwardActivationPrimitive.execute(_stream, _layers[i]->_forwardActivationArgs);
  }

  // Wait for the computation to finalize.
  _stream.wait();

  // Restoring the output later node values
  size_t lastLayer = layerCount - 1;
  size_t nodeCount = _layers[lastLayer]->_nodeCount;
  std::vector<float> resultData(_batchSize * nodeCount);
  read_from_dnnl_memory(resultData.data(), _layers[lastLayer]->_activationMem);

  _outputValues.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++)
  {
    _outputValues[i].resize(nodeCount);
    for (size_t j = 0; j < nodeCount; j++)
      _outputValues[i][j] = resultData[i * nodeCount + j];
  }

  // Restoring batch normalization means and variances, if enabled
  if (_batchNormalizationEnabled)
    for (size_t i = 1; i < layerCount; i++)
    {
      size_t nodeCount = _layers[i]->_nodeCount;
      std::vector<float> meanData(nodeCount);
      std::vector<float> varianceData(nodeCount);

      read_from_dnnl_memory(meanData.data(), _layers[i]->_batchNormalizationMeanMem);
      read_from_dnnl_memory(varianceData.data(), _layers[i]->_batchNormalizationVarianceMem);

      _layers[i]->_batchNormalizationMeans.resize(nodeCount);
      _layers[i]->_batchNormalizationVariances.resize(nodeCount);

      for (size_t j = 0; j < nodeCount; j++) _layers[i]->_batchNormalizationMeans[j] = meanData[j];
      for (size_t j = 0; j < nodeCount; j++) _layers[i]->_batchNormalizationVariances[j] = varianceData[j];
    }

#endif

  // Setting fact that forward propagation has been performed.
  _hasPerformedForwardPropagation = true;
}

void NeuralNetwork::backwardWeightsAndBias(std::vector<float> &outputDifferential)
{
  if (_hasPerformedForwardPropagation == false)
    KORALI_LOG_ERROR("Neural network being backward propagated without doing a forward propagation first. \n");

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;

  // Writing to last layers differential information wrt data
  write_to_dnnl_memory(outputDifferential.data(), _layers[lastLayer]->_activationDiffMem);

  // Backward propagating neural network
  for (size_t i = lastLayer; i > 0; i--)
  {
    _layers[i]->_backwardActivationPrimitive.execute(_stream, _layers[i]->_backwardActivationArgs);
    if (_batchNormalizationEnabled) _layers[i]->_backwardNormalizationPrimitive.execute(_stream, _layers[i]->_backwardNormalizationArgs);
    _layers[i]->_backwardDataPrimitive.execute(_stream, _layers[i]->_backwardDataArgs);
    _layers[i]->_backwardWeightsPrimitive.execute(_stream, _layers[i]->_backwardWeightsArgs);
  }

  // Wait for the computation to finalize.
  _stream.wait();

  // Retrieving weight and bias gradients
  for (size_t i = 1; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;
    memory::dims weightDims = {OC, IC};
    size_t weightCount = product(weightDims);

    // Reading weight gradients
    std::vector<float> weightsDiffData(weightCount);
    read_from_dnnl_memory(weightsDiffData.data(), _layers[i]->_weightsDiffMem);

    // Resizing weight gradient destination buffer
    _layers[i]->_weightGradient.resize(OC);
    for (size_t j = 0; j < (size_t)OC; j++) _layers[i]->_weightGradient[j].resize(IC);

    // Copying weight gradient data to the buffer
    for (size_t j = 0; j < (size_t)OC; j++)
      for (size_t k = 0; k < (size_t)IC; k++)
        _layers[i]->_weightGradient[j][k] = weightsDiffData[j * IC + k];

    // If not using layer normalization, we get the bias gradients
    if (_batchNormalizationEnabled == false)
    {
      std::vector<float> biasDiffData(OC);
      read_from_dnnl_memory(biasDiffData.data(), _layers[i]->_biasDiffMem);

      _layers[i]->_biasGradient.resize(OC);
      for (size_t j = 0; j < (size_t)OC; j++)
        _layers[i]->_biasGradient[j] = biasDiffData[j];
    }

    // If using layer normalization, we get normalization gradients
    if (_batchNormalizationEnabled == true)
    {
      std::vector<float> normalizationDiffData(2 * OC);
      read_from_dnnl_memory(normalizationDiffData.data(), _layers[i]->_batchNormalizationScaleShiftDiffMem);

      _layers[i]->_batchNormalizationScaleGradient.resize(OC);
      _layers[i]->_batchNormalizationShiftGradient.resize(OC);

      for (size_t j = 0; j < (size_t)OC; j++)
        _layers[i]->_batchNormalizationScaleGradient[j] = normalizationDiffData[j];

      for (size_t j = 0; j < (size_t)OC; j++)
        _layers[i]->_batchNormalizationShiftGradient[j] = normalizationDiffData[OC + j];
    }
  }

#endif
}

void NeuralNetwork::backwardData(std::vector<float> &outputDifferential)
{
  if (_hasPerformedForwardPropagation == false)
    KORALI_LOG_ERROR("Neural network being backward propagated without doing a forward propagation first. \n");

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;

  auto outputBatchSize = _layers[lastLayer]->_nodeCount * _batchSize;
  if (outputDifferential.size() != outputBatchSize)
    KORALI_LOG_ERROR("Incorrect size of output differential provided. Given: %lu, Expected: %lu. \n", outputDifferential.size(), outputBatchSize);

  // Writing to last layers differential information wrt data
  write_to_dnnl_memory(outputDifferential.data(), _layers[lastLayer]->_activationDiffMem);

  // Backward propagating neural network
  for (size_t i = lastLayer; i > 0; i--)
  {
    _layers[i]->_backwardActivationPrimitive.execute(_stream, _layers[i]->_backwardActivationArgs);
    if (_batchNormalizationEnabled) _layers[i]->_backwardNormalizationPrimitive.execute(_stream, _layers[i]->_backwardNormalizationArgs);
    _layers[i]->_backwardDataPrimitive.execute(_stream, _layers[i]->_backwardDataArgs);
  }

  _layers[0]->_backwardActivationPrimitive.execute(_stream, _layers[0]->_backwardActivationArgs);

  // Wait for the computation to finalize.
  _stream.wait();

  // Retreiving gradient of the input
  size_t inputSize = _layers[0]->_nodeCount;
  std::vector<float> dataDiff(_batchSize * inputSize);

  read_from_dnnl_memory(dataDiff.data(), _layers[0]->_nodeDiffMem);

  _inputGradient.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++)
  {
    _inputGradient[i].resize(inputSize);
    for (size_t j = 0; j < inputSize; j++)
      _inputGradient[i][j] = dataDiff[i * inputSize + j];
  }

#endif
}

} // namespace korali

#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

namespace korali
{
NeuralNetwork::NeuralNetwork()
{
  _isInitialized = false;
  _batchSize = 0;
}

void NeuralNetwork::initialize()
{
  if (_engine == "OneDNN")
  {
#ifdef _KORALI_USE_ONEDNN

    _stream = dnnl::stream(korali::_engine);

#else

    _k->_logger->logWarning("Normal", "Neural Network's engine set to OneDNN, but Korali was installed without support for OneDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";

#endif
  }

  if (_engine == "CUDNN")
  {
#ifdef _KORALI_USE_ONEDNN

    if (cudnnCreate(&_cuDNNHandle) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error initializing CUDNN Handle\n");

#else

    _k->_logger->logWarning("Normal", "Neural Network's engine set to CUDNN, but Korali was installed without support for CUDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";
#endif
  }

  if (_isInitialized) KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

  if (_engine == "Korali")
  {
#ifdef _KORALI_USE_EIGEN

#else

    KORALI_LOG_ERROR("Neural Network's engine set to Korali, but Korali was installed without support for Eigen3.\n");

#endif
  }

  // Assigning relevant metadata to all the layers
  size_t layerCount = _layers.size();
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->_prevLayer = i > 0 ? _layers[i - 1] : NULL;
    _layers[i]->_nextLayer = i < layerCount - 1 ? _layers[i + 1] : NULL;
    _layers[i]->_index = i;
    _layers[i]->_nn = this;
  }

  // Creating forward and backward-propagation pipeline
  for (size_t i = 0; i < layerCount; i++) _layers[i]->createHyperparameterMemory();

  // Getting normalization and weight/bias parameter counts
  _hyperparameterCount = 0;
  for (size_t i = 0; i < layerCount; i++) _hyperparameterCount += _layers[i]->_hyperparameterCount;

  // Check output scaling configuration
  size_t outputSize = _layers[layerCount - 1]->_nodeCount;

  if (_outputScale.empty() == false)
    if (_outputScale.size() != outputSize)
      KORALI_LOG_ERROR("Wrong number of output scaling factors passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputScale.size());

  // Check output shifting configuration
  if (_outputShift.empty() == false)
    if (_outputShift.size() != outputSize)
      KORALI_LOG_ERROR("Wrong number of output shift factors passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputShift.size());

  // Check output absolute mask configuration
  if (_outputAbsoluteMask.empty() == false)
    if (_outputAbsoluteMask.size() != outputSize)
      KORALI_LOG_ERROR("Wrong size of output absolute mask passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputAbsoluteMask.size());

  // Check output tanh mask configuration
  if (_outputTanhMask.empty() == false)
    if (_outputTanhMask.size() != outputSize)
      KORALI_LOG_ERROR("Wrong size of output Tanh mask passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputTanhMask.size());

  // Check output SoftPlus mask configuration
  if (_outputSoftplusMask.empty() == false)
  {
    if (_outputSoftplusMask.size() != outputSize)
      KORALI_LOG_ERROR("Wrong size of output SoftPlus mask passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputSoftplusMask.size());
  }

  // Check output SoftPlus mask configuration
  if (_outputSigmoidMask.empty() == false)
  {
    if (_outputSigmoidMask.size() != outputSize)
      KORALI_LOG_ERROR("Wrong size of output Sigmoid mask passed to the neural network. Expected: %lu, provided: %lu.\n", outputSize, _outputSigmoidMask.size());
  }

  // Making sure we do not re-initialize
  _isInitialized = true;
}

std::vector<float> NeuralNetwork::generateInitialHyperparameters()
{
  // Setting initial values for hyperparameters
  std::vector<float> initialHyperparameters;

  for (size_t i = 0; i < _layers.size(); i++)
  {
    auto layerParameters = _layers[i]->generateInitialHyperparameters();
    // tmp fix (DW)
    if (i == _layers.size() - 1)
      for (size_t j = 0; j < layerParameters.size(); ++j)
        layerParameters[j] *= 0.1;
    initialHyperparameters.insert(initialHyperparameters.end(), layerParameters.begin(), layerParameters.end());
  }

  return initialHyperparameters;
}

void NeuralNetwork::setInput(const std::vector<std::vector<float>> &input)
{
  // Getting batch dimensions
  size_t batchSize = input.size();
  size_t inputSize = _layers[0]->_nodeCount;
  size_t layerCount = _layers.size();

  // If batchsize is different than existing one, re-create pipelines
  if (batchSize != _batchSize)
  {
    _batchSize = batchSize;
    for (size_t i = 0; i < layerCount; i++) _layers[i]->createForwardPipeline();
    for (size_t i = 0; i < layerCount; i++) _layers[i]->createBackwardPipeline();
  }

  for (size_t i = 0; i < input.size(); i++)
    if (input[i].size() != inputSize)
      KORALI_LOG_ERROR("Input data set %lu has a different number of elements (%lu) than the input layer node count (%lu).\n", i, input[i].size(), inputSize);

  // Copying input data to first layer
  std::vector<float> batchInput(_batchSize * inputSize);
  for (size_t i = 0; i < _batchSize; i++)
    for (size_t j = 0; j < inputSize; j++)
      batchInput[i * inputSize + j] = input[i][j];

  // Writing input to the NN
  _layers[0]->setInputValues(batchInput.data());
}

void NeuralNetwork::forward()
{
  size_t layerCount = _layers.size();

  // forward propagating neural network
  for (size_t i = 0; i < layerCount; i++)
  {
    // Applying forward Wx+b operation
    if (i > 0) _layers[i]->forwardWeightsAndBias();

    // Applying activation function
    _layers[i]->forwardActivationFunction();
  }

  // Restoring the output later node values
  size_t lastLayer = layerCount - 1;
  size_t nodeCount = _layers[lastLayer]->_nodeCount;
  std::vector<float> resultData(_batchSize * nodeCount);
  _layers[lastLayer]->getOutputValues(resultData.data());

  // Copying NN output to class output
  _outputValues.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++) _outputValues[i].resize(nodeCount);

  // Creating storage for output value sign, if using absolute mask
  if (_outputAbsoluteMask.size() > 0)
  {
    _outputSign.resize(_batchSize);
    for (size_t i = 0; i < _batchSize; i++) _outputSign[i].resize(nodeCount);
  }

  for (size_t i = 0; i < _batchSize; i++)
    for (size_t j = 0; j < nodeCount; j++)
    {
      auto x = resultData[i * nodeCount + j];

      // If we use a softplus mask, then apply the mask now
      if (_outputSigmoidMask.size() > 0)
        if (_outputSigmoidMask[j] == true)
          x = 1. / (1. + std::exp(-x));

      // If we use a softplus mask, then apply the mask now
      if (_outputSoftplusMask.size() > 0)
        if (_outputSoftplusMask[j] == true)
          x = 0.5 * (x + std::sqrt(1. + x * x));

      // If we use a tanh mask, then apply the mask now
      if (_outputTanhMask.size() > 0)
        if (_outputTanhMask[j] == true)
          x = std::tanh(x);

      // If we use an absolute mask, then apply the mask now
      if (_outputAbsoluteMask.size() > 0)
      {
        // Storing original sign bit
        _outputSign[i][j] = std::signbit(x);

        // Using an absolute function to remove sign from number
        if (_outputAbsoluteMask[j] == true)
          x = std::fabs(x);
      }

      // If we  use scaling, then apply the scaling factors now
      if (_outputScale.size() > 0) x *= _outputScale[j];

      // If we  use shifting, then apply the scaling factors now
      if (_outputShift.size() > 0) x += _outputShift[j];

      _outputValues[i][j] = x;
    }
}

void NeuralNetwork::backward(const std::vector<std::vector<float>> &gradients)
{
  // Getting batch dimensions
  size_t batchSize = gradients.size();

  // If gradients batch size is different than forwarding batch size, fail with error
  if (batchSize != _batchSize)
    KORALI_LOG_ERROR("Wrong batch size passed for backward propagation. Expected: %lu, provided: %lu.\n", _batchSize, batchSize);

  // Running backward propagation
  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;
  size_t outputSize = _layers[lastLayer]->_nodeCount;

  // Copying incoming gradients to a contiguous vector
  std::vector<float> floatDiffs(_batchSize * outputSize);
  for (size_t i = 0; i < _batchSize; i++)
    for (size_t j = 0; j < outputSize; j++)
    {
      // Getting forward propagation output and passed gradients
      float x = _outputValues[i][j];
      float g = gradients[i][j];

      // If we use shift, then apply the inverse shift to the forward output
      if (_outputShift.size() > 0)
        x = x - _outputShift[j];

      // If we use scaling, then apply the inverse scaling factors gradient now
      if (_outputScale.size() > 0)
      {
        x = x / _outputScale[j];
        g = g * _outputScale[j];
      }

      // If we use an absolute mask, then restore the corresponding sign now
      if (_outputAbsoluteMask.size() > 0)
        if (_outputAbsoluteMask[j] == true)
          if (_outputSign[i][j])
            g = -g;

      // If we use an tanh mask, then apply its derivative
      if (_outputTanhMask.size() > 0)
        if (_outputTanhMask[j] == true)
        {
          g = g * (1.0f - x * x); // x is still tanh(x)
        }

      // If we use a softplus  mask, then apply its derivative
      if (_outputSoftplusMask.size() > 0)
        if (_outputSoftplusMask[j] == true)
        {
          float nnx = x - 0.25 / x; // Approximation NN output
          g = g * 0.5 * (nnx / std::sqrt(nnx * nnx + 1) + 1);
        }

      // If we use a sigmoid mask, then apply its derivative
      if (_outputSigmoidMask.size() > 0)
        if (_outputSigmoidMask[j] == true)
        {
          g = g * x * (1. - x); // x is still sigmoid(x)
        }

      floatDiffs[i * outputSize + j] = g;
    }

  // Copying gradients to the output end of the NN
  _layers[lastLayer]->setOutputGradients(floatDiffs.data());

  // Backward propagating neural network
  for (ssize_t i = lastLayer; i >= 0; i--)
  {
    _layers[i]->backwardActivationFunction();

    if (i > 0)
    {
      // Running backward data propagation
      _layers[i]->backwardDataPropagation();

      // Running backward weight+bias propagation
      _layers[i]->backwardWeightsAndBias();
    }
  }
}

std::vector<float> NeuralNetwork::getHyperparameters()
{
  auto params = std::vector<float>(_hyperparameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->getHyperparameters(&params[currPos]);
    currPos += _layers[i]->_hyperparameterCount;
  }

  return params;
}

void NeuralNetwork::setHyperparameters(const std::vector<float> &hyperparameters)
{
  if (hyperparameters.size() != _hyperparameterCount)
    KORALI_LOG_ERROR("Wrong number of hyperparameters passed to the neural network. Expected: %lu, provided: %lu.\n", _hyperparameterCount, hyperparameters.size());

  auto params = std::vector<float>(hyperparameters.begin(), hyperparameters.end());

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->setHyperparameters(&params[currPos]);
    currPos += _layers[i]->_hyperparameterCount;
  }
}

std::vector<float> NeuralNetwork::getHyperparameterGradients()
{
  auto gradients = std::vector<float>(_hyperparameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->getHyperparameterGradients(&gradients[currPos]);
    currPos += _layers[i]->_hyperparameterCount;
  }

  return gradients;
}

std::vector<float> NeuralNetwork::getDataGradients()
{
  size_t inputSize = _layers[0]->_nodeCount;
  std::vector<float> dataDiff(_batchSize * inputSize);

  _layers[0]->getInputGradients(dataDiff.data());

  return dataDiff;
}

} // namespace korali

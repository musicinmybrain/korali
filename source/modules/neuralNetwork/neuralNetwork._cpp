#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

namespace korali
{
NeuralNetwork::NeuralNetwork()
{
  _isInitialized = false;
  _batchSize = 0;
  _timestepCount = 0;
}

void NeuralNetwork::initialize()
{
  if (_engine == "OneDNN")
  {
#ifdef _KORALI_USE_ONEDNN

    _stream = dnnl::stream(korali::_dnnlEngine);

#else

    fprintf(stderr, "[Korali] Warning: Neural Network's engine set to OneDNN, but Korali was installed without support for OneDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";

#endif
  }

  if (_engine == "CuDNN")
  {
#ifdef _KORALI_USE_CUDNN

    if (cudnnCreate(&_cuDNNHandle) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error initializing CUDNN Handle\n");

#else

    fprintf(stderr, "[Korali] Warning: Neural Network's engine set to OneDNN, but Korali was installed without support for OneDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";
#endif
  }

  if (_isInitialized) KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

  if (_engine == "Korali")
  {
#ifdef _KORALI_USE_EIGEN

#else

    KORALI_LOG_ERROR("Neural Network's engine set to Korali, but Korali was installed without support for Eigen3.\n");

#endif
  }

  // Assigning relevant metadata to all the layers
  size_t layerCount = _layers.size();
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->_prevLayer = i > 0 ? _layers[i - 1] : NULL;
    _layers[i]->_nextLayer = i < layerCount - 1 ? _layers[i + 1] : NULL;
    _layers[i]->_index = i;
    _layers[i]->_nn = this;
  }

  // Initialize layers
  for (size_t i = 0; i < layerCount; i++) _layers[i]->initialize();

  // Creating forward and backward-propagation pipeline
  for (size_t i = 0; i < layerCount; i++) _layers[i]->createHyperparameterMemory();

  // Getting normalization and weight/bias parameter counts
  _hyperparameterCount = 0;
  for (size_t i = 0; i < layerCount; i++) _hyperparameterCount += _layers[i]->_hyperparameterCount;

  // Getting batch dimensions
  size_t N = _batchSize;
  size_t IC = _layers[0]->_outputChannels;
  size_t OC = _layers[layerCount-1]->_outputChannels;

  // Create forward and backward pipelines
  for (size_t i = 0; i < layerCount; i++) _layers[i]->createForwardPipeline();
  for (size_t i = 0; i < layerCount; i++) _layers[i]->createBackwardPipeline();

  // Resizing output vector
  _outputValues.resize(_timestepCount);
  for (size_t t = 0; t < _timestepCount; t++) _outputValues[t].resize(_batchSize);
  for (size_t t = 0; t < _timestepCount; t++) for (size_t i = 0; i < _batchSize; i++) _outputValues[t][i].resize(OC);

  // Resizeing cumulative gradient vector
  _hyperparameterGradients.resize(_hyperparameterCount);

  // Resizing batch buffers
  _batchInputData.resize(N * IC);
  _batchInputGradients.resize(N * IC);
  _batchOutputData.resize(N * OC);
  _batchOutputGradients.resize(N * OC);
  _batchHyperparameterGradients.resize(_hyperparameterCount);

  // Making sure we do not re-initialize
  _isInitialized = true;
}

std::vector<float> NeuralNetwork::generateInitialHyperparameters()
{
  // Setting initial values for hyperparameters
  std::vector<float> initialHyperparameters;

  for (size_t i = 0; i < _layers.size(); i++)
  {
    auto layerParameters = _layers[i]->generateInitialHyperparameters();
    initialHyperparameters.insert(initialHyperparameters.end(), layerParameters.begin(), layerParameters.end());
  }

  return initialHyperparameters;
}

void NeuralNetwork::forward(const std::vector<std::vector<std::vector<float>>> &input)
{
  size_t N = _batchSize;
  size_t IC = _layers[0]->_outputChannels;
  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;
  size_t nodeCount = _layers[lastLayer]->_outputChannels;

  // For each timestep, process the corresponding mini-batch
  for (_currentTimestep = 0; _currentTimestep < _timestepCount; _currentTimestep++)
  {
    // Setting NN input data
    for (size_t i = 0; i < N; i++)
      for (size_t j = 0; j < IC; j++)
       _batchInputData[i * IC + j] = input[_currentTimestep][i][j];

    // forward propagating neural network
    for (size_t i = 0; i < layerCount; i++)  _layers[i]->forwardData();

    // Copying the output layer node batch values for the current timestep
    for (size_t i = 0; i < _batchSize; i++)
      for (size_t j = 0; j < nodeCount; j++)
        _outputValues[_currentTimestep][i][j] = _batchOutputData[i * nodeCount + j];
  }
}

void NeuralNetwork::backward(const std::vector<std::vector<std::vector<float>>> &gradients)
{
  // Getting batch dimensions
  size_t gradientTimesteps = gradients.size();
  size_t batchSize = gradients[0].size();
  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;
  size_t outputSize = _layers[lastLayer]->_outputChannels;

  // If timesteps count provided is different than forwarding timesteps, fail with error
  if (gradientTimesteps > _timestepCount) KORALI_LOG_ERROR("More timesteps (%lu) passed for backward propagation than forward timesteps (%lu)\n", gradientTimesteps, _timestepCount);

  // If gradients batch size is different than forwarding batch size, fail with error
  if (batchSize != _batchSize) KORALI_LOG_ERROR("Wrong batch size passed for backward propagation. Expected: %lu, provided: %lu.\n", _batchSize, batchSize);

  // Resetting cumulative hyperparameter gradients
  for (size_t i = 0; i < _hyperparameterCount; i++) _hyperparameterGradients[i] = 0.0f;

  // Backward propagating in time, process the corresponding mini-batch
  for (size_t t = 0; t < _timestepCount; t++)
  {
   // Starting from the last timestep, and going backwards
   _currentTimestep = _timestepCount - t - 1;

   // Setting batch output gradients on timesteps for which gradients have been provided, otherwise using zeros
   if (t < gradientTimesteps)
   {
    for (size_t i = 0; i < _batchSize; i++)
     for (size_t j = 0; j < outputSize; j++)
      _batchOutputGradients[i*outputSize + j] = gradients[gradientTimesteps - t - 1][i][j];
   }
   else
   {
    for (size_t i = 0; i < _batchSize; i++)
     for (size_t j = 0; j < outputSize; j++)
     _batchOutputGradients[i*outputSize + j] = 0.0f;
   }

    // Backward propagating in layer space
    for (size_t i = 0; i < layerCount; i++)
    {
     // Starting from the last layer, and going backwards
     size_t curLayer = lastLayer - i;

     // Running backward data propagation
     _layers[curLayer]->backwardData();

     // Calculating hyperparamter gradients
     _layers[curLayer]->backwardHyperparameters();
    }

    // Getting hyperparameter gradients for the current timestep
    size_t currPos = 0;
    for (size_t i = 0; i < layerCount; i++)
    {
      _layers[i]->getHyperparameterGradients(&_batchHyperparameterGradients[currPos]);
      currPos += _layers[i]->_hyperparameterCount;
    }

    // Adding current hyperparameters to the cumulative vector
    for (size_t i = 0; i < _hyperparameterCount; i++)
     _hyperparameterGradients[i] += _batchHyperparameterGradients[i];
  }
}

std::vector<float> NeuralNetwork::getHyperparameters()
{
  auto params = std::vector<float>(_hyperparameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->getHyperparameters(&params[currPos]);
    currPos += _layers[i]->_hyperparameterCount;
  }

  return params;
}

void NeuralNetwork::setHyperparameters(const std::vector<float> &hyperparameters)
{
  if (hyperparameters.size() != _hyperparameterCount)
    KORALI_LOG_ERROR("Wrong number of hyperparameters passed to the neural network. Expected: %lu, provided: %lu.\n", _hyperparameterCount, hyperparameters.size());

  auto params = std::vector<float>(hyperparameters.begin(), hyperparameters.end());

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->setHyperparameters(&params[currPos]);
    currPos += _layers[i]->_hyperparameterCount;
  }
}
} // namespace korali

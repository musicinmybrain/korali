#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

/********************************************
 * Pending Performance Improvements
 * - Make sure all operations are made on memory objects with format_tag::any, reordered by oneDNN during creation.
 *   This guarantees that the best memory landscape is selected for all these operations
 * - Do not reserve memory nor create/execute primitives for differentiation when just inferring
 * - Use prop_kind::forward_inference always when inferring
 * - Some memory structures can be re-used (instead of having a separate diff vector, reuse the data vector)
 ********************************************/

namespace korali
{

NeuralNetwork::NeuralNetwork()
{
 _isInitialized = false;
}

void NeuralNetwork::initialize()
{
#ifndef _KORALI_USE_ONEDNN

  KORALI_LOG_ERROR("OneDNN has not been properly installed to support NN operations.\n");

#endif

  if (_isInitialized)
   KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

  // Initializing Engine and stream

  if (_engineKind == "CPU") _engine = engine(engine::kind::cpu, 0);
  if (_engineKind == "GPU") _engine = engine(engine::kind::gpu, 0);

  _stream = stream(_engine);

  _batchSize = 0;
  _isInitialized = true;
}

void NeuralNetwork::createForwardPipeline(size_t batchSize)
{
  if (_isInitialized == false) KORALI_LOG_ERROR("Neural Network not initialized.\n");

  if (batchSize == 0) KORALI_LOG_ERROR("You should provide an input batch size larger than zero.\n");

  // Updating NN's batch size
  _batchSize = batchSize;

#ifdef _KORALI_USE_ONEDNN

  /****************************************************************************
    * Checking input/output configuration
    ****************************************************************************/

  // Obtaining NN dimensions
  size_t inputSize = _layers[0]->_nodeCount;
  size_t layerCount = _layers.size();
  memory::dim N = _batchSize;

  if (_layers[0]->_type != "Input") KORALI_LOG_ERROR("The first layer must be of an input type.\n");
  for (size_t i = 1; i < layerCount - 1; i++)
  {
    if (_layers[i]->_type == "Input") KORALI_LOG_ERROR("Hidden layers cannot be input type.\n");
    if (_layers[i]->_type == "Output") KORALI_LOG_ERROR("Hidden layers cannot be output type.\n");
  }
  if (_layers[layerCount - 1]->_type != "Output") KORALI_LOG_ERROR("The last layer must be of an output type.\n");

  // Checking Layer sizes
  for (size_t i = 0; i < layerCount; i++)
   if (_layers[i]->_nodeCount == 0)
     KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", i);

  /*********************************************************************************
  *  Initializing memory objects and primitives for FORWARD propagation
  *********************************************************************************/

  // Creating layer's data memory storage and activation function
  for (size_t i = 0; i < layerCount; i++)
  {
    const memory::dim OC = _layers[i]->_nodeCount;
    const memory::dims layerDims = {N, OC};
    auto dataMemDesc = memory::desc(layerDims, memory::data_type::f32, memory::format_tag::ab);
    _layers[i]->_dataMem = memory(dataMemDesc, _engine);
    _layers[i]->_activationMem = memory(dataMemDesc, _engine);

    /*****************************************
    * Creating Activation Function primitive and memory
    * ***************************************/

    algorithm activationAlgorithm = algorithm::eltwise_linear;
    float alpha = 0.0f;
    float beta = 0.0f;

    if (_layers[i]->_activationFunction == "Identity")
      alpha = 1.0f;

    if (_layers[i]->_activationFunction == "ReLU")
      activationAlgorithm = algorithm::eltwise_relu;

    if (_layers[i]->_activationFunction == "Tanh")
      activationAlgorithm = algorithm::eltwise_tanh;

    // Creating descriptor
    auto activationDesc = eltwise_forward::desc(prop_kind::forward_training, activationAlgorithm, _layers[i]->_dataMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    _layers[i]->_forwardActivationPrimitiveDesc = eltwise_forward::primitive_desc(activationDesc, _engine);

    // Create the primitive.
    _layers[i]->_forwardActivationPrimitive = eltwise_forward(_layers[i]->_forwardActivationPrimitiveDesc);

    // Primitive arguments.
    _layers[i]->_forwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_dataMem});
    _layers[i]->_forwardActivationArgs.insert({DNNL_ARG_DST, _layers[i]->_activationMem});
  }

  // Creating batch input normalization primitives

  if (_batchNormalizationEnabled == true)
  {
    const memory::dim IC = _layers[0]->_nodeCount;
    const memory::dims scaleShiftDims = {2, IC};

    auto normalizationFlags = dnnl::normalization_flags::none;

    // If not training, assign given variances and means
    if (_inputNormalizationMeans.size() > 0 || _inputNormalizationVariances.size() > 0)
    {
      if (_inputNormalizationMeans.size() != (size_t)IC)
        KORALI_LOG_ERROR("Number of values (%lu) in the batch normalization means vector is different from that of the node count of the first layer (%lu).\n", _inputNormalizationMeans.size(), IC);

      if (_inputNormalizationVariances.size() != (size_t)IC)
        KORALI_LOG_ERROR("Number of values (%lu) in the batch normalization variances vector is different from that of the node count of the first layer (%lu).\n", _inputNormalizationVariances.size(), IC);

      normalizationFlags |= dnnl::normalization_flags::use_global_stats;
    }

    if (_batchNormalizationScale.size() > 0 || _batchNormalizationShift.size() > 0)
    {
      if (_batchNormalizationScale.size() != inputSize)
        KORALI_LOG_ERROR("Number of values (%lu) in the batch normalization scale vector is different from that of the node count of the first layer (%lu).\n", _batchNormalizationScale.size(), inputSize);

      if (_batchNormalizationShift.size() != inputSize)
        KORALI_LOG_ERROR("Number of values (%lu) in the batch normalization shift vector is different from that of the node count of the first layer (%lu).\n", _batchNormalizationShift.size(), inputSize);

      auto normalizationScaleShiftMemDesc = memory::desc(scaleShiftDims, memory::data_type::f32, memory::format_tag::nc);
      _inputNormalizationScaleShiftMem = memory(normalizationScaleShiftMemDesc, _engine);

      // Storing scale/shift data
      std::vector<float> scaleShiftData(product(scaleShiftDims));
      for (size_t i = 0; i < inputSize; i++)
      {
        scaleShiftData[i + inputSize * 0] = _batchNormalizationScale[i];
        scaleShiftData[i + inputSize * 1] = _batchNormalizationShift[i];
      }

      normalizationFlags |= normalization_flags::use_scale_shift;
      write_to_dnnl_memory(scaleShiftData.data(), _inputNormalizationScaleShiftMem);
    }

    // Create operation descriptor.
    auto normalizationDesc = dnnl::batch_normalization_forward::desc(
      prop_kind::forward_training,
      _layers[0]->_dataMem.get_desc(),
      _batchNormalizationEpsilon,
      normalizationFlags);

    _inputForwardNormalizationPrimitiveDesc = batch_normalization_forward::primitive_desc(normalizationDesc, _engine);
    _inputForwardNormalizationPrimitive = dnnl::batch_normalization_forward(_inputForwardNormalizationPrimitiveDesc);

    // Create memory objects using memory descriptors created by the primitive descriptor: mean, variance, workspace.
    _inputNormalizationMeanMem = memory(_inputForwardNormalizationPrimitiveDesc.mean_desc(), _engine);
    _inputNormalizationVarianceMem = memory(_inputForwardNormalizationPrimitiveDesc.variance_desc(), _engine);
    _inputNormalizationWorkspaceMem = memory(_inputForwardNormalizationPrimitiveDesc.workspace_desc(), _engine);

    _inputForwardNormalizationArgs.insert({DNNL_ARG_SRC, _layers[0]->_dataMem});
    _inputForwardNormalizationArgs.insert({DNNL_ARG_MEAN, _inputNormalizationMeanMem});
    _inputForwardNormalizationArgs.insert({DNNL_ARG_VARIANCE, _inputNormalizationVarianceMem});
    _inputForwardNormalizationArgs.insert({DNNL_ARG_SCALE_SHIFT, _inputNormalizationScaleShiftMem});
    _inputForwardNormalizationArgs.insert({DNNL_ARG_WORKSPACE, _inputNormalizationWorkspaceMem});
    _inputForwardNormalizationArgs.insert({DNNL_ARG_DST, _layers[0]->_dataMem});
  }

  // Initializing activation function, weight matrix, and bias memory and operations
  for (size_t i = 1; i < layerCount; i++)
  {
    /********************************************************
    * Creating Inner Product Memory Primitive
    * *******************************************************/
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;

    memory::dims weightDims = {OC, IC};

    // Allocating weight memory
    auto weightMemDesc = memory::desc(weightDims, memory::data_type::f32, memory::format_tag::ba);
    _layers[i]->_weightsMem = memory(weightMemDesc, _engine);
    _layers[i]->_weightsDiffMem = memory(weightMemDesc, _engine);

    // Allocating bias memory
    auto biasMemDesc = memory::desc({OC}, memory::data_type::f32, memory::format_tag::a);
    _layers[i]->_biasMem = memory(biasMemDesc, _engine);
    _layers[i]->_biasDiffMem = memory(biasMemDesc, _engine);

    // Create memory descriptor for weights with format_tag::any. This enables
    // the inner product primitive to choose the memory layout for an optimized
    // primitive implementation, and this format may differ from the one
    // provided by the user.
    auto weightsWorkMemDesc = memory::desc(weightDims, memory::data_type::f32, memory::format_tag::any);

    // Create operation descriptor.
    auto inner_product_d = inner_product_forward::desc(prop_kind::forward_training, _layers[i - 1]->_activationMem.get_desc(), weightsWorkMemDesc, biasMemDesc, _layers[i]->_dataMem.get_desc());

    // Create inner product primitive descriptor.
    dnnl::primitive_attr forwardPrimitiveAttributes;
    _layers[i]->_forwardInnerProductPrimitiveDesc = inner_product_forward::primitive_desc(inner_product_d, forwardPrimitiveAttributes, _engine);

    // For now, assume that the weights memory layout generated by the primitive
    // and the one provided by the user are identical.
    _layers[i]->_weightsWorkMem = _layers[i]->_weightsMem;

    // Reorder the data in case the weights memory layout generated by the
    // primitive and the one provided by the user are different. In this case,
    // we create additional memory objects with internal buffers that will
    // contain the reordered data.
    if (_layers[i]->_weightsWorkMem.get_desc() != _layers[i]->_forwardInnerProductPrimitiveDesc.weights_desc())
      _layers[i]->_weightsWorkMem = memory(_layers[i]->_forwardInnerProductPrimitiveDesc.weights_desc(), _engine);

    // Create the weights+bias primitive.
    _layers[i]->_forwardInnerProductPrimitive = inner_product_forward(_layers[i]->_forwardInnerProductPrimitiveDesc);

    // Configuring inner product arguments
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_SRC, _layers[i - 1]->_activationMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_WEIGHTS, _layers[i]->_weightsWorkMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_BIAS, _layers[i]->_biasMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_DST, _layers[i]->_dataMem});
  }

#endif
}

void NeuralNetwork::createBackwardPipeline()
{
  /*********************************************************************************
  *  Initializing memory objects and primitives for BACKWARD propagation
  *********************************************************************************/

  if (_isInitialized == false) KORALI_LOG_ERROR("Neural Network not initialized.\n");
  if (_batchSize == 0) KORALI_LOG_ERROR("You should provide an input batch size larger than zero. Have you forgotten to create the forward pipeline first?\n");

  #ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  const memory::dim N = _batchSize;

  // Creating layer's data diff memory storage
  for (size_t i = 0; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i]->_nodeCount;
    const memory::dims layerDims = {N, IC};
    auto dataDiffMemDesc = memory::desc(layerDims, memory::data_type::f32, memory::format_tag::ab);
    _layers[i]->_dataDiffMem = memory(dataDiffMemDesc, _engine);
    _layers[i]->_activationDiffMem = memory(dataDiffMemDesc, _engine);
  }

  for (ssize_t i = layerCount - 1; i >= 0; i--)
  {
    // Creating backward propagation primitives for activation functions
    auto activationAlgorithm = algorithm::eltwise_linear;
    float alpha = 0.0f;
    float beta = 0.0f;

    if (_layers[i]->_activationFunction == "Identity")
      alpha = 1.0f;

    if (_layers[i]->_activationFunction == "ReLU")
      activationAlgorithm = algorithm::eltwise_relu;

    if (_layers[i]->_activationFunction == "Tanh")
      activationAlgorithm = algorithm::eltwise_tanh;

    // Creating descriptor
    auto activationDesc = eltwise_backward::desc(activationAlgorithm, _layers[i]->_dataMem.get_desc(), _layers[i]->_activationMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    auto backwardActivationPrimitiveDesc = eltwise_backward::primitive_desc(activationDesc, _engine, _layers[i]->_forwardActivationPrimitiveDesc);

    // Create the primitive.
    _layers[i]->_backwardActivationPrimitive = eltwise_backward(backwardActivationPrimitiveDesc);

    // Primitive arguments.
    _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_SRC, _layers[i]->_dataMem});                // Input
    _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_activationDiffMem}); // Input
    _layers[i]->_backwardActivationArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i]->_dataDiffMem});       // Output

    // Do not defined the following primitives for the first layer
    if (i > 0)
    {
     auto backwardDataDesc = inner_product_backward_data::desc(
       _layers[i - 1]->_dataDiffMem.get_desc(),
       _layers[i]->_weightsWorkMem.get_desc(),
       _layers[i]->_dataMem.get_desc());

     // Create the primitive.
     auto backwardDataPrimitiveDesc = inner_product_backward_data::primitive_desc(backwardDataDesc, _engine, _layers[i]->_forwardInnerProductPrimitiveDesc);
     _layers[i]->_backwardDataPrimitive = inner_product_backward_data(backwardDataPrimitiveDesc);

     _layers[i]->_backwardDataArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_dataDiffMem});           // Input
     _layers[i]->_backwardDataArgs.insert({DNNL_ARG_WEIGHTS, _layers[i]->_weightsWorkMem});         // Input
     _layers[i]->_backwardDataArgs.insert({DNNL_ARG_DIFF_SRC, _layers[i - 1]->_activationDiffMem}); // Output

     auto backwardWeightsDesc = inner_product_backward_weights::desc(
       _layers[i - 1]->_dataMem.get_desc(),
       _layers[i]->_weightsMem.get_desc(),
       _layers[i]->_biasMem.get_desc(),
       _layers[i]->_dataDiffMem.get_desc());

     // Create the primitive.
     auto backwardWeightsPrimitiveDesc = inner_product_backward_weights::primitive_desc(backwardWeightsDesc, _engine, _layers[i]->_forwardInnerProductPrimitiveDesc);
     _layers[i]->_backwardWeightsPrimitive = inner_product_backward_weights(backwardWeightsPrimitiveDesc);

     _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_SRC, _layers[i - 1]->_activationMem});   // Input
     _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_DST, _layers[i]->_dataDiffMem});    // Input
     _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_WEIGHTS, _layers[i]->_weightsDiffMem}); // Output
     _layers[i]->_backwardWeightsArgs.insert({DNNL_ARG_DIFF_BIAS, _layers[i]->_biasDiffMem});       // Output
    }
  }

  // Creating backward primitive for the normalization operation
  if (_batchNormalizationEnabled == true)
  {
    const memory::dim IC = _layers[0]->_nodeCount;
    const memory::dims scaleShiftDims = {2, IC};

    auto normalizationFlags = dnnl::normalization_flags::use_global_stats;

    // Processing shift/scale part of batch normalization

    if (_batchNormalizationScale.size() > 0 || _batchNormalizationShift.size() > 0)
      normalizationFlags |= normalization_flags::use_scale_shift;

    // Create operation descriptor.
    auto normalizationDesc = dnnl::batch_normalization_backward::desc(
      prop_kind::backward_data,
      _layers[0]->_dataDiffMem.get_desc(),
      _layers[0]->_dataMem.get_desc(),
      _batchNormalizationEpsilon,
      normalizationFlags);

    auto normalizationPrimitiveDesc =  dnnl::batch_normalization_backward::primitive_desc(normalizationDesc, _engine, _inputForwardNormalizationPrimitiveDesc);
    _inputBackwardNormalizationPrimitive = dnnl::batch_normalization_backward(normalizationPrimitiveDesc);

    _inputBackwardNormalizationArgs.insert({DNNL_ARG_SRC, _layers[0]->_dataMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_DIFF_SRC, _layers[0]->_dataDiffMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_MEAN, _inputNormalizationMeanMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_VARIANCE, _inputNormalizationVarianceMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_SCALE_SHIFT, _inputNormalizationScaleShiftMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_DIFF_SCALE_SHIFT, _inputNormalizationScaleShiftMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_WORKSPACE, _inputNormalizationWorkspaceMem});
    _inputBackwardNormalizationArgs.insert({DNNL_ARG_DIFF_DST, _layers[0]->_dataDiffMem});
  }
#endif
}

void NeuralNetwork::setInput(const std::vector<std::vector<double>>& input)
{
 size_t inputSize = _layers[0]->_nodeCount;

 // Checking input
 if (input.size() != _batchSize)
  KORALI_LOG_ERROR("Input data set has a different batch size (%lu) than the one for which the NN was created for (%lu).\n", input.size(), _batchSize);

 for (size_t i = 0; i < input.size(); i++)
   if (input[i].size() != inputSize)
     KORALI_LOG_ERROR("Input data set %lu has a different number of elements (%lu) than the input layer node count (%lu).\n", i, input[i].size(), inputSize);

 // Copying input data to first layer
 std::vector<float> batchInput(_batchSize * inputSize);
 for (size_t i = 0; i < _batchSize; i++)
   for (size_t j = 0; j < inputSize; j++)
     batchInput[i * inputSize + j] = input[i][j];

 write_to_dnnl_memory(batchInput.data(), _layers[0]->_dataMem);

 // Normalizing input, if required

 if (_batchNormalizationEnabled == true)
 {
   const memory::dim IC = _layers[0]->_nodeCount;

   // Creating memory to read/write variances and means
   std::vector<float> inputNormalizationMeans(IC);
   std::vector<float> inputNormalizationVariances(IC);

   // If not training, assign given variances and means
   if (_inputNormalizationMeans.size() > 0)
   {
     for (size_t i = 0; i < (size_t)IC; i++)
     {
       inputNormalizationMeans[i] = _inputNormalizationMeans[i];
       inputNormalizationVariances[i] = _inputNormalizationVariances[i];
     }

     write_to_dnnl_memory(inputNormalizationMeans.data(), _inputNormalizationMeanMem);
     write_to_dnnl_memory(inputNormalizationVariances.data(), _inputNormalizationVarianceMem);
   }

   // Normalizing input
   _inputForwardNormalizationPrimitive.execute(_stream, _inputForwardNormalizationArgs);

   _stream.wait();

   // Retrieving normalization means and variances
   if (_inputNormalizationMeans.size() == 0 && _inputNormalizationVariances.size() == 0)
   {
     read_from_dnnl_memory(inputNormalizationMeans.data(), _inputNormalizationMeanMem);
     read_from_dnnl_memory(inputNormalizationVariances.data(), _inputNormalizationVarianceMem);

     _inputNormalizationMeans.resize(inputNormalizationMeans.size());
     _inputNormalizationVariances.resize(inputNormalizationVariances.size());
     _inputNormalizationSigmas.resize(inputNormalizationVariances.size());

     // Translating from variance to sigma values
     for (size_t i = 0; i < (size_t)IC; i++)
     {
       _inputNormalizationMeans[i] = inputNormalizationMeans[i];
       _inputNormalizationVariances[i] = inputNormalizationVariances[i];
       _inputNormalizationSigmas[i] = sqrt(inputNormalizationVariances[i]);
       // printf("%f, %f\n", _inputNormalizationMeans[i], _inputNormalizationVariances[i]);
     }
   }
 }
}

void NeuralNetwork::updateWeightsAndBias()
{
#ifdef _KORALI_USE_ONEDNN

  // Initialize solver's configuration here
  size_t layerCount = _layers.size();

  // Initializing weight matrix memories and values
  for (size_t i = 1; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;
    memory::dims weightDims = {OC, IC};
    size_t weightCount = product(weightDims);
    std::vector<float> weightsData(weightCount);
    std::vector<float> biasData(OC);

    if (_layers[i]->_weights.size() != (size_t)OC)
      KORALI_LOG_ERROR("Layer %lu weights were either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu weight sets. \n", i, OC, _layers[i]->_weights.size());

    for (size_t j = 0; j < (size_t)OC; j++)
      if (_layers[i]->_weights[j].size() != (size_t)IC)
        KORALI_LOG_ERROR("Layer %lu weight set %lu was either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu weight sets. \n", i, j, IC, _layers[i]->_weights[j].size());

    if (_layers[i]->_bias.size() != (size_t)OC)
      KORALI_LOG_ERROR("Layer %lu biases were either not initialized (perhaps the NN is not yet trained) or not provided correctly. Expected: %lu, provided: %lu biases. \n", i, OC, _layers[i]->_bias.size());

    for (size_t j = 0; j < (size_t)OC; j++)
      for (size_t k = 0; k < (size_t)IC; k++)
        weightsData[j * IC + k] = _layers[i]->_weights[j][k];

    for (size_t j = 0; j < (size_t)OC; j++)
      biasData[j] = _layers[i]->_bias[j];

    // Setting weight and bias data to oneDNN format
    write_to_dnnl_memory(weightsData.data(), _layers[i]->_weightsMem);
    write_to_dnnl_memory(biasData.data(), _layers[i]->_biasMem);

    // For now, assume that the weights memory layout generated by the primitive
    // and the one provided by the user are identical.
     reorder(_layers[i]->_weightsMem, _layers[i]->_weightsWorkMem).execute(_stream, _layers[i]->_weightsMem, _layers[i]->_weightsWorkMem);
  }

#endif

  // Lifting flag to make sure we do not backpropagate without running forward first
  _hasPerformedForwardPropagation = false;
}


void NeuralNetwork::forward()
{
#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();

  for (size_t i = 1; i < layerCount; i++)
  {
    // Configuring inner product arguments
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_SRC, _layers[i - 1]->_activationMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_WEIGHTS, _layers[i]->_weightsWorkMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_BIAS, _layers[i]->_biasMem});
    _layers[i]->_forwardInnerProductArgs.insert({DNNL_ARG_DST, _layers[i]->_dataMem});
  }

  // forward propagating neural network
  _layers[0]->_forwardActivationPrimitive.execute(_stream, _layers[0]->_forwardActivationArgs);
  for (size_t i = 1; i < layerCount; i++)
  {
    _layers[i]->_forwardInnerProductPrimitive.execute(_stream, _layers[i]->_forwardInnerProductArgs);
    _layers[i]->_forwardActivationPrimitive.execute(_stream, _layers[i]->_forwardActivationArgs);
  }

  // Wait for the computation to finalize.
  _stream.wait();

  #if 0

  // Restoring the output later node values
  std::vector<float> inputData(_batchSize * _layers[0]->_nodeCount);
  read_from_dnnl_memory(inputData.data(), _layers[1]->_forwardInnerProductArgs[DNNL_ARG_SRC]);

  for (size_t i = 0; i < _batchSize; i++)
   for (size_t j = 0; j < _layers[0]->_nodeCount; j++)
    printf("L0 Act Mem: %f\n", inputData[i * _layers[0]->_nodeCount + j]);

  std::vector<float> inputData1(_batchSize * _layers[1]->_nodeCount);
  read_from_dnnl_memory(inputData1.data(), _layers[1]->_dataMem);

  for (size_t i = 0; i < _batchSize; i++)
   for (size_t j = 0; j < _layers[1]->_nodeCount; j++)
    printf("L1 Data Mem: %f\n", inputData1[i * _layers[1]->_nodeCount + j]);

  std::vector<float> weightData(_layers[1]->_nodeCount * _layers[0]->_nodeCount);
  read_from_dnnl_memory(weightData.data(), _layers[1]->_forwardInnerProductArgs[DNNL_ARG_WEIGHTS]);

  for (size_t i = 0; i < _layers[1]->_nodeCount; i++)
   for (size_t j = 0; j < _layers[0]->_nodeCount; j++)
    printf("Weights: %f\n", weightData[i * _layers[0]->_nodeCount + j]);

  #endif

  // Restoring the output later node values
  size_t lastLayer = layerCount - 1;
  size_t nodeCount = _layers[lastLayer]->_nodeCount;
  std::vector<float> resultData(_batchSize * nodeCount);
  read_from_dnnl_memory(resultData.data(), _layers[lastLayer]->_activationMem);

  _outputValues.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++)
  {
   _outputValues[i].resize(nodeCount);
   for (size_t j = 0; j < nodeCount; j++)
   {
    _outputValues[i][j] = resultData[i * nodeCount + j];
   }
  }

#endif

  // Setting fact that forward propagation has been performed.
  _hasPerformedForwardPropagation = true;
}

void NeuralNetwork::backwardWeightsAndBias(std::vector<float>& outputDifferential)
{
  if (_hasPerformedForwardPropagation == false)
    KORALI_LOG_ERROR("Neural network being backward propagated without doing a forward propagation first. \n");

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;

  // Writing to last layers differential information wrt data
  write_to_dnnl_memory(outputDifferential.data(), _layers[lastLayer]->_activationDiffMem);

  // Backward propagating neural network
  for (size_t i = lastLayer; i > 0; i--)
  {
    _layers[i]->_backwardActivationPrimitive.execute(_stream, _layers[i]->_backwardActivationArgs);
    _layers[i]->_backwardDataPrimitive.execute(_stream, _layers[i]->_backwardDataArgs);
    _layers[i]->_backwardWeightsPrimitive.execute(_stream, _layers[i]->_backwardWeightsArgs);
  }

  // Wait for the computation to finalize.
  _stream.wait();

  // Retrieving weight and bias gradients
  for (size_t i = 1; i < layerCount; i++)
  {
    const memory::dim IC = _layers[i - 1]->_nodeCount;
    const memory::dim OC = _layers[i]->_nodeCount;
    memory::dims weightDims = {OC, IC};
    size_t weightCount = product(weightDims);

    std::vector<float> weightsDiffData(weightCount);
    std::vector<float> biasDiffData(OC);

    read_from_dnnl_memory(weightsDiffData.data(), _layers[i]->_weightsDiffMem);
    read_from_dnnl_memory(biasDiffData.data(), _layers[i]->_biasDiffMem);

    for (size_t j = 0; j < (size_t)OC; j++)
      for (size_t k = 0; k < (size_t)IC; k++)
        _layers[i]->_weights[j][k] = weightsDiffData[j * IC + k];

    for (size_t j = 0; j < (size_t)OC; j++)
      _layers[i]->_bias[j] = biasDiffData[j];
  }

#endif
}

void NeuralNetwork::backwardData(std::vector<float>& outputDifferential)
{
  if (_hasPerformedForwardPropagation == false)
    KORALI_LOG_ERROR("Neural network being backward propagated without doing a forward propagation first. \n");

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;

  auto outputBatchSize = _layers[lastLayer]->_nodeCount * _batchSize;
  if (outputDifferential.size() != outputBatchSize)
   KORALI_LOG_ERROR("Incorrect size of output differential provided. Given: %lu, Expected: %lu. \n", outputDifferential.size(), outputBatchSize);

  // Writing to last layers differential information wrt data
  write_to_dnnl_memory(outputDifferential.data(), _layers[lastLayer]->_activationDiffMem);

  // Backward propagating neural network
  for (size_t i = lastLayer; i > 0; i--)
  {
    _layers[i]->_backwardActivationPrimitive.execute(_stream, _layers[i]->_backwardActivationArgs);
    _layers[i]->_backwardDataPrimitive.execute(_stream, _layers[i]->_backwardDataArgs);
  }

  _layers[0]->_backwardActivationPrimitive.execute(_stream, _layers[0]->_backwardActivationArgs);

  if (_batchNormalizationEnabled == true)
   _inputBackwardNormalizationPrimitive.execute(_stream, _inputBackwardNormalizationArgs);

  // Wait for the computation to finalize.
  _stream.wait();

  // Retreiving gradient of the input
  size_t inputSize = _layers[0]->_nodeCount;
  std::vector<float> dataDiff(_batchSize * inputSize);

  read_from_dnnl_memory(dataDiff.data(), _layers[0]->_dataDiffMem);

  _inputGradient.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++)
  {
   _inputGradient[i].resize(inputSize);
   for (size_t j = 0; j < inputSize; j++)
    _inputGradient[i][j] = dataDiff[i * inputSize + j];
  }

#endif
}

} // namespace korali

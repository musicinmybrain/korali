#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include <omp.h>

namespace korali
{
NeuralNetwork::NeuralNetwork()
{
  _isInitialized = false;
  _batchSize = 0;
  _timestepCount = 0;
}

void NeuralNetwork::initialize()
{
  if (_engine == "OneDNN")
  {
#ifdef _KORALI_USE_ONEDNN
    _dnnlEngine = dnnl::engine(dnnl::engine::kind::cpu, 0);
    _dnnlStream = dnnl::stream(_dnnlEngine);
#else

    fprintf(stderr, "[Korali] Warning: Neural Network's engine set to OneDNN, but Korali was installed without support for OneDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";

#endif
  }

  if (_engine == "CuDNN")
  {
#ifdef _KORALI_USE_CUDNN

    if (cudnnCreate(&_cuDNNHandle) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error initializing CUDNN Handle\n");

#else

    fprintf(stderr, "[Korali] Warning: Neural Network's engine set to OneDNN, but Korali was installed without support for OneDNN. Using Korali's default NN Engine\n");
    _engine = "Korali";
#endif
  }

  if (_isInitialized) KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

  if (_engine == "Korali")
  {
#ifdef _KORALI_USE_EIGEN

#else

    KORALI_LOG_ERROR("Neural Network's engine set to Korali, but Korali was installed without support for Eigen3.\n");

#endif
  }

  // Creating layer pointer vector
  size_t layerCount = _layers.size();
  int maxThreads = omp_get_max_threads();
  _layerVector.resize(maxThreads);
  for (int curThread = 0; curThread < maxThreads; curThread++)
   _layerVector[curThread].resize(layerCount);

  // Creating layer objects
  for (size_t i = 0; i < layerCount; i++)
  {
   auto layerJson = _layers[i];
   _layerVector[0][i] = dynamic_cast<neuralNetwork::Layer *>(getModule(layerJson, _k));
   _layerVector[0][i]->applyModuleDefaults(_layers[i]);
   _layerVector[0][i]->setConfiguration(_layers[i]);
  }

  // Assigning relevant metadata to all the layers
  for (size_t i = 0; i < layerCount; i++)
  {
    _layerVector[0][i]->_prevLayer = i > 0 ? _layerVector[0][i - 1] : NULL;
    _layerVector[0][i]->_nextLayer = i < layerCount - 1 ? _layerVector[0][i + 1] : NULL;
    _layerVector[0][i]->_index = i;
    _layerVector[0][i]->_nn = this;
  }

  // Initialize layers
  for (size_t i = 0; i < layerCount; i++) _layerVector[0][i]->initialize();

  // Creating forward and backward-propagation pipeline
  for (size_t i = 0; i < layerCount; i++) _layerVector[0][i]->createHyperparameterMemory();

  // Getting layer parameter counts and indexes
  _hyperparameterCount = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
   _layerVector[0][i]->_hyperparameterIndex = _hyperparameterCount;
   _hyperparameterCount += _layerVector[0][i]->_hyperparameterCount;
  }

  // Getting batch dimensions
  const size_t T = _timestepCount;
  const size_t N = _batchSize;
  const size_t IC = _layerVector[0][0]->_outputChannels;
  const size_t OC = _layerVector[0][layerCount - 1]->_outputChannels;
  const size_t H = _hyperparameterCount;

  // Create forward and backward (only for training) pipelines
  for (size_t i = 0; i < layerCount; i++) _layerVector[0][i]->createForwardPipeline();
  if (_mode == "Training")
    for (size_t i = 0; i < layerCount; i++) _layerVector[0][i]->createBackwardPipeline();

  // Allocating NN Forward storage
  _rawInputValues.resize(T * N * IC);
  _rawOutputValues.resize(T * N * OC);
  _inputBatchLastStep.resize(N);

  // Allocating NN Backward storage (only for training)
  if (_mode == "Training")
  {
    _rawInputGradients.resize(T * N * IC);
    _rawOutputGradients.resize(T * N * OC);
    _hyperparameterGradients.resize(H);
  }

  // Allocating storage for formatted output values
  _outputValues.resize(N);
  for (size_t b = 0; b < N; b++) _outputValues[b].resize(OC);

  // Making sure we do not re-initialize
  _isInitialized = true;

  // Allocating storage for formatted input gradients (only for training)
  if (_mode == "Training")
  {
    _inputGradients.resize(N);
    for (size_t b = 0; b < N; b++) _inputGradients[b].resize(IC);
  }
}

std::vector<float> NeuralNetwork::generateInitialHyperparameters()
{
  // Setting initial values for hyperparameters
  std::vector<float> initialHyperparameters;

  for (size_t i = 0; i < _layerVector[0].size(); i++)
  {
    auto layerParameters = _layerVector[0][i]->generateInitialHyperparameters();
    initialHyperparameters.insert(initialHyperparameters.end(), layerParameters.begin(), layerParameters.end());
  }

  return initialHyperparameters;
}

void NeuralNetwork::forward(const std::vector<std::vector<std::vector<float>>> &inputValues)
{
  size_t T = _timestepCount;
  size_t N = _batchSize;
  size_t IC = _layerVector[0][0]->_outputChannels;
  size_t layerCount = _layerVector[0].size();
  size_t lastLayer = layerCount - 1;
  size_t OC = _layerVector[0][lastLayer]->_outputChannels;

  // Safety checks
  if (inputValues.size() != N) KORALI_LOG_ERROR("Batch size of NN input (%lu) is not the same as NN configuration (%lu).\n", inputValues.size(), N);
  for (size_t b = 0; b < N; b++)
  {
    if (inputValues[b].size() > T)
      KORALI_LOG_ERROR("Timestep of input batch (%lu) is larger (%lu) than max configured (%lu).\n", b, inputValues[b].size(), T);

    for (size_t t = 0; t < inputValues[b].size(); t++)
      if (inputValues[b][t].size() != IC)
        KORALI_LOG_ERROR("Input size of input batch (%lu), timestep (%lu) is different (%lu) than input channels configured (%lu).\n", b, t, inputValues[b][t].size(), IC);
  }

  // First, re-setting all input values to zero
  std::fill(_rawInputValues.begin(), _rawInputValues.end(), 0.0f);

  // Storing timestep count per batch input, for later use on backward propagation
  for (size_t b = 0; b < N; b++) _inputBatchLastStep[b] = inputValues[b].size() - 1;

  // Now replacing values provided by the user in N*T*IC format
  #pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t t = 0; t < inputValues[b].size(); t++)
      for (size_t i = 0; i < IC; i++)
        _rawInputValues[t * N * IC + b * IC + i] = inputValues[b][t][i];

  // Forward propagate layers, once per timestep
  for (_currentTimestep = 0; _currentTimestep < T; _currentTimestep++)
    for (size_t i = 0; i < layerCount; i++)
      _layerVector[0][i]->forwardData();

  #pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      _outputValues[b][i] = _rawOutputValues[_inputBatchLastStep[b] * N * OC + b * OC + i];
}

void NeuralNetwork::backward(const std::vector<std::vector<float>> &outputGradients)
{
  // Getting batch dimensions
  size_t T = _timestepCount;
  size_t N = _batchSize;
  size_t layerCount = _layerVector[0].size();
  size_t lastLayer = layerCount - 1;
  size_t OC = _layerVector[0][lastLayer]->_outputChannels;
  size_t IC = _layerVector[0][0]->_outputChannels;

  if (_mode == "Inference")
    KORALI_LOG_ERROR("Requesting backward propagation but NN was configured for inference only.\n");

  // Safety checks
  if (outputGradients.size() != N) KORALI_LOG_ERROR("Batch size of NN output gradients (%lu) is not the same as NN configuration (%lu).\n", outputGradients.size(), N);
  for (size_t b = 0; b < N; b++)
    if (outputGradients[b].size() > OC)
      KORALI_LOG_ERROR("Size (%lu) of output gradients batch %lu, is different than expected (%lu).\n", outputGradients[b].size(), b, OC);

  // First, re-setting all output gradients to zero
  std::fill(_rawOutputGradients.begin(), _rawOutputGradients.end(), 0.0f);

  // To store the gradients in the NN we place them on the last input timestep
  #pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      _rawOutputGradients[_inputBatchLastStep[b] * N * OC + b * OC + i] = outputGradients[b][i];

  // Resetting cumulative hyperparameter gradients
  std::fill(_hyperparameterGradients.begin(), _hyperparameterGradients.end(), 0.0f);

  // Backward propagating in time, process the corresponding mini-batch
  for (size_t t = 0; t < T; t++)
  {
    // Storage for the hyperparameter gradients for the current timestep, if needed (T > 1)
    std::vector<float> batchHyperparameterGradients(_hyperparameterCount);

    // Starting from the last timestep, and going backwards
    _currentTimestep = T - t - 1;

    // Backward propagating in layer space
    for (size_t i = 0; i < layerCount; i++)
    {
      // Starting from the last layer, and going backwards
      size_t curLayer = lastLayer - i;

      // Running backward data propagation
      _layerVector[0][curLayer]->backwardData();
    }

    for (size_t i = 0; i < layerCount; i++)
    {
     _layerVector[0][i]->backwardHyperparameters();

     // If we are passing only one timestep, copy the hyperparameters directly on the NN output
     if (T == 1) _layerVector[0][i]->getHyperparameterGradients(&_hyperparameterGradients[_layerVector[0][i]->_hyperparameterIndex]);
     if (T > 1) _layerVector[0][i]->getHyperparameterGradients(&batchHyperparameterGradients[_layerVector[0][i]->_hyperparameterIndex]);
    }

    // Adding current hyperparameters to the cumulative vector, only if more than one timestep used
    if (T > 1)
    {
     #pragma omp parallel for simd
     for (size_t i = 0; i < _hyperparameterCount; i++)
       _hyperparameterGradients[i] += batchHyperparameterGradients[i];
    }
  }

  // Copying input gradients -- only for the last timestep provided in the input
#pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < IC; i++)
      _inputGradients[b][i] = _rawInputGradients[_inputBatchLastStep[b] * N * IC + b * IC + i];
}

std::vector<float> NeuralNetwork::getHyperparameters()
{
  auto params = std::vector<float>(_hyperparameterCount);

  size_t layerCount = _layerVector[0].size();

  for (size_t i = 0; i < layerCount; i++)
    _layerVector[0][i]->getHyperparameters(&params[_layerVector[0][i]->_hyperparameterIndex]);

  return params;
}

void NeuralNetwork::setHyperparameters(const std::vector<float> &hyperparameters)
{
  if (hyperparameters.size() != _hyperparameterCount)
    KORALI_LOG_ERROR("Wrong number of hyperparameters passed to the neural network. Expected: %lu, provided: %lu.\n", _hyperparameterCount, hyperparameters.size());

  auto params = std::vector<float>(hyperparameters.begin(), hyperparameters.end());

  size_t layerCount = _layerVector[0].size();

  for (size_t i = 0; i < layerCount; i++)
    _layerVector[0][i]->setHyperparameters(&params[_layerVector[0][i]->_hyperparameterIndex]);
}
} // namespace korali

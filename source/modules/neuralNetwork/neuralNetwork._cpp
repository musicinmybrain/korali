#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include <chrono>

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

#ifdef _KORALI_USE_CUDNN
  #include <cuda.h>
  #include <cudnn.h>
#endif

/********************************************
 * Pending Performance Improvements
 * - Make sure all operations are made on memory objects with format_tag::any, reordered by oneDNN during creation.
 *   This guarantees that the best memory landscape is selected for all these operations
 * - Do not reserve memory nor create/execute primitives for differentiation when just inferring
 * - Use prop_kind::forward_inference always when inferring
 * - Some memory structures can be re-used (instead of having a separate diff vector, reuse the data vector)
 ********************************************/

namespace korali
{
double _inputWriteTime;
double _outputReadTime;
double _normalizationTime;
double _forwardStreamTime;
double _backwardStreamTime;

NeuralNetwork::NeuralNetwork()
{
  _isInitialized = false;
}

void NeuralNetwork::initialize()
{
#ifdef _KORALI_USE_NO_NN

  KORALI_LOG_ERROR("Korali was installed without support NN operations. Change installation configuration to select a NN framework and re-build Korali.\n");

#endif

  if (_isInitialized)
    KORALI_LOG_ERROR("Neural Network has already been initialized!.\n");

#ifdef _KORALI_USE_ONEDNN

  // Initializing Engine and stream

  if (_engineKind == "CPU") _engine = engine(engine::kind::cpu, 0);
  if (_engineKind == "GPU") _engine = engine(engine::kind::gpu, 0);

  _stream = stream(_engine);

#endif

#ifdef _KORALI_USE_CUDNN

  if (cudnnCreate(&_cuDNNHandle) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error initializing CUDNN Handle\n");

#endif

  // Assigning relevant metadata to all the layers
  size_t layerCount = _layers.size();
  for (size_t i = 0; i < layerCount; i++)
  {
    _layers[i]->_prevLayer = i > 0 ? _layers[i - 1] : NULL;
    _layers[i]->_nextLayer = i < layerCount - 1 ? _layers[i + 1] : NULL;
    _layers[i]->_index = i;
    _layers[i]->_nn = this;
  }

  // Creating forward and backward-propagation pipeline
  for (size_t i = 0; i < layerCount; i++) _layers[i]->createHyperparameterMemory();

  // Getting normalization and weight/bias parameter counts
  _normalizationParameterCount = 0;
  _weightParameterCount = 0;

  for (size_t i = 0; i < layerCount; i++) _weightParameterCount += _layers[i]->_weightParameterCount;
  for (size_t i = 0; i < layerCount; i++) _normalizationParameterCount += 2 * _layers[i]->_normalizationParameterCount;

  // Calculating total hyperparameter count
  _hyperparameterCount = _weightParameterCount + _normalizationParameterCount;

  // Check output scaling configuration
  if (_outputScaling.empty() == false)
    if (_outputScaling.size() != _layers[layerCount - 1]->_nodeCount)
      KORALI_LOG_ERROR("Wrong number of output scaling factors passed to the neural network. Expected: %lu, provided: %lu.\n", _layers[layerCount - 1]->_nodeCount, _outputScaling.size());

  // Check output shifting configuration
  if (_outputShift.empty() == false)
    if (_outputShift.size() != _layers[layerCount - 1]->_nodeCount)
      KORALI_LOG_ERROR("Wrong number of output shift factors passed to the neural network. Expected: %lu, provided: %lu.\n", _layers[layerCount - 1]->_nodeCount, _outputShift.size());

  // Making sure we do not re-initialize
  _isInitialized = true;
}

std::vector<float> NeuralNetwork::generateInitialHyperparameters()
{
  // Setting initial values for hyperparameters
  std::vector<float> initialHyperparameters;

  for (size_t i = 0; i < _layers.size(); i++)
  {
    auto layerParameters = _layers[i]->generateInitialHyperparameters();
    if (i == _layers.size()-1)
      for(size_t j = 0; j < layerParameters.size(); ++j) layerParameters[j] *= 0.1;
    initialHyperparameters.insert(initialHyperparameters.end(), layerParameters.begin(), layerParameters.end());
  }

  return initialHyperparameters;
}

void NeuralNetwork::setInput(const std::vector<std::vector<float>> &input)
{
  // [Profiling]
  std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();

  // Getting batch dimensions
  size_t batchSize = input.size();
  size_t inputSize = _layers[0]->_nodeCount;
  size_t layerCount = _layers.size();

  // If batchsize is different than existing one, re-create pipelines
  if (batchSize != _batchSize)
  {
    _batchSize = batchSize;
    for (size_t i = 0; i < layerCount; i++) _layers[i]->createForwardPipeline();
    for (size_t i = 0; i < layerCount; i++) _layers[i]->createBackwardPipeline();
  }

  for (size_t i = 0; i < input.size(); i++)
    if (input[i].size() != inputSize)
      KORALI_LOG_ERROR("Input data set %lu has a different number of elements (%lu) than the input layer node count (%lu).\n", i, input[i].size(), inputSize);

  // Copying input data to first layer
  std::vector<float> batchInput(_batchSize * inputSize);
  for (size_t i = 0; i < _batchSize; i++)
    for (size_t j = 0; j < inputSize; j++)
      batchInput[i * inputSize + j] = input[i][j];

// Writing input to the NN
#ifdef _KORALI_USE_ONEDNN
  write_to_dnnl_memory(batchInput.data(), _layers[0]->_nodeMem);
#endif

#ifdef _KORALI_USE_CUDNN
  if (cudaMemcpy(_layers[0]->_nodeTensor, batchInput.data(), batchInput.size() * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

  // [Profiling]
  std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();
  _inputWriteTime += std::chrono::duration_cast<std::chrono::nanoseconds>(end - begin).count();
}

void NeuralNetwork::forward()
{
  // [Profiling]
  size_t layerCount = _layers.size();
  std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();

  // forward propagating neural network
  for (size_t i = 0; i < layerCount; i++)
  {
    // Applying convolution filter (weights)
    if (i > 0)
    {
#ifdef _KORALI_USE_ONEDNN
      _layers[i]->_forwardInnerProductPrimitive.execute(_stream, _layers[i]->_forwardInnerProductArgs);
#endif

#ifdef _KORALI_USE_CUDNN

      float alpha1 = 1.0f;
      float alpha2 = 0.0f;
      if (cudnnConvolutionForward(
            _cuDNNHandle,
            &alpha1,
            _layers[i - 1]->_nodeTensorDesc,
            _layers[i - 1]->_activationTensor,
            _layers[i]->_weightsFilterDesc,
            _layers[i]->_weightsFilter,
            _layers[i]->_convolutionDesc,
            CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,
            _layers[i]->_convolutionWorkspace,
            _layers[i]->_convolutionWorkspaceSize,
            &alpha2,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running convolution operation");

      float alpha = 1.0f;
      float beta = 1.0f;
      if (_layers[i]->_batchNormalizationEnabled == false)
        cudnnAddTensor(_cuDNNHandle, &alpha, _layers[i]->_biasTensorDesc, _layers[i]->_biasTensor, &beta, _layers[i]->_nodeTensorDesc, _layers[i]->_nodeTensor);

#endif
    }

    // Applying batch normalization

    if (_layers[i]->_batchNormalizationEnabled)
    {
#ifdef _KORALI_USE_ONEDNN
      _layers[i]->_forwardNormalizationPrimitive.execute(_stream, _layers[i]->_forwardNormalizationArgs);
#endif

#ifdef _KORALI_USE_CUDNN

      float alpha = 1.0f;
      float beta = 0.0f;

      if (_usePreloadedNormalizationData == false)
      {
        if (cudnnBatchNormalizationForwardTraining(
              _cuDNNHandle,
              CUDNN_BATCHNORM_SPATIAL,
              &alpha,
              &beta,
              _layers[i]->_nodeTensorDesc,
              _layers[i]->_nodeTensor,
              _layers[i]->_nodeTensorDesc,
              _layers[i]->_batchNormalizationTensor,
              _layers[i]->_batchNormalizationDerivedTensorDesc,
              _layers[i]->_batchNormalizationScaleTensor,
              _layers[i]->_batchNormalizationShiftTensor,
              1.0f, // Replacing old mean completely
              _layers[i]->_batchNormalizationMeanTensor,
              _layers[i]->_batchNormalizationVarianceTensor,
              _layers[i]->_batchNormalizationEpsilon,
              _layers[i]->_batchNormalizationMeanTensorCache,
              _layers[i]->_batchNormalizationVarianceTensorCache) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running batch normalization\n");
      }
      else
      {
        if (cudnnBatchNormalizationForwardInference(
              _cuDNNHandle,
              CUDNN_BATCHNORM_SPATIAL,
              &alpha,
              &beta,
              _layers[i]->_nodeTensorDesc,
              _layers[i]->_nodeTensor,
              _layers[i]->_nodeTensorDesc,
              _layers[i]->_batchNormalizationTensor,
              _layers[i]->_batchNormalizationDerivedTensorDesc,
              _layers[i]->_batchNormalizationScaleTensor,
              _layers[i]->_batchNormalizationShiftTensor,
              _layers[i]->_batchNormalizationMeanTensor,
              _layers[i]->_batchNormalizationVarianceTensor,
              _layers[i]->_batchNormalizationEpsilon) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running batch normalization\n");
      }
#endif
    }

    // Applying activation function

#ifdef _KORALI_USE_ONEDNN
    _layers[i]->_forwardActivationPrimitive.execute(_stream, _layers[i]->_forwardActivationArgs);
#endif

#ifdef _KORALI_USE_CUDNN

    if (_layers[i]->_activationFunctionType == "Elementwise/Linear")
    {
      if (cudaMemcpy(
            _layers[i]->_activationTensor,
            _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationTensor : _layers[i]->_nodeTensor,
            _batchSize * _layers[i]->_nodeCount * sizeof(float),
            cudaMemcpyDeviceToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory (activation = linear)");
    }
    else if (_layers[i]->_activationFunctionType == "Softmax")
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnSoftmaxForward(
            _cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &alpha,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationTensor : _layers[i]->_nodeTensor,
            &beta,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running softmax forward propagation\n");
    }
    else
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnActivationForward(
            _cuDNNHandle,
            _layers[i]->_activationDesc,
            &alpha,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationTensor : _layers[i]->_nodeTensor,
            &beta,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running activation function\n");
    }

#endif
  }

// Waiting for oneDNN computation to finalize.
#ifdef _KORALI_USE_ONEDNN
  _stream.wait();
#endif

#ifdef _KORALI_USE_CUDNN
  if (cudaDeviceSynchronize())
    ;
#endif

  // [Profiling]
  std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();
  _forwardStreamTime += std::chrono::duration_cast<std::chrono::nanoseconds>(end - begin).count();
  begin = std::chrono::steady_clock::now();

  // Restoring the output later node values
  size_t lastLayer = layerCount - 1;
  size_t nodeCount = _layers[lastLayer]->_nodeCount;
  std::vector<float> resultData(_batchSize * nodeCount);

#ifdef _KORALI_USE_ONEDNN
  read_from_dnnl_memory(resultData.data(), _layers[lastLayer]->_activationMem);
#endif

#ifdef _KORALI_USE_CUDNN
  if (cudaMemcpy(resultData.data(), _layers[lastLayer]->_activationTensor, resultData.size() * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

  // Copying NN output to class output
  _outputValues.resize(_batchSize);
  for (size_t i = 0; i < _batchSize; i++)
  {
    _outputValues[i].resize(nodeCount);
    for (size_t j = 0; j < nodeCount; j++) _outputValues[i][j] = resultData[i * nodeCount + j];

    // If we  use scaling, then apply the scaling factors now
    if (_outputScaling.size() > 0)
      for (size_t j = 0; j < nodeCount; j++) _outputValues[i][j] *= _outputScaling[j];

    // If we  use shifting, then apply the scaling factors now
    if (_outputShift.size() > 0)
      for (size_t j = 0; j < nodeCount; j++) _outputValues[i][j] += _outputShift[j];
  }

  // [Profiling]
  end = std::chrono::steady_clock::now();
  _outputReadTime += std::chrono::duration_cast<std::chrono::nanoseconds>(end - begin).count();
}

void NeuralNetwork::backward(const std::vector<float> &outputDifferential)
{
  // [Profiling]
  std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();

  // Running backward propagation
  size_t layerCount = _layers.size();
  size_t lastLayer = layerCount - 1;

  // Writing to last layers differential information wrt data
  std::vector<float> floatDiffs(outputDifferential.begin(), outputDifferential.end());

  // If we use scaling, then apply the scaling factors gradient now
  if (_outputScaling.size() > 0)
    for (size_t i = 0; i < _outputScaling.size(); i++) floatDiffs[i] /= _outputScaling[i];

// Copying gradients to the output end of the NN
#ifdef _KORALI_USE_ONEDNN
  write_to_dnnl_memory(floatDiffs.data(), _layers[lastLayer]->_activationDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
  if (cudaMemcpy(_layers[lastLayer]->_activationDiffTensor, floatDiffs.data(), floatDiffs.size() * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

  // Backward propagating neural network
  for (ssize_t i = lastLayer; i >= 0; i--)
  {
#ifdef _KORALI_USE_ONEDNN

    _layers[i]->_backwardActivationPrimitive.execute(_stream, _layers[i]->_backwardActivationArgs);

#endif

#ifdef _KORALI_USE_CUDNN

    if (_layers[i]->_activationFunctionType == "Elementwise/Linear")
    {
      if (
        cudaMemcpy(
          _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationDiffTensor : _layers[i]->_nodeDiffTensor,
          _layers[i]->_activationDiffTensor,
          _batchSize * _layers[i]->_nodeCount * sizeof(float),
          cudaMemcpyDeviceToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory (backprop activation)");
    }
    else if (_layers[i]->_activationFunctionType == "Softmax")
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnSoftmaxBackward(
            _cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &alpha,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationDiffTensor,
            &beta,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationDiffTensor : _layers[i]->_nodeDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running softmax backward data propagation\n");
    }
    else
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnActivationBackward(
            _cuDNNHandle,
            _layers[i]->_activationDesc,
            &alpha,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_activationDiffTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeTensor,
            &beta,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_batchNormalizationEnabled ? _layers[i]->_batchNormalizationDiffTensor : _layers[i]->_nodeDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running activation backward data propagation\n");
    }

#endif

    if (_layers[i]->_batchNormalizationEnabled)
    {
#ifdef _KORALI_USE_ONEDNN

      _layers[i]->_backwardNormalizationPrimitive.execute(_stream, _layers[i]->_backwardNormalizationArgs);

#endif

#ifdef _KORALI_USE_CUDNN

      float alpha = 1.0f;
      float beta = 0.0f;

      if (cudnnBatchNormalizationBackward(
            _cuDNNHandle,
            CUDNN_BATCHNORM_SPATIAL,
            &alpha,
            &beta,
            &alpha,
            &beta,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_batchNormalizationDiffTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeDiffTensor,
            _layers[i]->_batchNormalizationDerivedTensorDesc,
            _layers[i]->_batchNormalizationScaleTensor,
            _layers[i]->_batchNormalizationScaleDiffTensor,
            _layers[i]->_batchNormalizationShiftDiffTensor,
            _layers[i]->_batchNormalizationEpsilon,
            _layers[i]->_batchNormalizationMeanTensorCache,
            _layers[i]->_batchNormalizationVarianceTensorCache) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running backward batch normalization\n");

#endif
    }

    if (i > 0)
    {
      // Running backward data propagation

#ifdef _KORALI_USE_ONEDNN

      _layers[i]->_backwardDataPrimitive.execute(_stream, _layers[i]->_backwardDataArgs);

#endif

#ifdef _KORALI_USE_CUDNN

      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnConvolutionBackwardData(
            _cuDNNHandle,
            &alpha,
            _layers[i]->_weightsFilterDesc,
            _layers[i]->_weightsFilter,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeDiffTensor,
            _layers[i]->_convolutionDesc,
            CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,
            _layers[i]->_convolutionWorkspace,
            _layers[i]->_convolutionWorkspaceSize,
            &beta,
            _layers[i - 1]->_nodeTensorDesc,
            _layers[i - 1]->_activationDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward data propagation\n");

#endif

        // Running backward weight+bias propagation

#ifdef _KORALI_USE_ONEDNN

      _layers[i]->_backwardWeightsPrimitive.execute(_stream, _layers[i]->_backwardWeightsArgs);

#endif

#ifdef _KORALI_USE_CUDNN

      if (cudnnConvolutionBackwardBias(
            _cuDNNHandle,
            &alpha,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeDiffTensor,
            &beta,
            _layers[i]->_biasTensorDesc,
            _layers[i]->_biasDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward bias propagation\n");

      if (cudnnConvolutionBackwardFilter(
            _cuDNNHandle,
            &alpha,
            _layers[i - 1]->_nodeTensorDesc,
            _layers[i - 1]->_activationTensor,
            _layers[i]->_nodeTensorDesc,
            _layers[i]->_nodeDiffTensor,
            _layers[i]->_convolutionDesc,
            CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,
            _layers[i]->_convolutionWorkspace,
            _layers[i]->_convolutionWorkspaceSize,
            &beta,
            _layers[i]->_weightsFilterDesc,
            _layers[i]->_weightsDiffFilter) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward filter propagation\n");

#endif
    }
  }

// Wait for oneDNN computation to finalize.
#ifdef _KORALI_USE_ONEDNN
  _stream.wait();
#endif

  // [Profiling]
  std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();
  _backwardStreamTime += std::chrono::duration_cast<std::chrono::nanoseconds>(end - begin).count();
}

void NeuralNetwork::normalize(const std::vector<std::vector<std::vector<float>>> &miniBatches)
{
  // [Profiling]
  std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();

  std::vector<float> newMeans(_normalizationParameterCount, 0.0);
  std::vector<float> newVariances(_normalizationParameterCount, 0.0);

  for (size_t step = 0; step < miniBatches.size(); step++)
  {
    setInput(miniBatches[step]);
    forward();

    // Getting this mini-batch's normalization means and variances
    auto curMeans = getNormalizationMeans();
    auto curVariances = getNormalizationVariances();

    // Accuulating their values into the new vector
    for (size_t i = 0; i < _normalizationParameterCount; i++) newMeans[i] += curMeans[i];
    for (size_t i = 0; i < _normalizationParameterCount; i++) newVariances[i] += curVariances[i];
  }

  // Calculating the actual mean and variance averages
  float invMiniBatchSize = 1.0f / (float)miniBatches.size();
  float unbiasedInvMiniBatchSize = 1.0f / ((float)miniBatches.size() - 1.0f);

  for (size_t i = 0; i < _normalizationParameterCount; i++)
  {
    newMeans[i] = newMeans[i] * invMiniBatchSize;
    newVariances[i] = newVariances[i] * unbiasedInvMiniBatchSize;
  }

  // Setting the new adjusted means and variances
  setNormalizationMeans(newMeans);
  setNormalizationVariances(newVariances);

  // [Profiling]
  std::chrono::steady_clock::time_point end = std::chrono::steady_clock::now();
  _normalizationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(end - begin).count();
}

std::vector<float> NeuralNetwork::getHyperparameters()
{
  // Creating single precision floating-point version of the hyperparameters
  auto params = std::vector<float>(_hyperparameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    size_t OC = _layers[i]->_nodeCount;

    // Setting weight data to the layer's weight memory
    if (i > 0)
    {
      size_t IC = _layers[i - 1]->_nodeCount;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_weightsMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_weightsFilter, IC * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += IC * OC;

      // Setting weight data to the layer's weight memory, only if batch normalization is not being used
      if (_layers[i]->_batchNormalizationEnabled == false)
      {
#ifdef _KORALI_USE_ONEDNN
        read_from_dnnl_memory(&params[currPos], _layers[i]->_biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
        if (cudaMemcpy(&params[currPos], _layers[i]->_biasTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

        currPos += OC;
      }
    }

    // If layer normalization is enabled, we fill in scale/shift data
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationScaleShiftMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationScaleTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos + OC], _layers[i]->_batchNormalizationShiftTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += 2 * OC;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationMeanMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationMeanTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += OC;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationVarianceMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationVarianceTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += OC;
    }
  }

  // Returning the hyperparameters converted to float-precision
  return params;
}

void NeuralNetwork::setHyperparameters(const std::vector<float> &hyperparameters)
{
  if (hyperparameters.size() != _hyperparameterCount)
    KORALI_LOG_ERROR("Wrong number of hyperparameters passed to the neural network. Expected: %lu, provided: %lu.\n", _hyperparameterCount, hyperparameters.size());

  // Creating single precision floating-point version of the hyperparameters
  auto params = std::vector<float>(hyperparameters.begin(), hyperparameters.end());

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    size_t OC = _layers[i]->_nodeCount;

    if (i > 0)
    {
      size_t IC = _layers[i - 1]->_nodeCount;

// Setting weight data to the layer's weight memory
#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_weightsMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_weightsFilter, &params[currPos], IC * OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying weights memory to device");
#endif

      currPos += IC * OC;

      // Setting weight data to the layer's weight memory, only if batch normalization is not being used
      if (_layers[i]->_batchNormalizationEnabled == false)
      {
#ifdef _KORALI_USE_ONEDNN
        write_to_dnnl_memory(&params[currPos], _layers[i]->_biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
        if (cudaMemcpy(_layers[i]->_biasTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

        currPos += OC;
      }
    }

    // If layer normalization is enabled, we fill in scale/shift data
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationScaleShiftMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationScaleTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationShiftTensor, &params[currPos + OC], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += 2 * OC;

#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationMeanMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationMeanTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += OC;

#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationVarianceMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationVarianceTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += OC;
    }
  }
}

std::vector<float> NeuralNetwork::getWeightsAndBiases()
{
  // Creating single precision floating-point version of the hyperparameters
  auto params = std::vector<float>(_weightParameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    size_t OC = _layers[i]->_nodeCount;

    // Setting weight data to the layer's weight memory
    if (i > 0)
    {
      size_t IC = _layers[i - 1]->_nodeCount;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_weightsMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_weightsFilter, IC * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += IC * OC;

      // Setting weight data to the layer's weight memory, only if batch normalization is not being used
      if (_layers[i]->_batchNormalizationEnabled == false)
      {
#ifdef _KORALI_USE_ONEDNN
        read_from_dnnl_memory(&params[currPos], _layers[i]->_biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
        if (cudaMemcpy(&params[currPos], _layers[i]->_biasTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

        currPos += OC;
      }
    }

    // If layer normalization is enabled, we fill in scale/shift data
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationScaleShiftMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationScaleTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos + OC], _layers[i]->_batchNormalizationShiftTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += 2 * OC;
    }
  }

  // Returning the hyperparameters converted to float-precision
  return params;
}

void NeuralNetwork::setWeightsAndBiases(const std::vector<float> &weights)
{
  if (weights.size() != _weightParameterCount)
    KORALI_LOG_ERROR("Wrong number of weight parameters passed to the neural network. Expected: %lu, provided: %lu.\n", _weightParameterCount, weights.size());

  // Creating single precision floating-point version of the hyperparameters
  auto params = weights;

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
  {
    size_t OC = _layers[i]->_nodeCount;

    if (i > 0)
    {
      size_t IC = _layers[i - 1]->_nodeCount;

// Setting weight data to the layer's weight memory
#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_weightsMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_weightsFilter, &params[currPos], IC * OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying weights memory to device");
#endif

      currPos += IC * OC;

      // Setting weight data to the layer's weight memory, only if batch normalization is not being used
      if (_layers[i]->_batchNormalizationEnabled == false)
      {
#ifdef _KORALI_USE_ONEDNN
        write_to_dnnl_memory(&params[currPos], _layers[i]->_biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
        if (cudaMemcpy(_layers[i]->_biasTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

        currPos += OC;
      }
    }

    // If layer normalization is enabled, we fill in scale/shift data
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationScaleShiftMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationScaleTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationShiftTensor, &params[currPos + OC], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += 2 * OC;
    }
  }
}

std::vector<float> NeuralNetwork::getNormalizationVariances()
{
  // Creating single precision floating-point version of the hyperparameters
  auto params = std::vector<float>(_normalizationParameterCount);

#ifdef _KORALI_USE_ONEDNN

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
      size_t OC = _layers[i]->_nodeCount;

  #ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationVarianceMem);
  #endif

  #ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationVarianceTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
  #endif

      currPos += OC;
    }

#endif

  // Returning the hyperparameters converted to float-precision
  return params;
}

void NeuralNetwork::setNormalizationVariances(const std::vector<float> &normalizationVariances)
{
  if (normalizationVariances.size() != _normalizationParameterCount)
    KORALI_LOG_ERROR("Wrong number of normalization parameters passed to the neural network. Expected: %lu, provided: %lu.\n", _normalizationParameterCount, normalizationVariances.size());

  // Creating single precision floating-point version of the normalization parameters
  auto params = normalizationVariances;

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
      size_t OC = _layers[i]->_nodeCount;

#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationVarianceMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationVarianceTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += OC;
    }
}

std::vector<float> NeuralNetwork::getNormalizationMeans()
{
  // Creating single precision floating-point version of the hyperparameters
  auto params = std::vector<float>(_normalizationParameterCount);

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
      size_t OC = _layers[i]->_nodeCount;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationMeanMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&params[currPos], _layers[i]->_batchNormalizationMeanTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += OC;
    }

  // Returning the hyperparameters converted to float-precision
  return params;
}

void NeuralNetwork::setNormalizationMeans(const std::vector<float> &normalizationMeans)
{
  if (normalizationMeans.size() != _normalizationParameterCount)
    KORALI_LOG_ERROR("Wrong number of normalization parameters passed to the neural network. Expected: %lu, provided: %lu.\n", _normalizationParameterCount, normalizationMeans.size());

  auto params = normalizationMeans;

  size_t layerCount = _layers.size();
  size_t currPos = 0;
  for (size_t i = 0; i < layerCount; i++)
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
      size_t OC = _layers[i]->_nodeCount;

#ifdef _KORALI_USE_ONEDNN
      write_to_dnnl_memory(&params[currPos], _layers[i]->_batchNormalizationMeanMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(_layers[i]->_batchNormalizationMeanTensor, &params[currPos], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif

      currPos += OC;
    }
}

std::vector<float> NeuralNetwork::getWeightAndBiasesGradients()
{
  // Copying back the gradients and biases back
  std::vector<float> gradientVector(_weightParameterCount);

  // Retrieving hyperparameter gradients
  size_t currPos = 0;
  size_t layerCount = _layers.size();

  for (size_t i = 0; i < layerCount; i++)
  {
    size_t OC = _layers[i]->_nodeCount;

    if (i > 0)
    {
      size_t IC = _layers[i - 1]->_nodeCount;

      // Reading weight gradients

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&gradientVector[currPos], _layers[i]->_weightsDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&gradientVector[currPos], _layers[i]->_weightsDiffFilter, IC * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += OC * IC;

      // If not using layer normalization, we get the bias gradients
      if (_layers[i]->_batchNormalizationEnabled == false)
      {
#ifdef _KORALI_USE_ONEDNN
        read_from_dnnl_memory(&gradientVector[currPos], _layers[i]->_biasDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
        if (cudaMemcpy(&gradientVector[currPos], _layers[i]->_biasDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

        currPos += OC;
      }
    }

    // If using layer normalization, we get normalization gradients
    if (_layers[i]->_batchNormalizationEnabled == true)
    {
#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&gradientVector[currPos], _layers[i]->_batchNormalizationScaleShiftDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&gradientVector[currPos], _layers[i]->_batchNormalizationScaleDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&gradientVector[currPos + OC], _layers[i]->_batchNormalizationShiftDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif

      currPos += 2 * OC;
    }
  }

  return gradientVector;
}

std::vector<float> NeuralNetwork::getDataGradients()
{
  // Retreiving gradient of the input
  size_t inputSize = _layers[0]->_nodeCount;
  std::vector<float> dataDiff(_batchSize * inputSize);

  if (_layers[0]->_batchNormalizationEnabled)
  {
#ifdef _KORALI_USE_ONEDNN
    read_from_dnnl_memory(dataDiff.data(), _layers[0]->_batchNormalizationDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
    size_t OC = _layers[0]->_nodeCount;
    if (cudaMemcpy(dataDiff.data(), _layers[0]->_batchNormalizationDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif
  }
  else
  {
#ifdef _KORALI_USE_ONEDNN
    read_from_dnnl_memory(dataDiff.data(), _layers[0]->_nodeDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
    size_t OC = _layers[0]->_nodeCount;
    if (cudaMemcpy(dataDiff.data(), _layers[0]->_nodeDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif
  }

  return dataDiff;
}

} // namespace korali

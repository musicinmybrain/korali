#include "modules/neuralNetwork/layer/recurrent/recurrent.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_CUDNN
  #include "auxiliar/cudaUtils.hpp"
#endif

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

#ifdef _KORALI_USE_EIGEN
  #include <Eigen/Dense>
using namespace Eigen;
#endif

namespace korali
{
namespace neuralNetwork
{
namespace layer
{
void Recurrent::initialize()
{
  // Checking Layer size
  if (_outputChannels == 0) KORALI_LOG_ERROR("Node count for layer %lu should be larger than zero.\n", _index);

  // Checking position
  if (_index == 0) KORALI_LOG_ERROR("Recurrent layers cannot be the starting layer of the NN\n");
  if (_index == _nn->_layers.size() - 1) KORALI_LOG_ERROR("Recurrent layers cannot be the last layer of the NN\n");

  // If using depth > 1, the input layer channels must be consistent
  if (_depth != 1) if (_prevLayer->_outputChannels != _outputChannels) KORALI_LOG_ERROR("Node count for layer %lu should be the same as that of the previous layer, when depth > 1.\n", _index);
}

} // namespace layer

} // namespace neuralNetwork

} // namespace korali

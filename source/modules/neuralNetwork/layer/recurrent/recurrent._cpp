#include "modules/neuralNetwork/layer/recurrent/recurrent.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_CUDNN
  #include "auxiliar/cudaUtils.hpp"
#endif

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

#ifdef _KORALI_USE_EIGEN
  #include <Eigen/Dense>
using namespace Eigen;
#endif

namespace korali
{
namespace neuralNetwork
{
namespace layer
{
void Recurrent::initialize()
{
  // Checking Layer size
  if (_outputChannels == 0) KORALI_LOG_ERROR("Node count for layer %lu should be larger than zero.\n", _index);

  // Checking position
  if (_index == 0) KORALI_LOG_ERROR("Recurrent layers cannot be the starting layer of the NN\n");
  if (_index == _nn->_layers.size() - 1) KORALI_LOG_ERROR("Recurrent layers cannot be the last layer of the NN\n");

  // If using depth > 1, the input layer channels must be consistent
  if (_depth != 1) if (_prevLayer->_outputChannels != _outputChannels) KORALI_LOG_ERROR("Node count for layer %lu should be the same as that of the previous layer, when depth > 1.\n", _index);
}

std::vector<float> Recurrent::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // Getting dimensions
  const size_t L = _depth;
  const size_t G = _gateCount;
  const size_t IC = _prevLayer->_outputChannels;
  const size_t OC = _outputChannels;

  // Calculate hyperparameters for weight and bias of all linear layers
  // Setting value for this layer's xavier constant
  float xavierConstant = (_weightScaling * sqrtf(6.0f)) / sqrt(_outputChannels + _prevLayer->_outputChannels);

  // Weights applied to the input layer(s)
  for (size_t layerId = 0; layerId < L; layerId++)
   for (size_t gateId = 0; gateId < G; gateId++)
    for (size_t i = 0; i < OC*IC; i++)
     hyperparameters.push_back(xavierConstant * _nn->_uniformGenerator->getRandomNumber());

   // Weights applied to the recurrent layer
  for (size_t layerId = 0; layerId < L; layerId++)
   for (size_t gateId = 0; gateId < G; gateId++)
    for (size_t i = 0; i < OC*OC; i++)
     hyperparameters.push_back(xavierConstant * _nn->_uniformGenerator->getRandomNumber());

   // Bias for the recurrent layer
  for (size_t layerId = 0; layerId < L; layerId++)
   for (size_t gateId = 0; gateId < G; gateId++)
    for (size_t i = 0; i < OC; i++)
     hyperparameters.push_back(0.0f);

  return hyperparameters;
}


} // namespace layer

} // namespace neuralNetwork

} // namespace korali

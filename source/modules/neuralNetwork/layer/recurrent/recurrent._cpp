#include "modules/neuralNetwork/layer/recurrent/recurrent.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_CUDNN
  #include "auxiliar/cudaUtils.hpp"
#endif

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
  using namespace dnnl;
#endif

#ifdef _KORALI_USE_EIGEN
  #include <Eigen/Dense>
using namespace Eigen;
#endif

namespace korali
{
namespace neuralNetwork
{
namespace layer
{
std::vector<float> Recurrent::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // Calculate hyperparameters for weight and bias of all linear layers
  // Setting value for this layer's xavier constant
  float xavierConstant = (_weightScaling * sqrtf(6.0f)) / sqrt(_nodeCount + _prevLayer->_nodeCount);

  for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
  {
   // Adding layer's weights hyperparameter values
   for (size_t i = 0; i < _linearLayerWeightSizes[linLayerID]; i++)
     hyperparameters.push_back(xavierConstant * _nn->_uniformGenerator->getRandomNumber());

   // Adding layer's bias hyperparameter values
   for (size_t i = 0; i < _linearLayerBiasSizes[linLayerID]; i++)
     hyperparameters.push_back(0.0f);
  }

  return hyperparameters;
}

void Recurrent::createHyperparameterMemory()
{
 if (_index == 0) KORALI_LOG_ERROR("Recurrent layers cannot be placed at the 0 position\n");

 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CUDNN")
 {
  // Setting RNN mode and calculating the number of internal linear layer
  if (_mode == "GRU")
  {
   _linearLayerCount = 6;
   _rnnMode = CUDNN_GRU;
  }

  if (_mode == "LSTM")
  {
   _linearLayerCount = 8;
   _rnnMode = CUDNN_LSTM;
  }

  // Creating dropout operator and its memory
  size_t seed = _nn->_k->_randomSeed++; // Pick a seed.
  cudnnErrCheck(cudnnCreateDropoutDescriptor(&_dropoutDesc));
  cudnnErrCheck(cudnnSetDropoutDescriptor(_dropoutDesc,
                              _nn->_cuDNNHandle,
                              _dropoutProbability,
                              NULL,
                              0,
                              seed));

  // Creating RNN operator
  cudnnErrCheck(cudnnCreateRNNDescriptor(&_rnnDesc));
  cudnnErrCheck(cudnnSetRNNDescriptor_v8(_rnnDesc,
                                       CUDNN_RNN_ALGO_STANDARD,
                                       _rnnMode,
                                       CUDNN_RNN_DOUBLE_BIAS,
                                       CUDNN_UNIDIRECTIONAL,
                                       CUDNN_LINEAR_INPUT,
                                       CUDNN_DATA_FLOAT,
                                       CUDNN_DATA_FLOAT,
                                       CUDNN_DEFAULT_MATH,
                                       _prevLayer->_nodeCount,
                                       _nodeCount,
                                       _nodeCount,
                                       1, // Pseudo Layer Count
                                       _dropoutDesc,
                                       0
                                       ));

  // Allocating memory for hyperparameters
  cudnnErrCheck(cudnnGetRNNWeightSpaceSize(_nn->_cuDNNHandle, _rnnDesc, &_weightsSize));

  // The number of hyperparameters is then the workspace size divided by the size of a float
  _hyperparameterCount = _weightsSize / sizeof(float);

  // Creating memory for hyperparameters and their gradients
  cudaErrCheck(cudaMalloc((void**)&_weightsTensor,  _weightsSize));
  cudaErrCheck(cudaMalloc((void**)&_weightsGradientTensor, _weightsSize));

  // Allocating space to store pointers to hyperparameters and their sizes
  _linearLayerWeightTensors.resize(_linearLayerCount);
  _linearLayerWeightSizes.resize(_linearLayerCount);
  _linearLayerBiasTensors.resize(_linearLayerCount);
  _linearLayerBiasSizes.resize(_linearLayerCount);

  // Getting pointers and length of all of the RNN hyperparameters
  for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
  {
     cudnnDataType_t dataType;
     cudnnTensorDescriptor_t  linLayerWeightDesc;
     cudnnTensorDescriptor_t  linLayerBiasDesc;
     cudnnErrCheck(cudnnCreateTensorDescriptor(&linLayerWeightDesc));
     cudnnErrCheck(cudnnCreateTensorDescriptor(&linLayerBiasDesc));

     cudnnErrCheck(cudnnGetRNNWeightParams(_nn->_cuDNNHandle,
                                             _rnnDesc,
                                             0,
                                             _weightsSize,
                                             _weightsTensor,
                                             linLayerID,
                                             linLayerWeightDesc,
                                             &_linearLayerWeightTensors[linLayerID],
                                             linLayerBiasDesc,
                                             &_linearLayerBiasTensors[linLayerID]));

     int weightsDims;
     int weightsDimA[3] = { 0, 0 ,0 };
     int weightsStrideA[3] = { 0, 0 ,0 };
     cudnnErrCheck(cudnnGetTensorNdDescriptor(linLayerWeightDesc,
                                              3,
                                              &dataType,
                                              &weightsDims,
                                              weightsDimA,
                                              weightsStrideA));
     _linearLayerWeightSizes[linLayerID] = weightsDimA[0] * weightsDimA[1] * weightsDimA[2];

     int biasDims;
     int biasDimA[3] = { 0, 0 ,0 };
     int biasStrideA[3] = { 0, 0 ,0 };
     cudnnErrCheck(cudnnGetTensorNdDescriptor(linLayerBiasDesc,
                                              3,
                                              &dataType,
                                              &biasDims,
                                              biasDimA,
                                              biasStrideA));

     _linearLayerBiasSizes[linLayerID] = biasDimA[0] * biasDimA[1] * biasDimA[2];

     cudnnErrCheck(cudnnDestroyTensorDescriptor(linLayerWeightDesc));
     cudnnErrCheck(cudnnDestroyTensorDescriptor(linLayerBiasDesc));
  }
 }
 #endif
}

void Recurrent::createForwardPipeline()
{
 // Calling base layer function
 Layer::createForwardPipeline();

 // Obtaining batch size
 ssize_t N = _nn->_batchSize;

 // Checking Layer sizes
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
 ssize_t OC = _nodeCount;

ssize_t IC = _prevLayer->_nodeCount;

#ifdef _KORALI_USE_CUDNN
if (_nn->_engine == "CUDNN")
{
 // Allocating hidden and candidate memory tensors
  cudaErrCheck(cudaMalloc((void **)&_hStateTensor, N * OC * sizeof(float)));
  cudaErrCheck(cudaMalloc((void **)&_cStateTensor, N * OC * sizeof(float)));

  // Creating RNN data descriptors for input and output
  cudnnErrCheck(cudnnCreateRNNDataDescriptor(&_inputRNNDataDesc));
  cudnnErrCheck(cudnnCreateRNNDataDescriptor(&_outputRNNDataDesc));

  // Setting and copying sequence length array to device
  std::vector<int> seqLengthArray(N, 1);
  cudaErrCheck(cudaMalloc((void **)&_devSequenceLengths, N * sizeof(int)));
  cudaErrCheck(cudaMemcpy(_devSequenceLengths, seqLengthArray.data(), N * sizeof(int), cudaMemcpyHostToDevice));

  // Setting intput/output RNN data descriptors
  cudnnErrCheck(cudnnSetRNNDataDescriptor(
      _inputRNNDataDesc,
      CUDNN_DATA_FLOAT,
      CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED,
      1, // Max Sequence Length
      N,
      IC,
      seqLengthArray.data(),
      NULL));

  cudnnErrCheck(cudnnSetRNNDataDescriptor(
      _outputRNNDataDesc,
      CUDNN_DATA_FLOAT,
      CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED,
      1, // Max Sequence Length
      N,
      OC,
      seqLengthArray.data(),
      NULL));
}

// Now allocating workspace
cudnnErrCheck(cudnnGetRNNTempSpaceSizes(
      _nn->_cuDNNHandle,
      _rnnDesc,
      CUDNN_FWD_MODE_TRAINING,
      _inputRNNDataDesc,
      &_workSpaceSize,
      &_reserveSpaceSize));

cudaErrCheck(cudaMalloc((void **)&_workSpaceTensor, _workSpaceSize));
cudaErrCheck(cudaMalloc((void **)&_reserveSpaceTensor, _reserveSpaceSize));

  #endif
}

void Recurrent::createBackwardPipeline()
{
 // Calling base layer function
 Layer::createBackwardPipeline();

 // Obtaining batch size
 ssize_t N = _nn->_batchSize;

 // Checking Layer sizes
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
 ssize_t OC = _nodeCount;

#ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CUDNN")
  {
   cudaErrCheck(cudaMalloc((void **)&_hStateGradientTensor, N * OC * sizeof(float)));
   cudaErrCheck(cudaMalloc((void **)&_cStateGradientTensor, N * OC * sizeof(float)));
  }
#endif
}

void Recurrent::forwardData()
{
  size_t N = _nn->_batchSize;
  size_t OC = _nodeCount;

  #ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CUDNN")
  {
   std::vector<int> seqLengthArray(N, 1);
   cudnnErrCheck(cudnnRNNForward(
                  _nn->_cuDNNHandle,
                  _rnnDesc,
                  CUDNN_FWD_MODE_TRAINING,
                  _devSequenceLengths, // Sequence Array
                  _inputRNNDataDesc,
                  _prevLayer->_outputTensor,
                  _outputRNNDataDesc,
                  _outputTensor,
                  _outputTensorDesc,
                  _hStateTensor,
                  _hStateGradientTensor,
                  _outputTensorDesc,
                  _cStateTensor,
                  _cStateGradientTensor,
                  _weightsSize,
                  _weightsTensor,
                  _workSpaceSize,
                  _workSpaceTensor,
                  _reserveSpaceSize,
                  _reserveSpaceTensor));
  }
  #endif

  printf("Ran forward data\n");
  exit(0);
}


void Recurrent::backwardData()
{
   printf("Ran backward data\n");
   exit(0);
}

void Recurrent::backwardHyperparameters()
{
 printf("Backward Hyperpars\n");
 exit(0);
}

void Recurrent::setHyperparameters(float *hyperparameters)
{
 size_t curPos = 0;

 // If this is not the initial layer, calculate hyperparameters for weight and bias of all linear layers
 for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
 {
   // Setting layer's weights hyperparameter values
   #ifdef _KORALI_USE_CUDNN
   if (_nn->_engine == "CUDNN") cudaErrCheck(cudaMemcpy(_linearLayerWeightTensors[linLayerID], &hyperparameters[curPos], _linearLayerWeightSizes[linLayerID] * sizeof(float), cudaMemcpyHostToDevice));
   #endif

   curPos += _linearLayerWeightSizes[linLayerID];

   // Setting layer's bias hyperparameter values
   #ifdef _KORALI_USE_CUDNN
   if (_nn->_engine == "CUDNN") cudaErrCheck(cudaMemcpy(_linearLayerBiasTensors[linLayerID], &hyperparameters[curPos], _linearLayerBiasSizes[linLayerID] * sizeof(float), cudaMemcpyHostToDevice));
   #endif

   curPos += _linearLayerBiasSizes[linLayerID];
 }

 // Synchronizing memory transfers
 #ifdef _KORALI_USE_CUDNN
 cudaErrCheck(cudaDeviceSynchronize());
 #endif

// printf("Set Hyperpars\n");
// exit(0);
}

void Recurrent::getHyperparameters(float *hyperparameters)
{
 size_t curPos = 0;

 // If this is not the initial layer, calculate hyperparameters for weight and bias of all linear layers
 for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
 {
   // Setting layer's weights hyperparameter values
   #ifdef _KORALI_USE_CUDNN
   if (_nn->_engine == "CUDNN") cudaErrCheck(cudaMemcpy(&hyperparameters[curPos], _linearLayerWeightTensors[linLayerID], _linearLayerWeightSizes[linLayerID] * sizeof(float), cudaMemcpyDeviceToHost));
   #endif

   curPos += _linearLayerWeightSizes[linLayerID];

   // Setting layer's bias hyperparameter values
   #ifdef _KORALI_USE_CUDNN
   if (_nn->_engine == "CUDNN") cudaErrCheck(cudaMemcpy(&hyperparameters[curPos], _linearLayerBiasTensors[linLayerID], _linearLayerBiasSizes[linLayerID] * sizeof(float), cudaMemcpyDeviceToHost));
   #endif

   curPos += _linearLayerBiasSizes[linLayerID];
 }

// printf("Get Hyperpars\n");
// exit(0);
}

void Recurrent::getHyperparameterGradients(float *gradient)
{
 printf("Get Hyperpars Gradients\n");
 exit(0);
}

} // namespace layer

} // namespace neuralNetwork

} // namespace korali

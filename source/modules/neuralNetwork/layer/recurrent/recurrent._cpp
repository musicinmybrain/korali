#include "modules/neuralNetwork/layer/recurrent/recurrent.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_CUDNN
  #include "auxiliar/cudaUtils.hpp"
#endif

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
  using namespace dnnl;
#endif

#ifdef _KORALI_USE_EIGEN
  #include <Eigen/Dense>
using namespace Eigen;
#endif

namespace korali
{
namespace neuralNetwork
{
namespace layer
{

void Recurrent::initialize()
{
 // Checking Layer size
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer %lu should be larger than zero.\n", _index);

 // Checking position
 if (_index == 0) KORALI_LOG_ERROR("Recurrent layers cannot be the starting layer of the NN\n");
 if (_index == _nn->_layers.size()-1) KORALI_LOG_ERROR("Recurrent layers cannot be the last layer of the NN\n");

 // Checking Layer size
 if (_prevLayer->_nodeCount != _nodeCount) KORALI_LOG_ERROR("Node count for layer %lu should be the same as that of the previous layer.\n", _index);
}

std::vector<float> Recurrent::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // Calculate hyperparameters for weight and bias of all linear layers
  // Setting value for this layer's xavier constant
  float xavierConstant = (_weightScaling * sqrtf(6.0f)) / sqrt(_nodeCount + _prevLayer->_nodeCount);

  for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
  {
   // Adding layer's weights hyperparameter values
   for (size_t i = 0; i < _linearLayerWeightSizes[linLayerID]; i++)
     hyperparameters.push_back(xavierConstant * _nn->_uniformGenerator->getRandomNumber());

   // Adding layer's bias hyperparameter values
   for (size_t i = 0; i < _linearLayerBiasSizes[linLayerID]; i++)
     hyperparameters.push_back(0.0f);
  }

  return hyperparameters;
}

void Recurrent::createHyperparameterMemory()
{
 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CuDNN")
 {
  // Setting RNN mode and calculating the number of internal linear layer
  if (_mode == "GRU")
  {
   _linearLayerCount = 6;
   _rnnMode = CUDNN_GRU;
  }

  if (_mode == "LSTM")
  {
   _linearLayerCount = 8;
   _rnnMode = CUDNN_LSTM;
  }

  // Creating dropout operator and its memory
  size_t seed = _nn->_k->_randomSeed++; // Pick a seed.
  cudnnErrCheck(cudnnCreateDropoutDescriptor(&_dropoutDesc));
  cudnnErrCheck(cudnnSetDropoutDescriptor(_dropoutDesc,
                              _nn->_cuDNNHandle,
                              _dropoutProbability,
                              NULL,
                              0,
                              seed));

  // Creating RNN operator
  cudnnErrCheck(cudnnCreateRNNDescriptor(&_rnnDesc));
  cudnnErrCheck(cudnnSetRNNDescriptor_v8(_rnnDesc,
                                       CUDNN_RNN_ALGO_STANDARD,
                                       _rnnMode,
                                       CUDNN_RNN_DOUBLE_BIAS,
                                       CUDNN_UNIDIRECTIONAL,
                                       CUDNN_SKIP_INPUT,
                                       CUDNN_DATA_FLOAT,
                                       CUDNN_DATA_FLOAT,
                                       CUDNN_DEFAULT_MATH,
                                       _nodeCount,
                                       _nodeCount,
                                       _nodeCount,
                                       1, // Pseudo Layer Count
                                       _dropoutDesc,
                                       CUDNN_RNN_PADDED_IO_DISABLED
                                       ));

  // Allocating memory for hyperparameters
  cudnnErrCheck(cudnnGetRNNWeightSpaceSize(_nn->_cuDNNHandle, _rnnDesc, &_weightsSize));

  // The number of hyperparameters is then the workspace size divided by the size of a float
  _hyperparameterCount = _weightsSize / sizeof(float);

  // Creating memory for hyperparameters and their gradients
  cudaErrCheck(cudaMalloc((void**)&_weightsTensor,  _weightsSize));
  cudaErrCheck(cudaMalloc((void**)&_weightsGradientTensor, _weightsSize));

  // Allocating space to store pointers to hyperparameters and their sizes
  _linearLayerWeightTensors.resize(_linearLayerCount);
  _linearLayerWeightSizes.resize(_linearLayerCount);
  _linearLayerBiasTensors.resize(_linearLayerCount);
  _linearLayerBiasSizes.resize(_linearLayerCount);

  // Getting pointers and length of all of the RNN hyperparameters
  for (int linLayerID = 0; linLayerID < _linearLayerCount; linLayerID++)
  {
     cudnnDataType_t dataType;
     cudnnTensorDescriptor_t  linLayerWeightDesc;
     cudnnTensorDescriptor_t  linLayerBiasDesc;
     cudnnErrCheck(cudnnCreateTensorDescriptor(&linLayerWeightDesc));
     cudnnErrCheck(cudnnCreateTensorDescriptor(&linLayerBiasDesc));

     cudnnErrCheck(cudnnGetRNNWeightParams(_nn->_cuDNNHandle,
                                             _rnnDesc,
                                             0,
                                             _weightsSize,
                                             _weightsTensor,
                                             linLayerID,
                                             linLayerWeightDesc,
                                             &_linearLayerWeightTensors[linLayerID],
                                             linLayerBiasDesc,
                                             &_linearLayerBiasTensors[linLayerID]));

     int weightsDims;
     int weightsDimA[3] = { 0, 0 ,0 };
     int weightsStrideA[3] = { 0, 0 ,0 };
     cudnnErrCheck(cudnnGetTensorNdDescriptor(linLayerWeightDesc,
                                              3,
                                              &dataType,
                                              &weightsDims,
                                              weightsDimA,
                                              weightsStrideA));
     _linearLayerWeightSizes[linLayerID] = weightsDimA[0] * weightsDimA[1] * weightsDimA[2];

     int biasDims;
     int biasDimA[3] = { 0, 0 ,0 };
     int biasStrideA[3] = { 0, 0 ,0 };
     cudnnErrCheck(cudnnGetTensorNdDescriptor(linLayerBiasDesc,
                                              3,
                                              &dataType,
                                              &biasDims,
                                              biasDimA,
                                              biasStrideA));

     _linearLayerBiasSizes[linLayerID] = biasDimA[0] * biasDimA[1] * biasDimA[2];

     cudnnErrCheck(cudnnDestroyTensorDescriptor(linLayerWeightDesc));
     cudnnErrCheck(cudnnDestroyTensorDescriptor(linLayerBiasDesc));
  }
 }
 #endif
}

void Recurrent::createForwardPipeline()
{
 // Calling base layer function
 Layer::createForwardPipeline();

 // Obtaining batch size
 ssize_t N = _nn->_batchSize;

 // Checking Layer sizes
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
 ssize_t OC = _nodeCount;
 ssize_t IC = _prevLayer->_nodeCount;

 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CuDNN")
 {
  int dimA[3];
  int strideA[3];

  dimA[0] = 1; // Hidden Layer count
  dimA[1] = N; // Minibatch size
  dimA[2] = OC; // Hidden Size

  strideA[0] = dimA[2] * dimA[1];
  strideA[1] = dimA[2];
  strideA[2] = 1;

  // Allocating hidden state descriptor
  cudnnErrCheck(cudnnCreateTensorDescriptor(&_hTensorDesc));
  cudnnErrCheck(cudnnSetTensorNdDescriptor(_hTensorDesc, CUDNN_DATA_FLOAT, 3, dimA, strideA));

  // Allocating cell state descriptor (only for LSTM)
  _cTensorDesc = NULL;
  if (_mode == "LSTM")
  {
   cudnnErrCheck(cudnnCreateTensorDescriptor(&_cTensorDesc));
   cudnnErrCheck(cudnnSetTensorNdDescriptor(_cTensorDesc, CUDNN_DATA_FLOAT, 3, dimA, strideA));
  }

  // Allocating hidden state tensors
  _hStateTensor.resize(_nn->_timestepCount);
  for (size_t i = 0; i < _nn->_timestepCount; i++) cudaErrCheck(cudaMalloc((void **)&_hStateTensor[i], N * OC * sizeof(float)));

  // Candidate cell state storage is only necessary when using LSTM
  _cStateTensor.resize(_nn->_timestepCount);
  if (_mode == "LSTM") for (size_t i = 0; i < _nn->_timestepCount; i++) cudaErrCheck(cudaMalloc((void **)&_cStateTensor[i], N * OC * sizeof(float)));

  // Creating RNN data descriptors for input and output
  cudnnErrCheck(cudnnCreateRNNDataDescriptor(&_inputRNNDataDesc));
  cudnnErrCheck(cudnnCreateRNNDataDescriptor(&_outputRNNDataDesc));

  // Setting and copying sequence length array to device
  std::vector<int> seqLengthArray(N, 1);
  cudaErrCheck(cudaMalloc((void **)&_devSequenceLengths, N * sizeof(int)));
  cudaErrCheck(cudaMemcpy(_devSequenceLengths, seqLengthArray.data(), N * sizeof(int), cudaMemcpyHostToDevice));

  // Setting intput/output RNN data descriptors
  cudnnErrCheck(cudnnSetRNNDataDescriptor(
      _inputRNNDataDesc,
      CUDNN_DATA_FLOAT,
      CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED,
      1, // Max Sequence Length
      N,
      IC,
      seqLengthArray.data(),
      NULL));

  cudnnErrCheck(cudnnSetRNNDataDescriptor(
      _outputRNNDataDesc,
      CUDNN_DATA_FLOAT,
      CUDNN_RNN_DATA_LAYOUT_SEQ_MAJOR_PACKED,
      1, // Max Sequence Length
      N,
      OC,
      seqLengthArray.data(),
      NULL));

  // Now allocating workspace
  cudnnErrCheck(cudnnGetRNNTempSpaceSizes(
        _nn->_cuDNNHandle,
        _rnnDesc,
        CUDNN_FWD_MODE_TRAINING,
        _inputRNNDataDesc,
        &_workSpaceSize,
        &_reserveSpaceSize));

  _workSpaceTensor.resize(_nn->_timestepCount);
  for (size_t t = 0; t < _nn->_timestepCount; t++) cudaErrCheck(cudaMalloc((void **)&_workSpaceTensor[t], _workSpaceSize));

  _reserveSpaceTensor.resize(_nn->_timestepCount);
  for (size_t t = 0; t < _nn->_timestepCount; t++) cudaErrCheck(cudaMalloc((void **)&_reserveSpaceTensor[t], _reserveSpaceSize));
 }
 #endif
}

void Recurrent::createBackwardPipeline()
{
 // Calling base layer function
 Layer::createBackwardPipeline();

 // Obtaining batch size
 ssize_t N = _nn->_batchSize;

 // Checking Layer sizes
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
 ssize_t OC = _nodeCount;

#ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN")
  {
   // Allocating hidden state tensors
   _hGradientTensor.resize(_nn->_timestepCount);
   for (size_t i = 0; i < _nn->_timestepCount; i++) cudaErrCheck(cudaMalloc((void **)&_hGradientTensor[i], N * OC * sizeof(float)));

   // Candidate cell state storage is only necessary when using LSTM
   _cGradientTensor.resize(_nn->_timestepCount);
   if (_mode == "LSTM") for (size_t i = 0; i < _nn->_timestepCount; i++) cudaErrCheck(cudaMalloc((void **)&_cGradientTensor[i], N * OC * sizeof(float)));
  }
#endif
}

void Recurrent::forwardData()
{
  size_t N = _nn->_batchSize;
  size_t OC = _nodeCount;
  size_t t = _nn->_currentTimestep;

  #ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN")
  {
   std::vector<int> seqLengthArray(N, 1);
   cudnnErrCheck(cudnnRNNForward(
                  _nn->_cuDNNHandle, // handle
                  _rnnDesc, // rnnDesc
                  CUDNN_FWD_MODE_TRAINING,
                  _devSequenceLengths, // devSeqLengths
                  _inputRNNDataDesc, // xDesc
                  _prevLayer->_outputTensor[t], // x
                  _outputRNNDataDesc, // yDesc
                  _outputTensor[t], // y
                  _hTensorDesc, // hDesc
                  t == 0 ? NULL : _hStateTensor[t-1], // hx
                  _hStateTensor[t], // hy
                  _cTensorDesc, // cDesc
                  t == 0 ? NULL : _cStateTensor[t-1], // cx
                  _cStateTensor[t], // cy
                  _weightsSize,
                  _weightsTensor,
                  _workSpaceSize,
                  _workSpaceTensor[t],
                  _reserveSpaceSize,
                  _reserveSpaceTensor[t]));
  }
  #endif
}


void Recurrent::backwardData()
{
 size_t N = _nn->_batchSize;
 size_t OC = _nodeCount;
 size_t t = _nn->_currentTimestep;

 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CuDNN")
 {
  cudnnErrCheck(cudnnRNNBackwardData_v8(
        _nn->_cuDNNHandle, // handle
       _rnnDesc, // rnnDesc
       _devSequenceLengths, // devSeqLengths
       _outputRNNDataDesc, // yDesc
       _outputTensor[t], // y
       _outputGradientTensor[t], // dy
       _inputRNNDataDesc, // xDesc
       _prevLayer->_outputGradientTensor[t], // dx
       _hTensorDesc, // hDesc
       t == 0 ? NULL : _hStateTensor[t-1], // hx
       t == _nn->_timestepCount - 1 ? NULL : _hGradientTensor[t], // dhy
       t == 0 ? NULL : _hGradientTensor[t-1], // dhx
       _cTensorDesc, // cDesc
       t == 0 ? NULL : _cStateTensor[t-1], // cx
       t == _nn->_timestepCount - 1 ? NULL : _cGradientTensor[t], // dcy
       t == 0 ? NULL : _cGradientTensor[t-1], //dcx
       _weightsSize,
       _weightsTensor,
       _workSpaceSize,
       _workSpaceTensor[t],
       _reserveSpaceSize,
       _reserveSpaceTensor[t]));
 }
 #endif
}

void Recurrent::backwardHyperparameters()
{
 size_t t = _nn->_currentTimestep;

 // We need to clear the gradients because they are additive in CUDNN
 cudaErrCheck(cudaMemset(_weightsGradientTensor, 0, _hyperparameterCount));

 cudnnErrCheck(cudnnRNNBackwardWeights_v8(
     _nn->_cuDNNHandle, // handle
     _rnnDesc, // rnnDesc
     CUDNN_WGRAD_MODE_ADD, // addGrad
     _devSequenceLengths, // devSeqLengths
     _inputRNNDataDesc, // xDesc
     _prevLayer->_outputTensor[t], // x
     _hTensorDesc, // hDesc
     t == 0 ? NULL : _hStateTensor[t-1], // hx
     _outputRNNDataDesc, // yDesc
     _outputTensor[t], // y
     _weightsSize,
     _weightsGradientTensor,
     _workSpaceSize,
     _workSpaceTensor[t],
     _reserveSpaceSize,
     _reserveSpaceTensor[t]));
}

void Recurrent::setHyperparameters(float *hyperparameters)
{
 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CuDNN") cudaErrCheck(cudaMemcpy(_weightsTensor, hyperparameters, _weightsSize, cudaMemcpyHostToDevice));
 #endif
}

void Recurrent::getHyperparameters(float *hyperparameters)
{
  #ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN") cudaErrCheck(cudaMemcpy(hyperparameters, _weightsTensor, _weightsSize, cudaMemcpyDeviceToHost));
  #endif
}

void Recurrent::getHyperparameterGradients(float *gradient)
{
 #ifdef _KORALI_USE_CUDNN
 if (_nn->_engine == "CuDNN") cudaErrCheck(cudaMemcpy(gradient, _weightsGradientTensor, _weightsSize, cudaMemcpyDeviceToHost));
 #endif
}

} // namespace layer

} // namespace neuralNetwork

} // namespace korali

#include "modules/neuralNetwork/layer/layer.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

namespace korali
{
namespace neuralNetwork
{
std::vector<float> Layer::generateInitialHyperparameters()
{
  std::vector<float> hyperparameters;

  // If this is not the initial layer, calculate hyperparameters for weight and bias operation
  if (_prevLayer != NULL)
  {
    // Setting value for this layer's xavier constant
    float xavierConstant = sqrtf(6.0f) / sqrt(_nodeCount + _prevLayer->_nodeCount);

    // Adding layer's weights hyperparameter values
    for (size_t i = 0; i < _nodeCount; i++)
      for (size_t j = 0; j < _prevLayer->_nodeCount; j++)
        hyperparameters.push_back(xavierConstant * _nn->_xavierGenerator->getRandomNumber());

    // Adding layer's bias hyperparameter values, if not using batch initialization
    for (size_t i = 0; i < _nodeCount; i++)
     hyperparameters.push_back(0.0f);
  }

  return hyperparameters;
}

void Layer::createHyperparameterMemory()
{
  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  // If this is not the input layer, we create the inner product (Wx + b) operation
  if (_prevLayer != NULL)
  {
    // Starting to count hyperparameters
    ssize_t IC = _prevLayer->_nodeCount;
    _hyperparameterCount = IC * OC + OC;

    // Allocating weight memory

#ifdef _KORALI_USE_ONEDNN

    memory::dims weightDims = {OC, IC};
    auto weightMemDesc = memory::desc(weightDims, memory::data_type::f32, memory::format_tag::ab);

    _weightsMem = memory(weightMemDesc, _engine);
    _weightsDiffMem = memory(_weightsMem.get_desc(), _engine);

#endif

#ifdef _KORALI_USE_CUDNN

    if (cudnnCreateFilterDescriptor(&_weightsFilterDesc) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating weights filter descriptor\n");
    if (cudnnSetFilter4dDescriptor(_weightsFilterDesc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW, OC, IC, 1, 1) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error setting weight filter descriptor\n");

    if (cudaMalloc((void **)&_weightsFilter, IC * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for Weights filter");
    if (cudaMalloc((void **)&_weightsDiffFilter, IC * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for Weights filter");

#endif

      // Allocating bias memory

#ifdef _KORALI_USE_ONEDNN

    auto biasMemDesc = memory::desc({OC}, memory::data_type::f32, memory::format_tag::a);

    _biasMem = memory(biasMemDesc, _engine);
    _biasDiffMem = memory(_biasMem.get_desc(), _engine);

#endif

#ifdef _KORALI_USE_CUDNN

    if (cudnnCreateTensorDescriptor(&_biasTensorDesc) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating weights filter descriptor\n");
    if (cudnnSetTensor4dDescriptor(_biasTensorDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, 1, OC, 1, 1) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error setting weight filter descriptor\n");

    if (cudaMalloc((void **)&_biasTensor, OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for Weights filter");
    if (cudaMalloc((void **)&_biasDiffTensor, OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for Weights filter");

#endif

    // Adding bias hyperparameters to the count
#ifdef _KORALI_USE_ONEDNN

      std::vector<float> biasData(_nodeCount, 0.0);
      write_to_dnnl_memory(biasData.data(), _biasMem);

#endif
  }
}

void Layer::createForwardPipeline()
{
  /****************************************************************************
   * Checking input/output configuration
   ****************************************************************************/

  // Obtaining batch size
  ssize_t N = _nn->_batchSize;

  // Checking Layer sizes
  if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);
  ssize_t OC = _nodeCount;

  /*********************************************************************************
  *  Initializing memory objects and primitives for FORWARD propagation
  *********************************************************************************/

#ifdef _KORALI_USE_ONEDNN

  // Creating layer's data memory storage and activation function
  const memory::dims layerDims = {N, OC};
  auto dataMemDesc = memory::desc(layerDims, memory::data_type::f32, memory::format_tag::nc);

  // Creating input (node) layer memory
  _nodeMem = memory(dataMemDesc, _engine);

  // Creating activation layer memory
  _activationMem = memory(dataMemDesc, _engine);

#endif

#ifdef _KORALI_USE_CUDNN

  if (cudnnCreateTensorDescriptor(&_nodeTensorDesc) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating node tensor descriptor\n");
  if (cudnnSetTensor4dDescriptor(_nodeTensorDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, N, OC, 1, 1) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error setting node tensor descriptor\n");
  if (cudaMalloc((void **)&_nodeTensor, N * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for node tensor ");
  if (cudaMalloc((void **)&_activationTensor, N * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating activation tensor memory");

#endif

    /*****************************************
  * Creating Activation Function primitive and memory
  * ***************************************/

#ifdef _KORALI_USE_ONEDNN

  // If it is an element-wise operation, create an element-wise primitive
  if (_activationFunctionType.rfind("Elementwise", 0) == 0)
  {
    if (_activationFunctionType == "Elementwise/Clip") _activationAlgorithm = algorithm::eltwise_clip;
    if (_activationFunctionType == "Elementwise/Linear") _activationAlgorithm = algorithm::eltwise_linear;
    if (_activationFunctionType == "Elementwise/Log") _activationAlgorithm = algorithm::eltwise_log;
    if (_activationFunctionType == "Elementwise/ReLU") _activationAlgorithm = algorithm::eltwise_relu;
    if (_activationFunctionType == "Elementwise/Tanh") _activationAlgorithm = algorithm::eltwise_tanh;
    if (_activationFunctionType == "Elementwise/Logistic") _activationAlgorithm = algorithm::eltwise_logistic;

    float alpha = _activationFunctionAlpha;
    float beta = _activationFunctionBeta;

    // Creating descriptor
    auto activationDesc = eltwise_forward::desc(prop_kind::forward_training, _activationAlgorithm, _nodeMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    _forwardEltwiseActivationPrimitiveDesc = eltwise_forward::primitive_desc(activationDesc, _engine);

    // Create the primitive.
    _forwardActivationPrimitive = eltwise_forward(_forwardEltwiseActivationPrimitiveDesc);
  }

  // Check other possible types of activation functions
  if (_activationFunctionType == "Softmax")
  {
    // Creating descriptor
    const int axis = 1;
    auto activationDesc = softmax_forward::desc(prop_kind::forward_training, _nodeMem.get_desc(), axis);

    // Create primitive descriptor.
    _forwardSoftmaxActivationPrimitiveDesc = softmax_forward::primitive_desc(activationDesc, _engine);

    // Create the primitive.
    _forwardActivationPrimitive = softmax_forward(_forwardSoftmaxActivationPrimitiveDesc);
  }

  // Primitive arguments.
  _forwardActivationArgs[DNNL_ARG_SRC] = _nodeMem;
  _forwardActivationArgs[DNNL_ARG_DST] = _activationMem;

#endif

#ifdef _KORALI_USE_CUDNN

  if (cudnnCreateActivationDescriptor(&_activationDesc) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating activation algorithm\n");
  cudnnActivationMode_t activationMode;

  if (_activationFunctionType == "Elementwise/Clip") activationMode = CUDNN_ACTIVATION_CLIPPED_RELU;
  if (_activationFunctionType == "Elementwise/Linear") activationMode = CUDNN_ACTIVATION_IDENTITY;
  if (_activationFunctionType == "Elementwise/Log") KORALI_LOG_ERROR("Activation function not supported: %s.\n", _activationFunctionType.c_str());
  if (_activationFunctionType == "Elementwise/ReLU") activationMode = CUDNN_ACTIVATION_RELU;
  if (_activationFunctionType == "Elementwise/Tanh") activationMode = CUDNN_ACTIVATION_TANH;
  if (_activationFunctionType == "Elementwise/Logistic") activationMode = CUDNN_ACTIVATION_SIGMOID;
  if (_activationFunctionType == "Softmax") activationMode = CUDNN_ACTIVATION_IDENTITY;

  if (cudnnSetActivationDescriptor(_activationDesc, activationMode, CUDNN_PROPAGATE_NAN, _activationFunctionAlpha) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating activation algorithm\n");

#endif

  // If this is not the input layer, we create the inner product (Wx + b) operation
  if (_prevLayer != NULL)
  {
#ifdef _KORALI_USE_ONEDNN

    // Create operation descriptor.
    auto inner_product_d = inner_product_forward::desc(prop_kind::forward_training, _prevLayer->_activationMem.get_desc(), _weightsMem.get_desc(), _biasMem.get_desc(), _nodeMem.get_desc());

    // Create inner product primitive descriptor.
    dnnl::primitive_attr forwardPrimitiveAttributes;
    _forwardInnerProductPrimitiveDesc = inner_product_forward::primitive_desc(inner_product_d, forwardPrimitiveAttributes, _engine);

    // Create the weights+bias primitive.
    _forwardInnerProductPrimitive = inner_product_forward(_forwardInnerProductPrimitiveDesc);

    // Configuring inner product arguments
    _forwardInnerProductArgs[DNNL_ARG_SRC] = _prevLayer->_activationMem;
    _forwardInnerProductArgs[DNNL_ARG_WEIGHTS] = _weightsMem;
    _forwardInnerProductArgs[DNNL_ARG_BIAS] = _biasMem;
    _forwardInnerProductArgs[DNNL_ARG_DST] = _nodeMem;

#endif

#ifdef _KORALI_USE_CUDNN

    if (cudnnCreateConvolutionDescriptor(&_convolutionDesc) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating convolution descriptor\n");
    if (cudnnSetConvolution2dDescriptor(_convolutionDesc, 0, 0, 1, 1, 1, 1, CUDNN_CONVOLUTION, CUDNN_DATA_FLOAT) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error initializing convolution descriptor\n");
    if (cudnnGetConvolutionForwardWorkspaceSize(_nn->_cuDNNHandle, _prevLayer->_nodeTensorDesc, _weightsFilterDesc, _convolutionDesc, _nodeTensorDesc, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, &_convolutionWorkspaceSize) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error obtaining convolution workspace size\n");
    if (cudaMalloc((void **)&_convolutionWorkspace, _convolutionWorkspaceSize * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating convolution memory");

#endif
  }
}

void Layer::createBackwardPipeline()
{
  /*********************************************************************************
  *  Initializing memory objects and primitives for BACKWARD propagation
  *********************************************************************************/

#ifdef _KORALI_USE_ONEDNN

  // Creating data-related gradient memory
  _nodeDiffMem = memory(_nodeMem.get_desc(), _engine);
  _activationDiffMem = memory(_nodeMem.get_desc(), _engine);

#endif

#ifdef _KORALI_USE_CUDNN

  ssize_t N = _nn->_batchSize;
  ssize_t OC = _nodeCount;

  if (cudaMalloc((void **)&_nodeDiffTensor, N * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating memory for node tensor ");
  if (cudaMalloc((void **)&_activationDiffTensor, N * OC * sizeof(float)) != cudaSuccess) KORALI_LOG_ERROR("Error creating activation tensor memory");

#endif

    // Creating backward propagation primitives for activation functions

#ifdef _KORALI_USE_ONEDNN

  float alpha = _activationFunctionAlpha;
  float beta = _activationFunctionBeta;

  // If it is an element-wise operation, create an element-wise backward primitive
  if (_activationFunctionType.rfind("Elementwise", 0) == 0)
  {
    // Creating descriptor
    auto activationDesc = eltwise_backward::desc(_activationAlgorithm, _nodeMem.get_desc(), _activationMem.get_desc(), alpha, beta);

    // Create primitive descriptor.
    auto backwardActivationPrimitiveDesc = eltwise_backward::primitive_desc(activationDesc, _engine, _forwardEltwiseActivationPrimitiveDesc);

    // Create the primitive.
    _backwardActivationPrimitive = eltwise_backward(backwardActivationPrimitiveDesc);
  }

  // Check other possible types of activation functions
  if (_activationFunctionType == "Softmax")
  {
    // Creating descriptor
    const int axis = 1;
    auto activationDesc = softmax_backward::desc(_nodeMem.get_desc(), _activationMem.get_desc(), axis);

    // Create primitive descriptor.
    auto backwardActivationPrimitiveDesc = softmax_backward::primitive_desc(activationDesc, _engine, _forwardSoftmaxActivationPrimitiveDesc);

    // Create the primitive.
    _backwardActivationPrimitive = softmax_backward(backwardActivationPrimitiveDesc);

    // Setting argument
    _backwardActivationArgs[DNNL_ARG_DST] = _activationMem; // Input
  }

#endif

#ifdef _KORALI_USE_ONEDNN

  // Primitive arguments.
  _backwardActivationArgs[DNNL_ARG_DIFF_DST] = _activationDiffMem; // Input
  _backwardActivationArgs[DNNL_ARG_SRC] = _nodeMem;          // Input
  _backwardActivationArgs[DNNL_ARG_DIFF_SRC] = _nodeDiffMem; // Output

#endif

  // We do not define the following primitives for the input layer
  if (_prevLayer != NULL)
  {
#ifdef _KORALI_USE_ONEDNN

    auto backwardDataDesc = inner_product_backward_data::desc(
      _prevLayer->_nodeMem.get_desc(),
      _weightsMem.get_desc(),
      _nodeMem.get_desc());

    // Create the primitive.
    auto backwardDataPrimitiveDesc = inner_product_backward_data::primitive_desc(backwardDataDesc, _engine, _forwardInnerProductPrimitiveDesc);
    _backwardDataPrimitive = inner_product_backward_data(backwardDataPrimitiveDesc);

    _backwardDataArgs[DNNL_ARG_DIFF_DST] = _nodeDiffMem;                   // Input
    _backwardDataArgs[DNNL_ARG_WEIGHTS] = _weightsMem;                     // Input
    _backwardDataArgs[DNNL_ARG_DIFF_SRC] = _prevLayer->_activationDiffMem; // Output

    auto backwardWeightsDesc = inner_product_backward_weights::desc(
      _prevLayer->_nodeMem.get_desc(),
      _weightsMem.get_desc(),
      _biasMem.get_desc(),
      _nodeDiffMem.get_desc());

    // Create the primitive.
    auto backwardWeightsPrimitiveDesc = inner_product_backward_weights::primitive_desc(backwardWeightsDesc, _engine, _forwardInnerProductPrimitiveDesc);
    _backwardWeightsPrimitive = inner_product_backward_weights(backwardWeightsPrimitiveDesc);

    _backwardWeightsArgs[DNNL_ARG_SRC] = _prevLayer->_activationMem; // Input
    _backwardWeightsArgs[DNNL_ARG_DIFF_DST] = _nodeDiffMem;          // Input
    _backwardWeightsArgs[DNNL_ARG_DIFF_WEIGHTS] = _weightsDiffMem;   // Output
    _backwardWeightsArgs[DNNL_ARG_DIFF_BIAS] = _biasDiffMem;         // Output

#endif
  }
}

void Layer::forwardWeightsAndBias()
{
#ifdef _KORALI_USE_ONEDNN
      _forwardInnerProductPrimitive.execute(_nn->_stream, _forwardInnerProductArgs);
#endif

#ifdef _KORALI_USE_CUDNN

      float alpha1 = 1.0f;
      float alpha2 = 0.0f;
      if (cudnnConvolutionForward(
        _nn->_cuDNNHandle,
            &alpha1,
            _prevLayer->_nodeTensorDesc,
            _prevLayer->_activationTensor,
            _weightsFilterDesc,
            _weightsFilter,
            _convolutionDesc,
            CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,
            _convolutionWorkspace,
            _convolutionWorkspaceSize,
            &alpha2,
            _nodeTensorDesc,
            _nodeTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running convolution operation");

      float alpha = 1.0f;
      float beta = 1.0f;
      cudnnAddTensor(_nn->_cuDNNHandle, &alpha, _biasTensorDesc, _biasTensor, &beta, _nodeTensorDesc, _nodeTensor);
#endif
}

void Layer::forwardActivationFunction()
{
#ifdef _KORALI_USE_ONEDNN
    _forwardActivationPrimitive.execute(_nn->_stream, _forwardActivationArgs);
#endif

#ifdef _KORALI_USE_CUDNN

    if (_activationFunctionType == "Elementwise/Linear")
    {
      if (cudaMemcpy(
            _activationTensor,
            _batchNormalizationEnabled ? _batchNormalizationTensor : _nodeTensor,
            _batchSize * _nodeCount * sizeof(float),
            cudaMemcpyDeviceToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory (activation = linear)");
    }
    else if (_activationFunctionType == "Softmax")
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnSoftmaxForward(
            _nn->_cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &alpha,
            _nodeTensorDesc,
            _nodeTensor,
            &beta,
            _nodeTensorDesc,
            _activationTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running softmax forward propagation\n");
    }
    else
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnActivationForward(
            _nn->_cuDNNHandle,
            _activationDesc,
            &alpha,
            _nodeTensorDesc,
            _nodeTensor,
            &beta,
            _nodeTensorDesc,
            _activationTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running activation function\n");
    }

#endif
}

void Layer::getOutputValues(float* output)
{
 #ifdef _KORALI_USE_ONEDNN
   _nn->_stream.wait();
   read_from_dnnl_memory(output, _activationMem);
 #endif

 #ifdef _KORALI_USE_CUDNN
   cudaDeviceSynchronize();
   if (cudaMemcpy(output, _activationTensor, resultData.size() * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
 #endif
}

void Layer::backwardActivationFunction()
{
#ifdef _KORALI_USE_ONEDNN

    _backwardActivationPrimitive.execute(_nn->_stream, _backwardActivationArgs);

#endif

#ifdef _KORALI_USE_CUDNN

    if (_activationFunctionType == "Elementwise/Linear")
    {
      if ( cudaMemcpy(
          _nodeDiffTensor,
          _activationDiffTensor,
          _batchSize * _nodeCount * sizeof(float),
          cudaMemcpyDeviceToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory (backprop activation)");
    }
    else if (_activationFunctionType == "Softmax")
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnSoftmaxBackward(
            _nn->_cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &alpha,
            _nodeTensorDesc,
            _activationTensor,
            _nodeTensorDesc,
            _activationDiffTensor,
            &beta,
            _nodeTensorDesc,
            _nodeDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running softmax backward data propagation\n");
    }
    else
    {
      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnActivationBackward(
            _nn->_cuDNNHandle,
            _activationDesc,
            &alpha,
            _nodeTensorDesc,
            _activationTensor,
            _nodeTensorDesc,
            _activationDiffTensor,
            _nodeTensorDesc,
            _nodeTensor,
            &beta,
            _nodeTensorDesc,
            _nodeDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running activation backward data propagation\n");
    }

#endif
}

void Layer::backwardDataPropagation()
{
#ifdef _KORALI_USE_ONEDNN

      _backwardDataPrimitive.execute(_nn->_stream, _backwardDataArgs);

#endif

#ifdef _KORALI_USE_CUDNN

      float alpha = 1.0f;
      float beta = 0.0f;
      if (cudnnConvolutionBackwardData(
            _nn->_cuDNNHandle,
            &alpha,
            _weightsFilterDesc,
            _weightsFilter,
            _nodeTensorDesc,
            _nodeDiffTensor,
            _convolutionDesc,
            CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,
            _convolutionWorkspace,
            _convolutionWorkspaceSize,
            &beta,
            _prevLayer->_nodeTensorDesc,
            _prevLayer->_activationDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward data propagation\n");

#endif
}

void Layer::backwardWeightsAndBias()
{
#ifdef _KORALI_USE_ONEDNN

      _backwardWeightsPrimitive.execute(_nn->_stream, _backwardWeightsArgs);

#endif

#ifdef _KORALI_USE_CUDNN

      if (cudnnConvolutionBackwardBias(
            _nn->_cuDNNHandle,
            &alpha,
            _nodeTensorDesc,
            _nodeDiffTensor,
            &beta,
            _biasTensorDesc,
            _biasDiffTensor) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward bias propagation\n");

      if (cudnnConvolutionBackwardFilter(
            _nn->_cuDNNHandle,
            &alpha,
            _prevLayer->_nodeTensorDesc,
            _prevLayer->_activationTensor,
            _nodeTensorDesc,
            _nodeDiffTensor,
            _convolutionDesc,
            CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,
            _convolutionWorkspace,
            _convolutionWorkspaceSize,
            &beta,
            _weightsFilterDesc,
            _weightsDiffFilter) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error running filter backward filter propagation\n");

#endif
}

void Layer::setOutputGradients(float* gradient)
{
  #ifdef _KORALI_USE_ONEDNN
    write_to_dnnl_memory(gradient, _activationDiffMem);
  #endif

  #ifdef _KORALI_USE_CUDNN
    if (cudaMemcpy(_activationDiffTensor, gradient, _nodeCount * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
  #endif
}

void Layer::setHyperparameters(float* hyperparameters)
{
 if (_prevLayer != NULL)
 {
  size_t IC = _prevLayer->_nodeCount;
  size_t OC = _nodeCount;

#ifdef _KORALI_USE_ONEDNN
   write_to_dnnl_memory(&hyperparameters[0], _weightsMem);
   write_to_dnnl_memory(&hyperparameters[IC*OC], _biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
   if (cudaMemcpy(_weightsFilter, &hyperparameters[0], IC * OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying weights memory to device");
   if (cudaMemcpy(_biasTensor, &hyperparameters[IC*OC], OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif
 }
}

void Layer::getHyperparameters(float* hyperparameters)
{
 if (_prevLayer != NULL)
 {
  size_t IC = _prevLayer->_nodeCount;
  size_t OC = _nodeCount;

#ifdef _KORALI_USE_ONEDNN
      read_from_dnnl_memory(&hyperparameters[0], _weightsMem);
      read_from_dnnl_memory(&hyperparameters[IC*OC], _biasMem);
#endif

#ifdef _KORALI_USE_CUDNN
      if (cudaMemcpy(&hyperparameters[0], _weightsFilter, IC * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
      if (cudaMemcpy(&hyperparameters[IC*OC], _biasTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif
 }
}

void Layer::getHyperparameterGradients(float* gradient)
{
  if (_prevLayer != NULL)
  {
   size_t IC = _prevLayer->_nodeCount;
   size_t OC = _nodeCount;

 #ifdef _KORALI_USE_ONEDNN
       read_from_dnnl_memory(&gradient[0], _weightsDiffMem);
       read_from_dnnl_memory(&gradient[IC*OC], _biasDiffMem);
 #endif

 #ifdef _KORALI_USE_CUDNN
       if (cudaMemcpy(&gradient[0], _weightsDiffFilter, IC * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
       if (cudaMemcpy(&gradient[IC*OC], _biasDiffTensor, OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
 #endif
  }
}

void Layer::getInputGradients(float* gradient)
{
 size_t N = _nn->_batchSize;
 size_t OC = _nodeCount;

#ifdef _KORALI_USE_ONEDNN
    read_from_dnnl_memory(gradient, _nodeDiffMem);
#endif

#ifdef _KORALI_USE_CUDNN
    if (cudaMemcpy(gradient, _nodeDiffTensor, N * OC * sizeof(float), cudaMemcpyDeviceToHost) != cudaSuccess) KORALI_LOG_ERROR("Error copying memory");
#endif
}

void Layer::setInputValues(float* input)
{
 size_t N = _nn->_batchSize;
 size_t OC = _nodeCount;

#ifdef _KORALI_USE_ONEDNN
  write_to_dnnl_memory(input, _nodeMem);
#endif

#ifdef _KORALI_USE_CUDNN
  if (cudaMemcpy(_nodeTensor, input, N * OC * sizeof(float), cudaMemcpyHostToDevice) != cudaSuccess) KORALI_LOG_ERROR("Error copying bias memory to device");
#endif
}

} // namespace neuralNetwork

} // namespace korali

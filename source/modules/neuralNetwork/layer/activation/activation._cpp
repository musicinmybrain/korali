#include "modules/neuralNetwork/layer/activation/activation.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"

#ifdef _KORALI_USE_CUDNN
  #include "auxiliar/cudaUtils.hpp"
#endif

#ifdef _KORALI_USE_ONEDNN
  #include "auxiliar/dnnUtils.hpp"
using namespace dnnl;
#endif

#ifdef _KORALI_USE_EIGEN
  #include <Eigen/Dense>
using namespace Eigen;
#endif

namespace korali
{
namespace neuralNetwork
{
namespace layer
{

void Activation::initialize()
{
 // The node count for this layer should be the same as the previous layer
 _nodeCount = _prevLayer->_nodeCount;

 // Checking Layer size
 if (_nodeCount == 0) KORALI_LOG_ERROR("Node count for layer (%lu) should be larger than zero.\n", _index);

 // Checking position
 if (_index == 0) KORALI_LOG_ERROR("Activation layers cannot be the starting layer of the NN\n");
 if (_index == _nn->_layers.size()-1) KORALI_LOG_ERROR("Activation layers cannot be the last layer of the NN\n");
}

void Activation::createForwardPipeline()
{
  // Calling base layer function
  Layer::createForwardPipeline();

  // Obtaining sizes
  ssize_t N = _nn->_batchSize;
  ssize_t OC = _nodeCount;

#ifdef _KORALI_USE_ONEDNN
  if (_nn->_engine == "OneDNN")
  {
    // If it is an element-wise operation, create an element-wise primitive
    if (_function.rfind("Elementwise", 0) == 0)
    {
      if (_function == "Elementwise/Clip") _activationAlgorithm = algorithm::eltwise_clip;
      if (_function == "Elementwise/Linear") _activationAlgorithm = algorithm::eltwise_linear;
      if (_function == "Elementwise/Log") _activationAlgorithm = algorithm::eltwise_log;
      if (_function == "Elementwise/ReLU") _activationAlgorithm = algorithm::eltwise_relu;
      if (_function == "Elementwise/Tanh") _activationAlgorithm = algorithm::eltwise_tanh;
      if (_function == "Elementwise/Logistic") _activationAlgorithm = algorithm::eltwise_logistic;
      if (_function == "Elementwise/SoftSign") KORALI_LOG_ERROR("ONEDNN does not support activation functions of type 'Elementwise/SoftSign'.");

      // Creating descriptor
      auto activationDesc = eltwise_forward::desc(prop_kind::forward_training, _activationAlgorithm, _prevLayer->_outputMem.get_desc(), _alpha, _beta);

      // Create primitive descriptor.
      _forwardEltwiseActivationPrimitiveDesc = eltwise_forward::primitive_desc(activationDesc, _engine);

      // Create the primitive.
      _forwardActivationPrimitive = eltwise_forward(_forwardEltwiseActivationPrimitiveDesc);
    }

    // Check other possible types of activation functions
    if (_function == "Softmax")
    {
      // Creating descriptor
      const int axis = 1;
      auto activationDesc = softmax_forward::desc(prop_kind::forward_training, _prevLayer->_outputMem.get_desc(), axis);

      // Create primitive descriptor.
      _forwardSoftmaxActivationPrimitiveDesc = softmax_forward::primitive_desc(activationDesc, _engine);

      // Create the primitive.
      _forwardActivationPrimitive = softmax_forward(_forwardSoftmaxActivationPrimitiveDesc);
    }
  }
#endif

#ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN")
  {
    cudnnErrCheck(cudnnCreateActivationDescriptor(&_activationDesc));
    cudnnActivationMode_t activationMode;

    if (_function == "Elementwise/Clip") activationMode = CUDNN_ACTIVATION_CLIPPED_RELU;
    if (_function == "Elementwise/Linear") activationMode = CUDNN_ACTIVATION_IDENTITY;
    if (_function == "Elementwise/Log") KORALI_LOG_ERROR("Activation function not supported: %s.\n", _function.c_str());
    if (_function == "Elementwise/ReLU") activationMode = CUDNN_ACTIVATION_RELU;
    if (_function == "Elementwise/Tanh") activationMode = CUDNN_ACTIVATION_TANH;
    if (_function == "Elementwise/Logistic") activationMode = CUDNN_ACTIVATION_SIGMOID;
    if (_function == "Softmax") activationMode = CUDNN_ACTIVATION_IDENTITY;
    if (_function == "Elementwise/SoftSign") KORALI_LOG_ERROR("CUDNN does not support activation functions of type 'Elementwise/SoftSign'.");

    if (cudnnSetActivationDescriptor(_activationDesc, activationMode, CUDNN_PROPAGATE_NAN, _alpha) != CUDNN_STATUS_SUCCESS) KORALI_LOG_ERROR("Error creating activation algorithm\n");
  }
#endif
}

void Activation::createBackwardPipeline()
{
  /*********************************************************************************
  *  Initializing memory objects and primitives for BACKWARD propagation
  *********************************************************************************/

 // Calling base layer function
 Layer::createBackwardPipeline();

  size_t N = _nn->_batchSize;
  size_t OC = _nodeCount;

// Creating backward propagation primitives for activation functions
#ifdef _KORALI_USE_ONEDNN
  if (_nn->_engine == "OneDNN")
  {
    // If it is an element-wise operation, create an element-wise backward primitive
    if (_function.rfind("Elementwise", 0) == 0)
    {
      // Creating descriptor
      auto activationDesc = eltwise_backward::desc(_activationAlgorithm, _prevLayer->_outputMem.get_desc(), _outputMem.get_desc(), _alpha, _beta);

      // Create primitive descriptor.
      auto backwardActivationPrimitiveDesc = eltwise_backward::primitive_desc(activationDesc, _engine, _forwardEltwiseActivationPrimitiveDesc);

      // Create the primitive.
      _backwardActivationPrimitive = eltwise_backward(backwardActivationPrimitiveDesc);
    }

    // Check other possible types of activation functions
    if (_function == "Softmax")
    {
      // Creating descriptor
      const int axis = 1;
      auto activationDesc = softmax_backward::desc(_prevLayer->_outputMem.get_desc(), _outputMem.get_desc(), axis);

      // Create primitive descriptor.
      auto backwardActivationPrimitiveDesc = softmax_backward::primitive_desc(activationDesc, _engine, _forwardSoftmaxActivationPrimitiveDesc);

      // Create the primitive.
      _backwardActivationPrimitive = softmax_backward(backwardActivationPrimitiveDesc);
    }
  }
#endif

}

void Activation::forwardData()
{
  size_t N = _nn->_batchSize;
  size_t OC = _nodeCount;
  int t = _nn->_currentTimestep;

#ifdef _KORALI_USE_EIGEN
  if (_nn->_engine == "Korali")
  {
    if (_function == "Identity")
      memcpy(_prevLayer->_outputValues, _outputValues, N * OC * sizeof(float));

    if (_function == "Elementwise/Linear")
      for (size_t i = 0; i < N * OC; i++)
        _outputValues[i] = _prevLayer->_outputValues[i] * _alpha;

    if (_function == "Elementwise/Log")
      for (size_t i = 0; i < N * OC; i++)
        _outputValues[i] = std::log(_prevLayer->_outputValues[i]);

    if (_function == "Elementwise/ReLU")
      for (size_t i = 0; i < N * OC; i++)
        if (_prevLayer->_outputValues[i] > 0.0f)
          _outputValues[i] = _prevLayer->_outputValues[i];
        else
          _outputValues[i] = _prevLayer->_outputValues[i] * _alpha;

    if (_function == "Elementwise/Tanh")
      for (size_t i = 0; i < N * OC; i++)
        _outputValues[i] = std::tanh(_prevLayer->_outputValues[i]);

    if (_function == "Elementwise/Logistic")
      for (size_t i = 0; i < N * OC; i++)
        _outputValues[i] = 1.0f / (1.0f + std::exp(-_prevLayer->_outputValues[i]));

    if (_activationFunctionType == "Elementwise/SoftSign")
      for (size_t i = 0; i < N * OC; i++)
        _outputValues[i] = _inputValues[i] / (1.0f + std::abs(_inputValues[i]));

    if (_function == "Softmax")
    {
      for (size_t i = 0; i < N; i++)
      {
        float LSE = logSumExp(&_prevLayer->_outputValues[i * OC], OC);
        for (size_t j = 0; j < OC; j++)
          _outputValues[i * OC + j] = std::exp(_prevLayer->_outputValues[i * OC + j] - LSE);
      }
    }
  }
#endif

#ifdef _KORALI_USE_ONEDNN
  if (_nn->_engine == "OneDNN")
  {
   // Primitive arguments.
   _forwardActivationArgs[DNNL_ARG_SRC] = _prevLayer->_outputMem;
   _forwardActivationArgs[DNNL_ARG_DST] = _outputMem;

    _forwardActivationPrimitive.execute(_nn->_stream, _forwardActivationArgs);
  }
#endif

#ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN")
  {
    if (_function == "Elementwise/Linear")
    {
     cudaErrCheck(cudaMemcpy(
            _outputTensor[t],
            _prevLayer->_outputTensor[t],
            N * OC * sizeof(float),
            cudaMemcpyDeviceToDevice));
    }
    else if (_function == "Softmax")
    {
      cudnnErrCheck(cudnnSoftmaxForward(
            _nn->_cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &_alpha,
            _prevLayer->_outputTensorDesc,
            _prevLayer->_outputTensor[t],
            &_beta,
            _outputTensorDesc,
            _outputTensor[t]));
    }
    else
    {
      cudnnErrCheck(cudnnActivationForward(
            _nn->_cuDNNHandle,
            _activationDesc,
            &_alpha,
            _prevLayer->_outputTensorDesc,
            _prevLayer->_outputTensor[t],
            &_beta,
            _outputTensorDesc,
            _outputTensor[t]));
    }
  }
#endif
}

void Activation::backwardData()
{
  size_t N = _nn->_batchSize;
  size_t OC = _nodeCount;
  int t = _nn->_currentTimestep;

#ifdef _KORALI_USE_EIGEN
  if (_nn->_engine == "Korali")
  {
    if (_function == "Identity")
      memcpy(_prevLayer->_outputGradient, _outputGradient, N * OC * sizeof(float));

    if (_function == "Elementwise/Linear")
      for (size_t i = 0; i < N * OC; i++)
        _prevLayer->_outputGradient[i] = _outputGradient[i] * _alpha;

    if (_function == "Elementwise/Log")
      for (size_t i = 0; i < N * OC; i++)
        _prevLayer->_outputGradient[i] = _outputGradient[i] / _prevLayer->_outputValues[i];

    if (_function == "Elementwise/ReLU")
      for (size_t i = 0; i < N * OC; i++)
        if (_prevLayer->_outputValues[i] > 0.0f)
          _prevLayer->_outputGradient[i] = _outputGradient[i];
        else
          _prevLayer->_outputGradient[i] = _outputGradient[i] * _alpha;

    if (_function == "Elementwise/Tanh")
      for (size_t i = 0; i < N * OC; i++)
        _prevLayer->_outputGradient[i] = _outputGradient[i] * (1.0f - _outputValues[i] * _outputValues[i]);

    if (_function == "Elementwise/Logistic")
      for (size_t i = 0; i < N * OC; i++)
        _prevLayer->_outputGradient[i] = _outputGradient[i] * _outputValues[i] * (1.0f - _outputValues[i]);

    if (_activationFunctionType == "Elementwise/SoftSign")
      for (size_t i = 0; i < N * OC; i++)
        _inputDiff[i] = _outputDiff[i] / ((1.0f + std::abs(_outputValues[i])) * (1.0f + std::abs(_outputValues[i])));

    if (_function == "Softmax")
      for (size_t i = 0; i < N * OC; i++)
        _prevLayer->_outputGradient[i] = _outputGradient[i] * _outputValues[i] * (1.0f - _outputValues[i]);
  }
#endif

#ifdef _KORALI_USE_ONEDNN
  if (_nn->_engine == "OneDNN")
  {
   // Primitive arguments.
   _backwardActivationArgs[DNNL_ARG_DIFF_DST] = _outputGradientMem; // Input
   _backwardActivationArgs[DNNL_ARG_SRC] = _prevLayer->_outputMem;                // Input
   _backwardActivationArgs[DNNL_ARG_DIFF_SRC] = _prevLayer->_outputGradientMem;       // Output
   if (_function == "Softmax") _backwardActivationArgs[DNNL_ARG_DST] = _outputMem;

    _backwardActivationPrimitive.execute(_nn->_stream, _backwardActivationArgs);
  }
#endif

#ifdef _KORALI_USE_CUDNN
  if (_nn->_engine == "CuDNN")
  {
    if (_function == "Elementwise/Linear")
    {
     cudaErrCheck(cudaMemcpy(
            _prevLayer->_outputGradientTensor[t],
            _outputGradientTensor[t],
            N * OC * sizeof(float),
            cudaMemcpyDeviceToDevice));
    }
    else if (_function == "Softmax")
    {
      cudnnErrCheck(cudnnSoftmaxBackward(
            _nn->_cuDNNHandle,
            CUDNN_SOFTMAX_LOG,
            CUDNN_SOFTMAX_MODE_CHANNEL,
            &_alpha,
            _outputTensorDesc,
            _outputTensor[t],
            _outputTensorDesc,
            _outputGradientTensor[t],
            &_beta,
            _prevLayer->_outputTensorDesc,
            _prevLayer->_outputGradientTensor[t]));
    }
    else
    {
      cudnnErrCheck(cudnnActivationBackward(
            _nn->_cuDNNHandle,
            _activationDesc,
            &_alpha,
            _outputTensorDesc,
            _outputTensor[t],
            _outputTensorDesc,
            _outputGradientTensor[t],
            _prevLayer->_outputTensorDesc,
            _prevLayer->_outputTensor[t],
            &_beta,
            _prevLayer->_outputTensorDesc,
            _prevLayer->_outputGradientTensor[t]));
    }
  }
#endif
}

} // namespace layer

} // namespace neuralNetwork

} // namespace korali

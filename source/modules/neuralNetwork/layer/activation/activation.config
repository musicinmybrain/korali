{
  "Module Data":
  {
    "Class Name": "Activation",
    "Namespace": ["korali", "neuralNetwork", "layer"],
    "Parent Class Name": "Layer"
  },

 "Configuration Settings":
 [
   {
    "Name": [ "Function" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Elementwise/Linear", "Description": "Transforms (s) input element-wise to alpha*s + beta. The default represents an identity transformation." },
                { "Value": "Elementwise/Tanh", "Description": "Applies the element-wise tanh function." },
                { "Value": "Elementwise/ReLU", "Description": "Applies an element-wise rectifier linear unit function." },
                { "Value": "Elementwise/Logistic", "Description": "Applies an element-wise logistic (sigmoid) function." },
                { "Value": "Elementwise/Clip", "Description": "Clips the input (s) to be clipped into the alpha < s < beta range." },
                { "Value": "Elementwise/Log", "Description": "Applies the element-wise log function." },
                { "Value": "Elementwise/SoftSign", "Description": "Applies an element-wise soft sign unit function (currently only supported with the Eigen library)." },
                { "Value": "Softmax", "Description": "Applies the layer-wide softmax operation." }
               ],
    "Description": "Indicates the activation function for the weighted inputs to the current layer."
   },
   {
    "Name": [ "Alpha" ],
    "Type": "float",
    "Description": "First (alpha) argument to the activation function, as detailed in https://oneapi-src.github.io/oneDNN/dev_guide_eltwise.html"
   },
   {
    "Name": [ "Beta" ],
    "Type": "float",
    "Description": "Second (beta) argument to the activation function, as detailed in https://oneapi-src.github.io/oneDNN/dev_guide_eltwise.html"
   }
 ],

  "Internal Settings":
 [
 ],

 "Module Defaults":
 {
   "Alpha": 1.0,
   "Beta": 0.0
 }
}

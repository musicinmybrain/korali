#include "engine.hpp"
#include "modules/conduit/distributed/distributed.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/solver.hpp"

using namespace std;

namespace korali
{
#ifdef _KORALI_USE_MPI

  #define MPI_TAG_ACTION_JSON_SIZE 1
  #define MPI_TAG_ACTION_JSON_CONTENT 2

MPI_Comm __KoraliTeamComm;
MPI_Comm getKoraliMPIComm() { return __KoraliTeamComm; }
long int getKoraliMPICommPointer() { return (long int)(&__KoraliTeamComm); }
#endif

namespace conduit
{
void Distributed::initialize()
{
#ifdef _KORALI_USE_MPI
  _rankCount = 1;
  _rankId = 0;

  int isInitialized = 0;
  MPI_Initialized(&isInitialized);

  if (isInitialized == 0) MPI_Init(NULL, NULL);

  if (_communicator == 0)
    _mpiCommunicator = MPI_COMM_WORLD;
  else
  #ifdef OMPI_MPI_H
    _mpiCommunicator = *((MPI_Comm *)_communicator);
  #else
    _mpiCommunicator = (MPI_Comm)_communicator;
  #endif

  int mpiRankCount;
  int mpiRankId;
  MPI_Comm_size(_mpiCommunicator, &mpiRankCount);
  MPI_Comm_rank(_mpiCommunicator, &mpiRankId);
  _rankCount = mpiRankCount;
  _rankId = mpiRankId;
#endif

#ifndef _KORALI_USE_MPI
  KORALI_LOG_ERROR("Running an Distributed-based Korali application, but Korali was installed without support for Distributed.\n");
#endif

#ifdef _KORALI_USE_MPI
  MPI_Barrier(_mpiCommunicator);
  _continueEvaluations = true;

  if (_rankCount == 1) KORALI_LOG_ERROR("Korali Distributed applications require at least 2 Distributed ranks to run.\n");

  _teamCount = (_rankCount - 1) / _workersPerTeam;
  _teamId = 0;
  _localRankId = 0;

  _teamIdSet = false;
  size_t currentRank = 0;
  _teamWorkers.clear();
  _teamQueue = queue<size_t>();
  while (!_teamQueue.empty()) _teamQueue.pop();
  for (size_t i = 0; i < _teamCount; i++)
  {
    _teamQueue.push(i);
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      if (currentRank == _rankId)
      {
        _teamId = i;
        _localRankId = j;
        _teamIdSet = true;
      }
      _teamWorkers[i].push_back(currentRank++);
    }
  }

  if (isRoot())
  {
    int mpiSize;
    MPI_Comm_size(_mpiCommunicator, &mpiSize);

    if (_rankCount < _workersPerTeam + 1)
      KORALI_LOG_ERROR("You are running Korali with %d ranks. However, you need at least %d ranks to have at least one worker team. \n", _rankCount, _workersPerTeam + 1);

    _teamId = _teamCount + 1;
  }

  MPI_Comm_split(_mpiCommunicator, _teamId, _rankId, &__KoraliTeamComm);
  MPI_Barrier(_mpiCommunicator);
#endif
}

void Distributed::initServer()
{
#ifdef _KORALI_USE_MPI
  if (!isRoot()) workerThread();
#endif
}

void Distributed::update(Sample &sample)
{
#ifdef _KORALI_USE_MPI
  if (_localRankId == 0)
  {
    string resultJsonString = sample._js.getJson().dump();
    size_t resultJsonSize = resultJsonString.size();
    MPI_Send(&resultJsonSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
    MPI_Send(resultJsonString.c_str(), resultJsonSize, MPI_CHAR, getRootRank(), MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
  }
#endif
}

void Distributed::finalize()
{
#ifdef _KORALI_USE_MPI
  auto terminationJs = knlohmann::json();
  terminationJs["Conduit Action"] = "Terminate";

  string terminationString = terminationJs.dump();
  size_t terminationStringSize = terminationString.size();

  if (isRoot())
  {
    for (size_t i = 0; i < _teamCount; i++)
      for (size_t j = 0; j < _workersPerTeam; j++)
      {
        MPI_Send(&terminationStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
        MPI_Send(terminationString.c_str(), terminationStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
      }
  }

  MPI_Barrier(_mpiCommunicator);
#endif

  Conduit::finalize();
}

void Distributed::workerThread()
{
#ifdef _KORALI_USE_MPI
  if (_teamIdSet == false) return;

  while (true)
  {
    size_t jsonStringSize;
    MPI_Recv(&jsonStringSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator, MPI_STATUS_IGNORE);

    char jsonStringChar[jsonStringSize + 1];
    MPI_Recv(jsonStringChar, jsonStringSize, MPI_CHAR, getRootRank(), MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);

    jsonStringChar[jsonStringSize] = '\0';
    auto actionJs = knlohmann::json::parse(jsonStringChar);

    if (actionJs["Conduit Action"] == "Terminate") return;
    if (actionJs["Conduit Action"] == "Process Sample") initiateSample(actionJs);
    if (actionJs["Conduit Action"] == "Broadcast Globals") _globals = actionJs;
    if (actionJs["Conduit Action"] == "Stack Engine") _engineStack.push(Engine::deserialize(actionJs["Engine"]));
    if (actionJs["Conduit Action"] == "Pop Engine") _engineStack.pop();

    MPI_Barrier(__KoraliTeamComm);
  }
#endif
}

void Distributed::processSample(Sample &sample)
{
#ifdef _KORALI_USE_MPI
  Engine *engine = _engineStack.top();

  int teamId = -1;

  if (isDefined(sample._js.getJson(), "Worker"))
  {
    teamId = sample["Worker"];
  }
  else
  {
    while (_teamQueue.empty())
    {
      sample._state = SampleState::waiting;
      co_switch(engine->_currentExperiment->_thread);
    }
    teamId = _teamQueue.front();
    _teamQueue.pop();
  }

  auto sampleJs = sample._js.getJson();
  sampleJs["Conduit Action"] = "Process Sample";

  string sampleJsonString = sampleJs.dump();
  size_t sampleJsonSize = sampleJsonString.size();

  for (size_t i = 0; i < _workersPerTeam; i++)
  {
    int workerId = _teamWorkers[teamId][i];
    MPI_Send(&sampleJsonSize, 1, MPI_UNSIGNED_LONG, workerId, MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
    MPI_Send(sampleJsonString.c_str(), sampleJsonSize, MPI_CHAR, workerId, MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
  }

  size_t resultJsonSize;
  MPI_Request resultJsonRequest;
  MPI_Irecv(&resultJsonSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[teamId][0], MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator, &resultJsonRequest);

  auto timelineJs = knlohmann::json();
  timelineJs["Start Time"] = chrono::duration<double>(chrono::high_resolution_clock::now() - _startTime).count() + _cumulativeTime;

  int flag = 0;
  while (flag == 0)
  {
    MPI_Test(&resultJsonRequest, &flag, MPI_STATUS_IGNORE);
    if (flag)
    {
      char resultStringChar[resultJsonSize + 1];
      MPI_Recv(resultStringChar, resultJsonSize, MPI_CHAR, _teamWorkers[teamId][0], MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);
      resultStringChar[resultJsonSize] = '\0';
      sample._js.getJson() = knlohmann::json::parse(resultStringChar);
      _teamQueue.push(teamId);
    }
    else
    {
      sample._state = SampleState::waiting;
      co_switch(engine->_currentExperiment->_thread);
    }
  }

  timelineJs["End Time"] = chrono::duration<double>(chrono::high_resolution_clock::now() - _startTime).count() + _cumulativeTime;
  timelineJs["Solver Id"] = engine->_currentExperiment->_experimentId;
  timelineJs["Current Generation"] = engine->_currentExperiment->_currentGeneration;
  __profiler["Timelines"]["Worker " + to_string(teamId)] += timelineJs;

#endif
}

size_t Distributed::maxConcurrency()
{
#ifdef _KORALI_USE_MPI
  return _teamCount;
#endif
}

void Distributed::broadcastGlobals(knlohmann::json &globalsJs)
{
#ifdef _KORALI_USE_MPI

  globalsJs["Conduit Action"] = "Broadcast Globals";
  string globalsString = globalsJs.dump();
  size_t globalsStringSize = globalsString.size();

  for (size_t i = 0; i < _teamCount; i++)
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      MPI_Send(&globalsStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
      MPI_Send(globalsString.c_str(), globalsStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
    }

#endif
}

size_t Distributed::getRootRank()
{
#ifdef _KORALI_USE_MPI
  return _rankCount - 1;
#endif

  return 0;
}

bool Distributed::isRoot()
{
#ifdef _KORALI_USE_MPI
  return _rankId == getRootRank();
#endif

  return true;
}

void Distributed::abort()
{
#ifdef _KORALI_USE_MPI
  MPI_Abort(_mpiCommunicator, -1);
#endif
}

void Distributed::stackEngine(Engine *engine)
{
#ifdef _KORALI_USE_MPI

  knlohmann::json engineJs;
  engineJs["Conduit Action"] = "Stack Engine";
  engine->serialize(engineJs["Engine"]);

  string engineString = engineJs.dump();
  size_t engineStringSize = engineString.size();

  for (size_t i = 0; i < _teamCount; i++)
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      MPI_Send(&engineStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
      MPI_Send(engineString.c_str(), engineStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
    }

#endif
}

void Distributed::popEngine()
{
#ifdef _KORALI_USE_MPI

  auto popJs = knlohmann::json();
  popJs["Conduit Action"] = "Pop Engine";
  string popString = popJs.dump();
  size_t popStringSize = popString.size();

  if (isRoot())
  {
    for (size_t i = 0; i < _teamCount; i++)
      for (size_t j = 0; j < _workersPerTeam; j++)
      {
        MPI_Send(&popStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_SIZE, _mpiCommunicator);
        MPI_Send(popString.c_str(), popStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_TAG_ACTION_JSON_CONTENT, _mpiCommunicator);
      }
  }

#endif
}

} // namespace conduit

} // namespace korali

#include "engine.hpp"
#include "modules/conduit/distributed/distributed.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/solver.hpp"
#include "sample/sample.hpp"

using namespace std;

@startNamespace0

#ifdef _KORALI_USE_MPI

  #define MPI_MESSAGE_JSON_SIZE 1
  #define MPI_MESSAGE_JSON 2

  MPI_Comm __KoraliTeamComm;
  MPI_Comm getKoraliMPIComm() { return __KoraliTeamComm; }
  long int getKoraliMPICommPointer() { return (long int)(&__KoraliTeamComm); }
#endif

@startNamespace1

void @className::initialize()
{
#ifndef _KORALI_USE_MPI
  KORALI_LOG_ERROR("Running an Distributed-based Korali application, but Korali was installed without support for Distributed.\n");
#else

  int isInitialized = 0;
  MPI_Initialized(&isInitialized);

  if (isInitialized == 0) MPI_Init(NULL, NULL);

  if (_communicator == 0)
    _mpiCommunicator = MPI_COMM_WORLD;
  else
  #ifdef OMPI_MPI_H
    _mpiCommunicator = *((MPI_Comm *)_communicator);
  #else
    _mpiCommunicator = (MPI_Comm)_communicator;
  #endif

  MPI_Comm_size(_mpiCommunicator, &_rankCount);
  MPI_Comm_rank(_mpiCommunicator, &_rankId);

  MPI_Barrier(_mpiCommunicator);
  _continueEvaluations = true;

  if (_rankCount == 1) KORALI_LOG_ERROR("Korali Distributed applications require at least 2 Distributed ranks to run.\n");

  int curWorker = 0;
  _workerCount = (_rankCount - 1) / _ranksPerWorker;
  _localRankId = 0;

  _workerIdSet = false;

  // Storage to map MPI ranks to their corresponding worker
  _workerTeams.resize(_workerCount);
  for (int i = 0; i < _workerCount; i++)
    _workerTeams[i].resize(_ranksPerWorker);

  // Storage to map workers to MPI ranks
  _rankToWorkerMap.resize(_rankCount);

  // Initializing available worker queue
  _workerQueue = queue<size_t>();
  while (!_workerQueue.empty()) _workerQueue.pop();

  // Putting workers in the queue
  for (int i = 0; i < _workerCount; i++)
    _workerQueue.push(i);

  // Now assigning ranks to workers and viceversa
  int currentRank = 0;
  for (int i = 0; i < _workerCount; i++)
    for (int j = 0; j < _ranksPerWorker; j++)
    {
      if (currentRank == _rankId)
      {
        curWorker = i;
        _localRankId = j;
        _workerIdSet = true;
      }

      _workerTeams[i][j] = currentRank;
      _rankToWorkerMap[currentRank] = i;
      currentRank++;
    }

  // If this is the root rank, check whether the number of ranks is correct
  if (isRoot())
  {
    int mpiSize;
    MPI_Comm_size(_mpiCommunicator, &mpiSize);

    if (_rankCount < _ranksPerWorker + 1)
      KORALI_LOG_ERROR("You are running Korali with %d ranks. However, you need at least %d ranks to have at least one worker team. \n", _rankCount, _ranksPerWorker + 1);

    curWorker = _workerCount + 1;
  }

  // Creating communicator
  MPI_Comm_split(_mpiCommunicator, curWorker, _rankId, &__KoraliTeamComm);

  // Waiting for all ranks to reach this point
  MPI_Barrier(_mpiCommunicator);
#endif
}

void @className::initServer()
{
#ifdef _KORALI_USE_MPI
  if (isRoot() == false && _workerIdSet == true) worker();
#endif
}

void @className::finalize()
{
#ifdef _KORALI_USE_MPI
  auto terminationJs = knlohmann::json();
  terminationJs["Conduit Action"] = "Terminate";

  string terminationString = terminationJs.dump();
  size_t terminationStringSize = terminationString.size();

  if (isRoot())
  {
    for (int i = 0; i < _workerCount; i++)
      for (int j = 0; j < _ranksPerWorker; j++)
        MPI_Send(terminationString.c_str(), terminationStringSize, MPI_CHAR, _workerTeams[i][j], MPI_MESSAGE_JSON, _mpiCommunicator);
  }

  MPI_Barrier(_mpiCommunicator);
#endif

  Conduit::finalize();
}

void @className::broadcastMessageToWorkers(knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  // Run broadcast only if this is the master process
  if (!isRoot()) return;

  string messageString = message.dump();
  size_t messageStringSize = messageString.size();

  for (int i = 0; i < _workerCount; i++)
    for (int j = 0; j < _ranksPerWorker; j++)
      MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, _workerTeams[i][j], MPI_MESSAGE_JSON, _mpiCommunicator);
#endif
}

int @className::getRootRank()
{
#ifdef _KORALI_USE_MPI
  return _rankCount - 1;
#endif

  return 0;
}

bool @className::isRoot()
{
#ifdef _KORALI_USE_MPI
  return _rankId == getRootRank();
#endif

  return true;
}

void @className::abort()
{
#ifdef _KORALI_USE_MPI
  MPI_Abort(_mpiCommunicator, -1);
#endif
}

void @className::sendMessageToEngine(knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  if (_localRankId == 0)
  {
    string messageString = message.dump();
    size_t messageStringSize = messageString.size();
    MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON, _mpiCommunicator);
  }
#endif
}

knlohmann::json @className::recvMessageFromEngine()
{
  auto message = knlohmann::json();

#ifdef _KORALI_USE_MPI
  MPI_Barrier(__KoraliTeamComm);

  MPI_Status status;
  MPI_Probe(getRootRank(), MPI_MESSAGE_JSON, _mpiCommunicator, &status);
  int messageSize = 0;
  MPI_Get_count(&status, MPI_CHAR, &messageSize);

  char *jsonStringChar = (char *)malloc(sizeof(char) * (messageSize + 1));
  MPI_Recv(jsonStringChar, messageSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON, _mpiCommunicator, MPI_STATUS_IGNORE);

  jsonStringChar[messageSize] = '\0';
  message = knlohmann::json::parse(jsonStringChar);
  free(jsonStringChar);
#endif

  return message;
}

void @className::listenWorkers()
{
#ifdef _KORALI_USE_MPI

  // Scanning all incoming messages
  int foundMessage = 0;

  // Reading pending messages from any worker
  MPI_Status status;
  MPI_Iprobe(MPI_ANY_SOURCE, MPI_MESSAGE_JSON, _mpiCommunicator, &foundMessage, &status);

  // If message found, receive it and storing in the corresponding sample's queue
  if (foundMessage == 1)
  {
    // Obtaining source rank, worker ID, and destination sample from the message
    int source = status.MPI_SOURCE;
    int worker = _rankToWorkerMap[source];
    auto sample = _workerToSampleMap[worker];

    // Receiving message from the worker
    int messageSize = 0;
    MPI_Get_count(&status, MPI_CHAR, &messageSize);
    char *messageStringChar = (char *)malloc(sizeof(char) * (messageSize + 1));
    MPI_Recv(messageStringChar, messageSize, MPI_CHAR, source, MPI_MESSAGE_JSON, _mpiCommunicator, MPI_STATUS_IGNORE);
    messageStringChar[messageSize] = '\0';
    auto message = knlohmann::json::parse(messageStringChar);
    free(messageStringChar);

    // Storing message in the sample message queue
    sample->_messageQueue.push(message);
  }

#endif
}

void @className::sendMessageToSample(Sample &sample, knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  string messageString = message.dump();
  size_t messageStringSize = messageString.size();

  for (int i = 0; i < _ranksPerWorker; i++)
  {
    int rankId = _workerTeams[sample._workerId][i];
    MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, rankId, MPI_MESSAGE_JSON, _mpiCommunicator);
  }
#endif
}

void @className::stackEngine(Engine *engine)
{
#ifdef _KORALI_USE_MPI
  // (Engine-Side) Adding engine to the stack to support Korali-in-Korali execution
  _engineStack.push(engine);

  knlohmann::json engineJs;
  engineJs["Conduit Action"] = "Stack Engine";
  engine->serialize(engineJs["Engine"]);

  broadcastMessageToWorkers(engineJs);
#endif
}

void @className::popEngine()
{
#ifdef _KORALI_USE_MPI
  // (Engine-Side) Removing the current engine to the conduit's engine stack
  _engineStack.pop();

  auto popJs = knlohmann::json();
  popJs["Conduit Action"] = "Pop Engine";
  broadcastMessageToWorkers(popJs);
#endif
}

size_t @className::getProcessId()
{
  return _rankId;
}

@moduleAutoCode

@endNamespace1

@endNamespace0


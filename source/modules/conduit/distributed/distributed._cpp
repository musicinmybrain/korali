#include "engine.hpp"
#include "modules/conduit/distributed/distributed.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/solver.hpp"
#include "sample/sample.hpp"

using namespace std;

namespace korali
{
#ifdef _KORALI_USE_MPI

  #define MPI_MESSAGE_JSON_SIZE 1
  #define MPI_MESSAGE_JSON_CONTENT 2

MPI_Comm __KoraliTeamComm;
MPI_Comm getKoraliMPIComm() { return __KoraliTeamComm; }
long int getKoraliMPICommPointer() { return (long int)(&__KoraliTeamComm); }
#endif

namespace conduit
{
void Distributed::initialize()
{
#ifdef _KORALI_USE_MPI
  _rankCount = 1;
  _rankId = 0;

  int isInitialized = 0;
  MPI_Initialized(&isInitialized);

  if (isInitialized == 0) MPI_Init(NULL, NULL);

  if (_communicator == 0)
    _mpiCommunicator = MPI_COMM_WORLD;
  else
  #ifdef OMPI_MPI_H
    _mpiCommunicator = *((MPI_Comm *)_communicator);
  #else
    _mpiCommunicator = (MPI_Comm)_communicator;
  #endif

  int mpiRankCount;
  int mpiRankId;
  MPI_Comm_size(_mpiCommunicator, &mpiRankCount);
  MPI_Comm_rank(_mpiCommunicator, &mpiRankId);
  _rankCount = mpiRankCount;
  _rankId = mpiRankId;
#endif

#ifndef _KORALI_USE_MPI
  KORALI_LOG_ERROR("Running an Distributed-based Korali application, but Korali was installed without support for Distributed.\n");
#endif

#ifdef _KORALI_USE_MPI
  MPI_Barrier(_mpiCommunicator);
  _continueEvaluations = true;

  if (_rankCount == 1) KORALI_LOG_ERROR("Korali Distributed applications require at least 2 Distributed ranks to run.\n");

  size_t teamId = 0;
  _teamCount = (_rankCount - 1) / _workersPerTeam;
  _localRankId = 0;

  _teamIdSet = false;
  size_t currentRank = 0;
  _teamWorkers.clear();
  _teamQueue = queue<size_t>();
  while (!_teamQueue.empty()) _teamQueue.pop();
  for (size_t i = 0; i < _teamCount; i++)
  {
    _teamQueue.push(i);
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      if (currentRank == _rankId)
      {
        teamId = i;
        _localRankId = j;
        _teamIdSet = true;
      }
      _teamWorkers[i].push_back(currentRank++);
    }
  }

  if (isRoot())
  {
    int mpiSize;
    MPI_Comm_size(_mpiCommunicator, &mpiSize);

    if (_rankCount < _workersPerTeam + 1)
      KORALI_LOG_ERROR("You are running Korali with %d ranks. However, you need at least %d ranks to have at least one worker team. \n", _rankCount, _workersPerTeam + 1);

    teamId = _teamCount + 1;
  }

  MPI_Comm_split(_mpiCommunicator, teamId, _rankId, &__KoraliTeamComm);
  MPI_Barrier(_mpiCommunicator);
#endif
}

void Distributed::initServer()
{
#ifdef _KORALI_USE_MPI
  if (isRoot() == false && _teamIdSet == true) workerThread();
#endif
}

void Distributed::finalize()
{
#ifdef _KORALI_USE_MPI
  auto terminationJs = knlohmann::json();
  terminationJs["Conduit Action"] = "Terminate";

  string terminationString = terminationJs.dump();
  size_t terminationStringSize = terminationString.size();

  if (isRoot())
  {
    for (size_t i = 0; i < _teamCount; i++)
      for (size_t j = 0; j < _workersPerTeam; j++)
      {
        MPI_Send(&terminationStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
        MPI_Send(terminationString.c_str(), terminationStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
      }
  }

  MPI_Barrier(_mpiCommunicator);
#endif

  Conduit::finalize();
}

void Distributed::workerThread()
{
#ifdef _KORALI_USE_MPI
  while (true)
  {
    auto actionJs = recvMessageFromEngine();

    if (actionJs["Conduit Action"] == "Terminate") return;
    if (actionJs["Conduit Action"] == "Process Sample")
    {
      Engine *engine = _engineStack.top();
      auto expId = actionJs["Experiment Id"];
      Sample s;
      s._globals = &engine->_experimentVector[expId]->_globals;
      s._js.getJson() = actionJs;
      s.sampleLauncher();

      sendMessageToEngine(s._js.getJson());
    }
    if (actionJs["Conduit Action"] == "Broadcast Globals")
    {
      Engine *engine = _engineStack.top();
      auto expId = actionJs["Experiment Id"];
      auto key = actionJs["Globals"]["Key"].get<std::string>();
      engine->_experimentVector[expId]->_globals[key] = actionJs["Globals"]["Values"];
    }
    if (actionJs["Conduit Action"] == "Stack Engine") _engineStack.push(Engine::deserialize(actionJs["Engine"]));
    if (actionJs["Conduit Action"] == "Pop Engine") _engineStack.pop();

    MPI_Barrier(__KoraliTeamComm);
  }
#endif
}

void Distributed::processSample(Sample &sample)
{
#ifdef _KORALI_USE_MPI
  Engine *engine = _engineStack.top();

  while (_teamQueue.empty())
  {
    sample._state = SampleState::waiting;
    co_switch(engine->_currentExperiment->_thread);
  }
  sample._workerId = _teamQueue.front();
  _teamQueue.pop();

  auto timelineJs = knlohmann::json();
  timelineJs["Start Time"] = chrono::duration<double>(chrono::high_resolution_clock::now() - _startTime).count() + _cumulativeTime;

  // Sending sample information to worker
  auto sampleJs = sample._js.getJson();
  sampleJs["Conduit Action"] = "Process Sample";
  sendMessageToSample(sample, sampleJs);

  // Receiving sample information from worker
  sample._js.getJson() = recvMessageFromSample(sample);
  _teamQueue.push(sample._workerId);

  timelineJs["End Time"] = chrono::duration<double>(chrono::high_resolution_clock::now() - _startTime).count() + _cumulativeTime;
  timelineJs["Solver Id"] = engine->_currentExperiment->_experimentId;
  timelineJs["Current Generation"] = engine->_currentExperiment->_currentGeneration;
  __profiler["Timelines"]["Worker " + to_string(sample._workerId)] += timelineJs;

#endif
}

void Distributed::broadcastMessageToWorkers(knlohmann::json& message)
{
 // Run broadcast only if this is the master process
 if (!isRoot()) return;

 string messageString = message.dump();
 size_t messageStringSize = messageString.size();

 for (size_t i = 0; i < _teamCount; i++)
   for (size_t j = 0; j < _workersPerTeam; j++)
   {
     MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[i][j], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
     MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, _teamWorkers[i][j], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
   }
}

size_t Distributed::getRootRank()
{
#ifdef _KORALI_USE_MPI
  return _rankCount - 1;
#endif

  return 0;
}

bool Distributed::isRoot()
{
#ifdef _KORALI_USE_MPI
  return _rankId == getRootRank();
#endif

  return true;
}

void Distributed::abort()
{
#ifdef _KORALI_USE_MPI
  MPI_Abort(_mpiCommunicator, -1);
#endif
}

void Distributed::sendMessageToEngine(knlohmann::json& message)
{
 if (_localRankId == 0)
 {
   string messageString = message.dump();
   size_t messageStringSize = messageString.size();
   MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
   MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
 }
}

knlohmann::json Distributed::recvMessageFromEngine()
{
 size_t jsonStringSize;
 MPI_Recv(&jsonStringSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_MESSAGE_JSON_SIZE, _mpiCommunicator, MPI_STATUS_IGNORE);

 char jsonStringChar[jsonStringSize + 1];
 MPI_Recv(jsonStringChar, jsonStringSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);

 jsonStringChar[jsonStringSize] = '\0';
 auto message = knlohmann::json::parse(jsonStringChar);

 return message;
}

knlohmann::json Distributed::recvMessageFromSample(Sample& sample)
{
 Engine *engine = _engineStack.top();

 size_t messageSize;
 MPI_Request messageRequest;
 MPI_Irecv(&messageSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[sample._workerId][0], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator, &messageRequest);
 knlohmann::json message;

 int flag = 0;
 while (flag == 0)
 {
   MPI_Test(&messageRequest, &flag, MPI_STATUS_IGNORE);
   if (flag)
   {
     char messageStringChar[messageSize + 1];
     MPI_Recv(messageStringChar, messageSize, MPI_CHAR, _teamWorkers[sample._workerId][0], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);
     messageStringChar[messageSize] = '\0';
     message = knlohmann::json::parse(messageStringChar);
   }
   else
   {
     sample._state = SampleState::waiting;
     co_switch(engine->_currentExperiment->_thread);
   }
 }

 return message;
}

void Distributed::sendMessageToSample(Sample& sample, knlohmann::json& message)
{
 string messageString = message.dump();
 size_t messageStringSize = messageString.size();

 for (size_t i = 0; i < _workersPerTeam; i++)
 {
   int workerId = _teamWorkers[sample._workerId][i];
   MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, workerId, MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
   MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, workerId, MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
 }
}

} // namespace conduit

} // namespace korali

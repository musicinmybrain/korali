#include "engine.hpp"
#include "modules/conduit/distributed/distributed.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/solver.hpp"
#include "sample/sample.hpp"

using namespace std;

namespace korali
{
#ifdef _KORALI_USE_MPI

  #define MPI_MESSAGE_JSON_SIZE 1
  #define MPI_MESSAGE_JSON_CONTENT 2

MPI_Comm __KoraliTeamComm;
MPI_Comm getKoraliMPIComm() { return __KoraliTeamComm; }
long int getKoraliMPICommPointer() { return (long int)(&__KoraliTeamComm); }
#endif

namespace conduit
{
void Distributed::initialize()
{
#ifdef _KORALI_USE_MPI
  _rankCount = 1;
  _rankId = 0;

  int isInitialized = 0;
  MPI_Initialized(&isInitialized);

  if (isInitialized == 0) MPI_Init(NULL, NULL);

  if (_communicator == 0)
    _mpiCommunicator = MPI_COMM_WORLD;
  else
  #ifdef OMPI_MPI_H
    _mpiCommunicator = *((MPI_Comm *)_communicator);
  #else
    _mpiCommunicator = (MPI_Comm)_communicator;
  #endif

  int mpiRankCount;
  int mpiRankId;
  MPI_Comm_size(_mpiCommunicator, &mpiRankCount);
  MPI_Comm_rank(_mpiCommunicator, &mpiRankId);
  _rankCount = mpiRankCount;
  _rankId = mpiRankId;
#endif

#ifndef _KORALI_USE_MPI
  KORALI_LOG_ERROR("Running an Distributed-based Korali application, but Korali was installed without support for Distributed.\n");
#endif

#ifdef _KORALI_USE_MPI
  MPI_Barrier(_mpiCommunicator);
  _continueEvaluations = true;

  if (_rankCount == 1) KORALI_LOG_ERROR("Korali Distributed applications require at least 2 Distributed ranks to run.\n");

  size_t teamId = 0;
  _teamCount = (_rankCount - 1) / _workersPerTeam;
  _localRankId = 0;

  _teamIdSet = false;
  size_t currentRank = 0;
  _workerProcesses.clear();
  _workerQueue = queue<size_t>();
  while (!_workerQueue.empty()) _workerQueue.pop();
  for (size_t i = 0; i < _teamCount; i++)
  {
    _workerQueue.push(i);
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      if (currentRank == _rankId)
      {
        teamId = i;
        _localRankId = j;
        _teamIdSet = true;
      }
      _workerProcesses[i].push_back(currentRank++);
    }
  }

  if (isRoot())
  {
    int mpiSize;
    MPI_Comm_size(_mpiCommunicator, &mpiSize);

    if (_rankCount < _workersPerTeam + 1)
      KORALI_LOG_ERROR("You are running Korali with %d ranks. However, you need at least %d ranks to have at least one worker team. \n", _rankCount, _workersPerTeam + 1);

    teamId = _teamCount + 1;
  }

  MPI_Comm_split(_mpiCommunicator, teamId, _rankId, &__KoraliTeamComm);
  MPI_Barrier(_mpiCommunicator);
#endif
}

void Distributed::initServer()
{
#ifdef _KORALI_USE_MPI
  if (isRoot() == false && _teamIdSet == true) worker();
#endif
}

void Distributed::finalize()
{
#ifdef _KORALI_USE_MPI
  auto terminationJs = knlohmann::json();
  terminationJs["Conduit Action"] = "Terminate";

  string terminationString = terminationJs.dump();
  size_t terminationStringSize = terminationString.size();

  if (isRoot())
  {
    for (size_t i = 0; i < _teamCount; i++)
      for (size_t j = 0; j < _workersPerTeam; j++)
      {
        MPI_Send(&terminationStringSize, 1, MPI_UNSIGNED_LONG, _workerProcesses[i][j], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
        MPI_Send(terminationString.c_str(), terminationStringSize, MPI_CHAR, _workerProcesses[i][j], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
      }
  }

  MPI_Barrier(_mpiCommunicator);
#endif

  Conduit::finalize();
}

void Distributed::broadcastMessageToWorkers(knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  // Run broadcast only if this is the master process
  if (!isRoot()) return;

  string messageString = message.dump();
  size_t messageStringSize = messageString.size();

  for (size_t i = 0; i < _teamCount; i++)
    for (size_t j = 0; j < _workersPerTeam; j++)
    {
      MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, _workerProcesses[i][j], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
      MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, _workerProcesses[i][j], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
    }
#endif
}

size_t Distributed::getRootRank()
{
#ifdef _KORALI_USE_MPI
  return _rankCount - 1;
#endif

  return 0;
}

bool Distributed::isRoot()
{
#ifdef _KORALI_USE_MPI
  return _rankId == getRootRank();
#endif

  return true;
}

void Distributed::abort()
{
#ifdef _KORALI_USE_MPI
  MPI_Abort(_mpiCommunicator, -1);
#endif
}

void Distributed::sendMessageToEngine(knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  if (_localRankId == 0)
  {
    string messageString = message.dump();
    size_t messageStringSize = messageString.size();
    MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
    MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
  }
#endif
}

knlohmann::json Distributed::recvMessageFromEngine()
{
  auto message = knlohmann::json();

#ifdef _KORALI_USE_MPI
  MPI_Barrier(__KoraliTeamComm);

  size_t jsonStringSize;
  MPI_Recv(&jsonStringSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_MESSAGE_JSON_SIZE, _mpiCommunicator, MPI_STATUS_IGNORE);

  char jsonStringChar[jsonStringSize + 1];
  MPI_Recv(jsonStringChar, jsonStringSize, MPI_CHAR, getRootRank(), MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);

  jsonStringChar[jsonStringSize] = '\0';
  message = knlohmann::json::parse(jsonStringChar);
#endif

  return message;
}

knlohmann::json Distributed::recvMessageFromSample(Sample &sample)
{
  knlohmann::json message;

#ifdef _KORALI_USE_MPI

  Engine *engine = _engineStack.top();
  size_t messageSize;
  MPI_Request messageRequest;
  MPI_Irecv(&messageSize, 1, MPI_UNSIGNED_LONG, _workerProcesses[sample._workerId][0], MPI_MESSAGE_JSON_SIZE, _mpiCommunicator, &messageRequest);

  int flag = 0;
  while (flag == 0)
  {
    MPI_Test(&messageRequest, &flag, MPI_STATUS_IGNORE);
    if (flag)
    {
      char messageStringChar[messageSize + 1];
      MPI_Recv(messageStringChar, messageSize, MPI_CHAR, _workerProcesses[sample._workerId][0], MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator, MPI_STATUS_IGNORE);
      messageStringChar[messageSize] = '\0';
      message = knlohmann::json::parse(messageStringChar);
    }
    else
    {
      co_switch(engine->_currentExperiment->_thread);
    }
  }

#endif

  return message;
}

void Distributed::sendMessageToSample(Sample &sample, knlohmann::json &message)
{
#ifdef _KORALI_USE_MPI
  string messageString = message.dump();
  size_t messageStringSize = messageString.size();

  for (size_t i = 0; i < _workersPerTeam; i++)
  {
    int workerId = _workerProcesses[sample._workerId][i];
    MPI_Send(&messageStringSize, 1, MPI_UNSIGNED_LONG, workerId, MPI_MESSAGE_JSON_SIZE, _mpiCommunicator);
    MPI_Send(messageString.c_str(), messageStringSize, MPI_CHAR, workerId, MPI_MESSAGE_JSON_CONTENT, _mpiCommunicator);
  }
#endif
}

void Distributed::stackEngine(Engine *engine)
{
#ifdef _KORALI_USE_MPI
  // (Engine-Side) Adding engine to the stack to support Korali-in-Korali execution
  _engineStack.push(engine);

  knlohmann::json engineJs;
  engineJs["Conduit Action"] = "Stack Engine";
  engine->serialize(engineJs["Engine"]);

  broadcastMessageToWorkers(engineJs);
#endif
}

void Distributed::popEngine()
{
#ifdef _KORALI_USE_MPI
  // (Engine-Side) Removing the current engine to the conduit's engine stack
  _engineStack.pop();

  auto popJs = knlohmann::json();
  popJs["Conduit Action"] = "Pop Engine";
  broadcastMessageToWorkers(popJs);
#endif
}

} // namespace conduit

} // namespace korali

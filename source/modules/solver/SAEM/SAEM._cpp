#include "modules/solver/SAEM/SAEM.hpp"
#include "modules/problem/problem.hpp"
#include "modules/engine/engine.hpp"
#include "sample/sample.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/conduit/conduit.hpp"

#include <string>
#include <stdio.h>
#include <vector>
#include <cmath>
#include <numeric>

void korali::solver::SAEM::initialize()
{

  if( _k->_problem->getType() != "Bayesian/Latent")
    korali::logError("SAEM can only optimize problems of type 'Bayesian/Latent' .\n");

   _latentProblem = dynamic_cast<korali::problem::bayesian::Latent*>(_k->_problem); // .get<korali::problem::bayesian::Latent>()

  _numberVariables = _k->_variables.size();
  _numberLatent = _latentProblem->_latentVariableIndices.size();
  _numberHyperparameters = _numberVariables - _numberLatent;

  if (_k->_currentGeneration > 0) return;

  for (size_t i = 0; i < _numberVariables; i++)
    if( std::isfinite(_k->_variables[i]->_initialValue) == false )
      korali::logError("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  if (_latentVariableSampler){
    _hasUserDefinedSampler = true;}
  else _hasUserDefinedSampler = false;
    //  _hasUserDefinedSampler = (_latentVariableSampler != 0); // Todo: not sure if this is the right way to check for not-passed sampler



  _currentSamplesSVectors.resize(_numberMarkovChainSamples);
  _currentS.resize(_numberLatent);
  _previousS.resize(_numberLatent);
  std::fill(_currentS.begin(), _currentS.end(), 0);
  std::fill(_previousS.begin(), _previousS.end(), -1); // something that is not 0, just in case / to detect if that is used anywhere

  /* */

  // set starting values initial samples (Probably don't need this)
 /* _currentSamples.resize( _numberMarkovChainSamples);
  for (size_t j = 0; j < _numberMarkovChainSamples; j++){
    _currentSamples[j].resize( _numberLatent);
      for (size_t i = 0; i < _numberLatent; i++){
          size_t idx = _latentProblem->_latentVariableIndices[i];
          _currentSamples[j][i] = _k->_variables[idx]->_initialValue;    // _k->_variables: vector of korali-variables* (std::vector<korali::Variable*>).
      }
  }*/

  // set initial "sample means"
   _previousLatentSampleMeans.resize(_numberLatent);
  for (size_t i = 0; i < _numberLatent; i++){
          size_t idx = _latentProblem->_latentVariableIndices[i];
          _previousLatentSampleMeans[i] = _k->_variables[idx]->_initialValue;
      }
    
   // Set starting values for hyperparameters
   _currentHyperparameters.resize(_numberHyperparameters);
   for (size_t i = 0; i < _numberVariables; i++){
   /*	 // check whether i is a latent variable index; if yes, continue
  	  if(std::find( _latentProblem->_latentVariableIndices.begin(),
  	  		_latentProblem->_latentVariableIndices.end(), i) != _latentProblem->_latentVariableIndices.end())*/
  	  if (isLatent(i))
			continue;      
      _currentHyperparameters[i] = _k->_variables[i]->_initialValue;
  }
  
  
  // _bestSamples = _currentSamples;
  //_delta.resize( N, _delta0 );
  //_currentGradient.resize( N, 0);
  //_previousGradient.resize( N, 0.0 );

  _bestEvaluation = korali::Inf;
  //_xDiff = korali::Inf;
  //_maxStallCounter = 0;
  //_normPreviousGradient = korali::Inf;
  //_previousEvaluation   = korali::Inf;
}



void korali::solver::SAEM::runGeneration( void )
{

  if (_k->_currentGeneration > 0) initialize();

  /* E1: Sample latent variable values */
  sampleLatent();
  std::cout << "Sampled; generation: " << _k->_currentGeneration << std::endl;

  /* E2: Update posterior probability function Q */
  calculateSampleSVectors();
  updateS();

  /* M:  Find argmax Q(theta) */

  // Define the function to be optimized
  //  (needs to set: "F(x)", uses "Parameters")
  updateQFunction();

  // Set up a korali experiment and optimize the function
  updateHyperparameters();

  /* * * * * * */



/*   evaluateFunctionAndGradient( );

  korali::logInfo("Normal","X = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_currentX[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","F(X) = %le \n", _currentEvaluation );

  korali::logInfo("Normal","DF(X) = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_currentGradient[k]);
  korali::logData("Normal"," ]\n");

  korali::logInfo("Normal","X_best = [ ");
  for(size_t k=0; k<N; k++) korali::logData("Normal"," %.5le  ",_bestX[k]);
  korali::logData("Normal"," ]\n");

  Update_iminus();

  _previousEvaluation   = _currentEvaluation;
  _previousGradient     = _currentGradient;
  _normPreviousGradient = vectorNorm(_previousGradient);

  if( _currentEvaluation < _bestEvaluation ){
    _bestEvaluation = _currentEvaluation;
    std::vector<double> tmp(N);
    for(size_t j=0; j<N; j++) tmp[j] = _bestX[j]-_currentX[j];
    _xDiff = vectorNorm( tmp );
    _bestX = _currentX;
    _maxStallCounter = 0;
  }
  else{
    _maxStallCounter++;
  } */

}




void korali::solver::SAEM::sampleLatent(){
    // If no sampling function is given, create a sampling experiment, run it and get the results.

    if (_hasUserDefinedSampler) {
      //for(size_t i=0; i< _numberMarkovChainSamples; i++){
          korali::Sample sample;
          sample["Hyperparameters"] = _currentHyperparameters;
          sample["Number Samples"] = _numberMarkovChainSamples;
          sample["Number Of Latent Variables"] = _numberLatent;
          sample.run(_latentVariableSampler); // Like in: problem/reference.cpp, def of evaluateLogLikelihood()
          std::vector<std::vector<double>> v =  sample["Samples"].get<std::vector<std::vector<double>>>();
          if (v.size() != _numberMarkovChainSamples) korali::logError("User defined sampler did not return the correct number of samples ('Number Samples').");
          _currentSamples = v; //.push_back(v);
          //}
    }
    else {
      runBuiltinSampler();
    }


}



void korali::solver::SAEM::runBuiltinSampler()
{

    /*
     * probability to sample from:
     * p(d, z | theta) * p(theta) -- that's the (log-)posterior
     * - use a "Custom" bayesian problem, with our log-posterior as "Likelihood Model" , but with the current values for theta inserted.
    */

    /* Create one sampling experiment to sample all latent variables. After all the latent vars are correlated / have a joint distrib.
        Todo: does it make sense to re-create these experiments at every E-M step? Otherwise, how
            to automatically update the initial mean and the distribution function of the sampling experiments?*/

         auto k = korali::Engine();
         auto e = korali::Experiment();
         //auto p = heat2DInit(&argc, &argv);

         // Based on tutorial a2-sampling
         e["Problem"]["Type"] = "Direct/Basic";
         e["Problem"]["Objective Function"] = [params=_currentHyperparameters, this](korali::Sample& s) -> void {
                        if (! s.contains("Latent Variables")){
                            korali::logError("You try to evaluate the likelihood without passing values for the latent variables to the sample.\n");
                        }
                        s["Hyperparameters"] = params; // _currentHyperparameters;
                        // Ugly? & Probably doesnt work
                        _latentProblem->evaluateLogLikelihood(s);
                         s["F(x)"] = s["logLikelihood"];
                    };

        for (size_t i=0; i < _numberLatent; i++){

             size_t idx = _latentProblem->_latentVariableIndices[i];
             std::string varName = _k->_variables[idx]->_name;

             if (_k->_currentGeneration == 0){
                _previousSampleMean =  _previousLatentSampleMeans[i]; // TODO: Check, do I need a vector of vectors instead? Edit: No.
             } else {
                _previousSampleMean = _k->_variables[idx]->_initialValue;
             }
            // Defining problem's variables
            e["Variables"][i]["Name"] = varName;
            e["Variables"][i]["Initial Mean"] = _previousSampleMean;
            e["Variables"][i]["Initial Standard Deviation"] = 1.0;
        }

        // Configuring the MCMC sampler parameters
        e["Solver"]["Type"]  = "Sampler/MCMC";
        e["Solver"]["Burn In"] = 500;
        e["Solver"]["Termination Criteria"]["Max Samples"] = 5000;

        // Configuring output settings
        e["Results"]["Frequency"] = 500;
        e["Console"]["Frequency"] = 500;
        e["Console"]["Verbosity"] = "Detailed";

        // Todo: I don't think a result path is needed (and it'd need a step id in the pathname as well)
        //e["Results"]["Path"] = "setup/results_phase_1/" + "0"*(3 - str(i).length()) +  std:to_string(i);
        k.run(e);

        std::vector<std::vector<double>> db = e["Solver"]["Sample Database"].get<std::vector<std::vector<double>>>();
        printf("Database size: %lu\n", db.size());
        /*for (size_t i = 0; i < db.size(); i++)
        {
        printf("[ ");
        for (size_t j = 0; j < db[i].size(); j++)
        printf("%f, ", db[i][j]);
        printf("]\n");
        }*/
        // TODO: modify this
        std::vector<std::vector<double>>::const_iterator first = db.end() - _numberMarkovChainSamples;
        std::vector<std::vector<double>>::const_iterator last = db.end();
        std::vector<std::vector<double>> samples(first, last);

        _currentSamples = samples;

        k["Samples"] = samples;

        // set new "previous sample means"
        for(size_t i= 0; i< _numberLatent; i++){
            double sum = 0;
            for(size_t j = 0; j < _numberMarkovChainSamples; j++) {
                sum += samples[j][i];
                }
            _previousLatentSampleMeans[i] = sum / static_cast<double>(_numberLatent);
        }

}

/*void korali::solver::optimizer::Rprop::evaluateFunctionAndGradient( void )
{
  int Ns = 1;
  // Initializing Sample Evaluation
  std::vector<korali::Sample> samples(Ns);
  for (size_t i = 0; i < Ns; i++){
    samples[i]["Operation"]  = "Basic Evaluation";
    samples[i]["Parameters"] = _currentX;
    samples[i]["Sample Id"]  = i;
    _modelEvaluationCount++;
    korali::_conduit->start(samples[i]);
  }

  // Waiting for samples to finish
  korali::_conduit->waitAll(samples);

  // Processing results
  // The 'minus' is there because we want Rprop to do Maximization be default.
  for (size_t i = 0; i < Ns; i++){
    _currentEvaluation = samples[i]["Evaluation"];
    _currentEvaluation = -_currentEvaluation;
    for( size_t j=0; j<N; j++){
      _currentGradient[j] = samples[i]["Gradient"][j];
      _currentGradient[j] = -_currentGradient[j];
    }
  }
}*/



void korali::solver::SAEM::calculateSampleSVectors(){
    /* Call this after sampling; sets _currentSamplesSVectors using each current sample of latent variables
     and the current hyperparameter values

     Uses:
            _currentSamples,
            _currentHyperparameters,
            the S function of _latentProblem
     Sets:
            _currentSamplesSVectors
     */
      for(size_t i=0; i<_numberMarkovChainSamples; i++){

    korali::Sample k;
    k["Latent Variables"] = _currentSamples[i];
    k["Hyperparameters"] = _currentHyperparameters;
    k.run(_latentProblem->_sOfLikelihoodModel);
    //_latentProblem->_sOfLikelihoodModel(k);

    std::vector<double> v = k["S"];
    _currentSamplesSVectors[i] = v;
    return;
  }
}


void korali::solver::SAEM::updateS(){
    /*  Robbins-Monro-update our estimate of S.

     Uses:
            _currentSamplesSValues
            _currentS
            _k->_currentGeneration
     Sets:
           _previousS <-- _currentS
           _currentS
     */
     _previousS = _currentS;

     // Determine alpha
     double alpha;
     if(_k->_currentGeneration > _numberInitialSteps)
        alpha = _alpha2;
     else
        alpha = _alpha1;

     // --> decay factor gamma
     double curGen = static_cast<double>(_k->_currentGeneration);
     double gamma = std::pow( curGen, - alpha);

     // Calculate mean S of our current samples
     std::vector<double> sumVec(_numberLatent, 0);
     std::vector<double> meanSampleS(_numberLatent, 0);

     for(size_t j = 0; j < _numberMarkovChainSamples; j++){
        for(size_t i = 0; i < _numberLatent; i++) {
            sumVec[i] += _currentSamplesSVectors[j][i];
        }
     }
     for(size_t i = 0; i < _numberLatent; i++) {
        meanSampleS[i] += sumVec[i] / _numberMarkovChainSamples;
      }

      // Now do the Robbins-Monro update
     if (_k->_currentGeneration == 0)
        _currentS = meanSampleS;
     else {
         for(size_t i = 0; i < _numberLatent; i++) {
             _currentS[i] =   _previousS[i]  + gamma * (meanSampleS[i] - _previousS[i]);
         }
     }
     return;
}

void korali::solver::SAEM::updateQFunction(){
         /* Re-defines the function to be optimized in the M-step.
            Uses:
                _currentS
                zeta and phi functions of _latentProblem
            Sets:
                _currentQ
         */
         _currentQ = [S=_currentS, _latentProblem=_latentProblem](korali::Sample& s) -> void {
                        if (! s.contains("Parameters")){
                            korali::logError("Implementation error.\n");
                        }
                        s["Hyperparameters"] = s["Parameters"];
                        s.run(_latentProblem->_zetaOfLikelihoodModel);
                        s.run(_latentProblem->_phiOfLikelihoodModel);
                        double zeta = s["zeta"];
                        std::vector<double> phi = s["phi"];
                        if (phi.size() != S.size()){
                            korali::logError("Implementation error. Current Robbins-Monro-averaged vector S did not have the same length as vector phi from the problem definition (from the model). \n");
                        }
                         s["F(x)"] = -zeta + std::inner_product(std::begin(S), std::end(S),
                                                                                    std::begin(phi), 0.0);
                    };

}

void korali::solver::SAEM::updateHyperparameters(){
    /* Set up a korali experiment and optimize the current Q function to get a new vector of hyperparameters. */
    auto k = korali::Engine();
    auto e = korali::Experiment();

    e["Random Seed"] = 0x2A;
    e["Problem"]["Type"] = "Optimization/Stochastic";
    e["Problem"]["Objective Function"] = _currentQ; //&_currentQ;

    // The variables are our hyperparameters
    int hyperparamIdx = 0;
    for(size_t i = 0; i < _numberVariables; i++){
        if(isLatent(i))
            continue;
        auto var =  _k->_variables[i];
        if (var->_lowerBound >= var->_upperBound)
            korali::logError("Lower bound was equal to or higher than upper bound for a hyperparameter. Did you perhaps forget to set upper and lower bounds for this hyperparamter?");
        e["Variables"][hyperparamIdx]["Name"] = var->_name;
        e["Variables"][hyperparamIdx]["Lower Bound"] = var->_lowerBound;
        e["Variables"][hyperparamIdx]["Upper Bound"] = var->_upperBound;
        hyperparamIdx++;
    }

   // configure the solver
   if (_mStepSolverType != "CMAES")
        korali::logError("Currently, only using a CMAES solver is implemented");
   e["Solver"]["Type"] = "CMAES";
   e["Solver"]["Population Size"] = 4;
   e["Solver"]["Termination Criteria"]["Min Value Difference Threshold"] = 1e-15;
   e["Solver"]["Termination Criteria"]["Max Generations"] = 100;
   k.run(e);

   // Now access the results! How?
    std::cout << "line to set a breakpoint, nothing else" << "\n";


}


bool korali::solver::SAEM::isLatent(int idx){
    /* checks whether the variable at index idx, i.e., k->_variables[idx], is latent or not*/
    return _k->_variables[idx]->_bayesianType == "Latent";
    // Alternative:
    //return std::find( _latentProblem->_latentVariableIndices.begin(),
  	//  		_latentProblem->_latentVariableIndices.end(), i) != _latentProblem->_latentVariableIndices.end())
    }


void korali::solver::SAEM::printGenerationBefore(){ return; }

void korali::solver::SAEM::printGenerationAfter() { return; }

void korali::solver::SAEM::finalize()  { return; }


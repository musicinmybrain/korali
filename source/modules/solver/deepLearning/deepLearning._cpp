#include "modules/solver/deepLearning/deepLearning.hpp"
#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"

void korali::solver::DeepLearning::initialize()
{
 // Getting problem pointer
 _problem = dynamic_cast<korali::problem::Learning *>(_k->_problem);

 if (_problem->_trainingBatchSize == 0) _k->_logger->logError("Training data has not been provided for variable 0.\n");
 if (_problem->_validationBatchSize == 0) _k->_logger->logError("Validation data has not been provided for variable 0.\n");

 // Setting training NN
 _trainingNeuralNetwork = dynamic_cast<korali::NeuralNetwork*>(korali::Module::duplicate(_neuralNetwork));
 _validationNeuralNetwork = dynamic_cast<korali::NeuralNetwork*>(korali::Module::duplicate(_neuralNetwork));
 _testNeuralNetwork = dynamic_cast<korali::NeuralNetwork*>(korali::Module::duplicate(_neuralNetwork));

 // Setting input training data
 _trainingNeuralNetwork->_layers[0]->_nodeValues.resize(_problem->_trainingBatchSize);
 for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
 {
  _trainingNeuralNetwork->_layers[0]->_nodeValues[i].resize(_problem->_inputVectorSize);
  for (size_t j = 0; j < _problem->_inputVectorSize; j++)
  {
    size_t varIdx = _problem->_inputVectorIndexes[j];
   _trainingNeuralNetwork->_layers[0]->_nodeValues[i][j] = _k->_variables[varIdx]->_trainingData[i];
  }
 }

 // Setting input validation data
 _validationNeuralNetwork->_layers[0]->_nodeValues.resize(_problem->_validationBatchSize);
 for (size_t i = 0; i < _problem->_validationBatchSize; i++)
 {
  _validationNeuralNetwork->_layers[0]->_nodeValues[i].resize(_problem->_inputVectorSize);
  for (size_t j = 0; j < _problem->_inputVectorSize; j++)
  {
    size_t varIdx = _problem->_inputVectorIndexes[j];
    _validationNeuralNetwork->_layers[0]->_nodeValues[i][j] = _k->_variables[varIdx]->_validationData[i];
  }
 }

 // Setting Solution for NN training and validation
 _trainingNeuralNetwork->_solution.resize(_problem->_trainingBatchSize);
 for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
 {
  _trainingNeuralNetwork->_solution[i].resize(_problem->_outputVectorSize);
  for (size_t j = 0; j < _problem->_outputVectorSize; j++)
  {
    size_t varIdx = _problem->_outputVectorIndexes[j];
    _trainingNeuralNetwork->_solution[i][j] = _k->_variables[varIdx]->_trainingData[i];
  }
 }

 _validationNeuralNetwork->_solution =  _trainingNeuralNetwork->_solution;

 // Creating Neural Networks internal structures
 _trainingNeuralNetwork->create();
 _validationNeuralNetwork->create();
}

void korali::solver::DeepLearning::runGeneration()
{
 /**************************************************************
  * Training Stage
  *************************************************************/

 _trainingNeuralNetwork->train();

 /**************************************************************
  * Validation Stage
  *************************************************************/

 // Setting input validation data
 korali::NeuralNetwork::runSample(_trainingNeuralNetwork->_bestSample, _validationNeuralNetwork);

 // Getting results of optimization
 _currentValidationLoss = - _trainingNeuralNetwork->_bestSample["F(x)"].get<double>();

 // If validation is better, saving it as the best network for use as for test batch later.
 _currentInactiveSteps++;
 if (_currentValidationLoss < _lowestValidationLoss)
 {
  // Reseting inactive counter
  _currentInactiveSteps = 0;

  // Saving lowest validation loss
  _lowestValidationLoss = _currentValidationLoss;

  // Storing best NN
  knlohmann::json js;
  _validationNeuralNetwork->getConfiguration(js);
  _testNeuralNetwork->_layers.clear();
  _testNeuralNetwork->setConfiguration(js);
 }

 // Printing results so far
 _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _trainingNeuralNetwork->_currentTrainingLoss);
 _k->_logger->logInfo("Normal", "Current Validation Loss: %.15f\n", _currentValidationLoss);
 _k->_logger->logInfo("Normal", "Lowest Validation Loss: %.15f\n", _lowestValidationLoss);
 _k->_logger->logInfo("Normal", "Inactive Step Counter: %lu\n", _currentInactiveSteps);
}

std::vector<std::vector<double>> korali::solver::DeepLearning::test(const std::vector<std::vector<double>>& inputBatch)
{
 _testNeuralNetwork->_layers[0]->_nodeValues = inputBatch;
 _testNeuralNetwork->create();

 // Updating the network's weights and biases
 _testNeuralNetwork->update();

 // Running the input values through the neural network
 _testNeuralNetwork->forward();

 // Printing Layer values
 size_t outputLayerId = _testNeuralNetwork->_layers.size()-1;
 size_t batchSize = _testNeuralNetwork->_layers[outputLayerId]->_nodeValues.size();
 size_t outputSize = _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[0].size();

// printf("Results:\n");
// for (size_t i = 0; i < batchSize; i++)
// {
//  printf("Input %lu - ( %f", i, _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[i][0]);
//  for (size_t j = 1; j < outputSize; j++)
//   printf(", %f", _testNeuralNetwork->_layers[outputLayerId]->_nodeValues[i][j]);
//  printf(")\n");
// }

 return _testNeuralNetwork->_layers[outputLayerId]->_nodeValues;
}


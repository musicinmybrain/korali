#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepGD/deepGD.hpp"

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepGD::initialize()
{
  // If the learner is alread initialized, do not do anything else
  if (_isLearnerInitialized == false)
  {
    // Getting problem pointer
    _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

    /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

    // If we use normalization, inform the underlying NN
    _neuralNetwork["Batch Normalization"]["Enabled"] = _batchNormalizationEnabled;

    // Setting the number of input layer nodes as number of input vector size
    _neuralNetwork["Layers"][0]["Node Count"] = _problem->_inputVectorSize;

    // Setting the number of output layer nodes as number of output vector size
    size_t outputLayerId = _neuralNetwork["Layers"].size() - 1;
    _neuralNetwork["Layers"][outputLayerId]["Node Count"] = _problem->_outputVectorSize;

    // Creating training neural network, setting its batch size as the number of inputs to the problem
    auto neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Batch Size"] = _problem->_inputs.size();
    _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    // Creating inference neural network, setting its batch size as one (evaluation per call) and following the global (mean/variance) stats of the trained NN
    neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Batch Size"] = 1;
    neuralNetworkConfig["Batch Normalization"]["Use Global Stats"] = true;
    _evaluationNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

    // Creating evaluation lambda function for optimization
    auto fc = [this, nn = &_trainingNeuralNetwork, out = &_problem->_outputs](Sample &sample) { this->evaluateHyperparameters(sample, *nn, *out, true); };

    _optExperiment["Problem"]["Type"] = "Optimization";
    _optExperiment["Problem"]["Objective Function"] = fc;

    size_t currentHyperparameter = 0;
    for (size_t i = 1; i < _trainingNeuralNetwork->_layers.size(); i++)
    {
      for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
        for (size_t k = 0; k < _trainingNeuralNetwork->_layers[i - 1]->_nodeCount; k++)
        {
          char varName[512];
          sprintf(varName, "Weight [%lu] %lu->%lu", i, j, k);
          std::string varNameString(varName);
          _optExperiment["Variables"][currentHyperparameter]["Name"] = varNameString;

          // Setting initial weight values
          double initialGuess = _trainingNeuralNetwork->_layers[i]->_weights[j][k];

          _optExperiment["Variables"][currentHyperparameter]["Initial Value"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Mean"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Standard Deviation"] = abs(initialGuess) * 0.1;
          currentHyperparameter++;
        }

      // If we are not using normalization, then we define biases
      if (_batchNormalizationEnabled == false)
      {
        for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
        {
          char varName[512];
          sprintf(varName, "Bias [%lu] %lu", i, j);
          std::string varNameString(varName);
          _optExperiment["Variables"][currentHyperparameter]["Name"] = varNameString;

          // Setting initial biases values
          double initialGuess = _trainingNeuralNetwork->_layers[i]->_bias[j];

          _optExperiment["Variables"][currentHyperparameter]["Initial Value"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Mean"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Standard Deviation"] = abs(initialGuess) * 0.1;
          currentHyperparameter++;
        }
      }
      else // If we are using normalization, then define scale and shift variables
      {
        for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
        {
          char varName[512];
          sprintf(varName, "Scale [%lu] %lu", i, j);
          std::string varNameString(varName);
          _optExperiment["Variables"][currentHyperparameter]["Name"] = varNameString;

          // Setting initial biases values
          double initialGuess = _trainingNeuralNetwork->_layers[i]->_batchNormalizationScale[j];

          _optExperiment["Variables"][currentHyperparameter]["Initial Value"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Mean"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Standard Deviation"] = 0.1;
          currentHyperparameter++;
        }

        for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
        {
          char varName[512];
          sprintf(varName, "Shift [%lu] %lu", i, j);
          std::string varNameString(varName);
          _optExperiment["Variables"][currentHyperparameter]["Name"] = varNameString;

          // Setting initial biases values
          double initialGuess = _trainingNeuralNetwork->_layers[i]->_batchNormalizationShift[j];

          _optExperiment["Variables"][currentHyperparameter]["Initial Value"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Mean"] = initialGuess;
          _optExperiment["Variables"][currentHyperparameter]["Initial Standard Deviation"] = abs(initialGuess) * 0.1;
          currentHyperparameter++;
        }
      }
    }

    _optExperiment["Solver"] = _optimizer;
    _optExperiment["File Output"]["Frequency"] = 0;
    _optExperiment["File Output"]["Enabled"] = false;
    _optExperiment["Console Output"]["Frequency"] = 0;
    _optExperiment["Console Output"]["Verbosity"] = "Silent";
    _optExperiment["Random Seed"] = _k->_randomSeed++;

    // Running initialization to verify that the configuration is correct
    _optEngine.initialize(_optExperiment);

    // Setting learner initialized flag
    _isLearnerInitialized = true;
  }

  // Update/Copy the inputs into the NN
  _trainingNeuralNetwork->setInput(_problem->_inputs);
}

void DeepGD::runGeneration()
{
  _optExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _optExperiment._currentGeneration + _stepsPerGeneration;
  _optEngine.resume(_optExperiment);
  _currentLoss = _optExperiment["Results"]["Best Sample"]["F(x)"].get<double>();
}

void DeepGD::finalize()
{
  // Copying hyperparameters from training to evaluation
  setHyperparameters(getHyperparameters());

  _k->_logger->logInfo("Normal", "Current Loss: %.15f\n", _currentLoss);
}

knlohmann::json &DeepGD::getHyperparameters()
{
  // Getting the best paramaters from the optimization, if it has run at least once
  if (_optExperiment._currentGeneration > 0)
  {
    auto parameters = _optExperiment["Results"]["Best Sample"]["Parameters"].get<std::vector<double>>();
    _trainingNeuralNetwork->storeHyperparameters(parameters);
  }

  // Updating hyperparameters
  for (size_t i = 1; i < _trainingNeuralNetwork->_layers.size(); i++)
  {
    _hyperparameters["Weights"][i] = _trainingNeuralNetwork->_layers[i]->_weights;
    _hyperparameters["Bias"][i] = _trainingNeuralNetwork->_layers[i]->_bias;
    _hyperparameters["Layer Normalization"]["Scale"][i] = _trainingNeuralNetwork->_layers[i]->_batchNormalizationScale;
    _hyperparameters["Layer Normalization"]["Shift"][i] = _trainingNeuralNetwork->_layers[i]->_batchNormalizationShift;
    _hyperparameters["Layer Normalization"]["Means"][i] = _trainingNeuralNetwork->_layers[i]->_batchNormalizationMeans;
    _hyperparameters["Layer Normalization"]["Variances"][i] = _trainingNeuralNetwork->_layers[i]->_batchNormalizationVariances;
  }

  return _hyperparameters;
}

void DeepGD::setHyperparameters(const knlohmann::json &parameters)
{
  for (size_t i = 1; i < _evaluationNeuralNetwork->_layers.size(); i++)
  {
    _evaluationNeuralNetwork->_layers[i]->_weights = parameters["Weights"][i].get<std::vector<std::vector<double>>>();
    _evaluationNeuralNetwork->_layers[i]->_bias = parameters["Bias"][i].get<std::vector<double>>();
    _evaluationNeuralNetwork->_layers[i]->_batchNormalizationScale = parameters["Layer Normalization"]["Scale"][i].get<std::vector<double>>();
    _evaluationNeuralNetwork->_layers[i]->_batchNormalizationShift = parameters["Layer Normalization"]["Shift"][i].get<std::vector<double>>();
    _evaluationNeuralNetwork->_layers[i]->_batchNormalizationMeans = parameters["Layer Normalization"]["Means"][i].get<std::vector<double>>();
    _evaluationNeuralNetwork->_layers[i]->_batchNormalizationVariances = parameters["Layer Normalization"]["Variances"][i].get<std::vector<double>>();
  }

  _evaluationNeuralNetwork->applyHyperparameters();
}

std::vector<double> DeepGD::getEvaluation(const std::vector<double> &inputBatch)
{
  // Updating NN inputs
  _evaluationNeuralNetwork->setInput({ inputBatch });

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues[0];
}

std::vector<double> DeepGD::getGradients(const std::vector<double> &outputDiffs)
{
  // Converting precision
  std::vector<float> singlePrecisionDiffs(outputDiffs.begin(), outputDiffs.end());

  // Running backward propagation wrt data
  _evaluationNeuralNetwork->backwardData(singlePrecisionDiffs);

  return _evaluationNeuralNetwork->_inputGradient[0];
}

void DeepGD::printGenerationAfter()
{
  // Printing results so far
  _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _currentLoss);
}

void DeepGD::evaluateHyperparameters(Sample &sample, NeuralNetwork *nn, const std::vector<std::vector<double>> &solution, bool getGradients)
{
  // Getting parameters for weight and bias update
  auto parameters = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Storing the weights and biases into the neural network
  nn->storeHyperparameters(parameters);

  // Updating the training network's weights, biases, and normalization factors
  nn->applyHyperparameters();

  // Running the input values through the training neural network
  nn->forward();

  // Getting NN's dimensions
  size_t batchSize = nn->_outputValues.size();
  size_t outputSize = nn->_outputValues[0].size();

  // Calculating mean square error
  double meanSquaredError = 0.0;

  // Saving values for the last layer's diff
  std::vector<double> outputDiff(batchSize * outputSize);

  for (size_t i = 0; i < batchSize; i++)
    for (size_t j = 0; j < outputSize; j++)
    {
      double diff = solution[i][j] - nn->_outputValues[i][j];
      outputDiff[i * outputSize + j] = diff;
      meanSquaredError += diff * diff;
    }

  meanSquaredError = meanSquaredError / ((double)batchSize * 2);

  // Saving the negative of the error because we want to minimize it
  sample["F(x)"] = -meanSquaredError;

  // If we need the gradients of the weights, we also compute them
  if (getGradients) sample["Gradient"] = nn->getHyperparameterGradients(outputDiff);
}

} // namespace learner

} // namespace solver

} // namespace korali

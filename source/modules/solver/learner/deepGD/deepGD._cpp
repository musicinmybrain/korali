#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepGD/deepGD.hpp"
#include "sample/sample.hpp"

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepGD::initialize()
{
  // If the learner is alread initialized, do not do anything else
  if (_isLearnerInitialized == false)
  {
    // Getting problem pointer
    _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

    /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

    // Setting the number of input layer nodes as number of input vector size
    _neuralNetwork["Layers"][0]["Node Count"] = _problem->_inputVectorSize;

    // Setting the number of output layer nodes as number of output vector size
    size_t outputLayerId = _neuralNetwork["Layers"].size() - 1;
    _neuralNetwork["Layers"][outputLayerId]["Node Count"] = _problem->_solutionVectorSize;

    // Creating training neural network, setting its batch size as the number of inputs to the problem
    auto neuralNetworkConfig = _neuralNetwork;
    _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    // Creating inference neural network, setting its batch size as one (evaluation per call) and following the global (mean/variance) stats of the trained NN
    neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Use Preloaded Normalization Data"] = true;
    _evaluationNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

    _hyperparameters = _trainingNeuralNetwork->generateInitialHyperparameters();
    _normalizationMeans = _trainingNeuralNetwork->getNormalizationMeans();
    _normalizationVariances = _trainingNeuralNetwork->getNormalizationVariances();

    _adamSolver = new korali::fAdam(_hyperparameters.size());
    _adamSolver->_eta = _learningRate;

    for (size_t i = 0; i < _hyperparameters.size(); i++)
      _adamSolver->_initialValues[i]  = _hyperparameters[i];

    // Resetting
    _adamSolver->reset();

    // Creating and setting hyperparameter structures
    _trainingNeuralNetwork->setHyperparameters(_hyperparameters);

    // Copying hyperparameters and normalization parameters from training to evaluation
    _evaluationNeuralNetwork->setHyperparameters(_hyperparameters);
    _evaluationNeuralNetwork->setNormalizationMeans(_normalizationMeans);
    _evaluationNeuralNetwork->setNormalizationVariances(_normalizationVariances);

    // Setting learner initialized flag
    _isLearnerInitialized = true;
  }

  // Initialize statistics
  if (_k->_currentGeneration == 0)
  {
   _minimumLoss = korali::Inf;
   _suboptimalGenerationCount = 0;
  }

  // Update/Copy the inputs into the NN
  _trainingNeuralNetwork->setInput(_problem->_inputs);
}

void DeepGD::runGeneration()
{
 for (size_t step = 0; step < _stepsPerGeneration; step++)
 {
  // Forward propagating the input values through the training neural network
  _trainingNeuralNetwork->forward();

  // Getting solution's dimensions
  size_t batchSize = _problem->_solution.size();
  size_t solutionSize = _problem->_solution[0].size();

  // Saving values for the loss function and its gradient
  _currentLoss = 0.0;
  std::vector<float> lossDiff(batchSize * solutionSize);

  // Assigning values to evaluation and gradients, depending on the loss function selected
  for (size_t i = 0; i < batchSize; i++)
    for (size_t j = 0; j < solutionSize; j++)
    {
     float diff = 0.0;
     if (_lossFunction == "Direct") diff = _problem->_solution[i][j];
     if (_lossFunction == "Mean Squared Error") diff = _problem->_solution[i][j] - _trainingNeuralNetwork->_outputValues[i][j];
      lossDiff[i * solutionSize + j] = diff;
      _currentLoss += diff*diff;
    }

  // Averaging the loss across the batch size
  _currentLoss = _currentLoss / ((float)batchSize * 2.0f);

  // Backward propagating the gradients through the training neural network
  _trainingNeuralNetwork->backward(lossDiff);

  // If we need the gradients of the weights, we also compute them
  auto gradient = _trainingNeuralNetwork->getHyperparameterGradients();

  // Passing loss and gradient through an ADAM update
  _adamSolver->processResult(_currentLoss, gradient);

  // Updating best loss if better, or increasing suboptimal generation count
  if (_currentLoss < _minimumLoss) { _minimumLoss = _currentLoss; _suboptimalGenerationCount = 0; }
  else _suboptimalGenerationCount++;

  // Getting new set of hyperparameters from Adam
  _trainingNeuralNetwork->setHyperparameters(_adamSolver->_currentValue);
 }
}

void DeepGD::finalize()
{
  _hyperparameters = _trainingNeuralNetwork->getHyperparameters();
  _normalizationMeans = _trainingNeuralNetwork->getNormalizationMeans();
  _normalizationVariances = _trainingNeuralNetwork->getNormalizationVariances();

  // Copying hyperparameters and normalization parameters from training to evaluation
  _evaluationNeuralNetwork->setHyperparameters(_hyperparameters);
  _evaluationNeuralNetwork->setNormalizationMeans(_normalizationMeans);
  _evaluationNeuralNetwork->setNormalizationVariances(_normalizationVariances);
}

std::vector<float> DeepGD::getHyperparameters()
{
  std::vector<float> parameters;

  // Concatenating parameters
  parameters.insert(parameters.end(), _hyperparameters.begin(), _hyperparameters.end());
  parameters.insert(parameters.end(), _normalizationMeans.begin(), _normalizationMeans.end());
  parameters.insert(parameters.end(), _normalizationVariances.begin(), _normalizationVariances.end());

  return parameters;
}

void DeepGD::setHyperparameters(const std::vector<float> &hyperparameters)
{
  size_t curPos = 0;

  // Setting NN hyperparameters first
  _hyperparameters = std::vector<float>(hyperparameters.begin() + curPos, hyperparameters.begin() + _trainingNeuralNetwork->_hyperparameterCount);
  curPos += _hyperparameters.size();

  // Setting NN normalization means second
  _normalizationMeans = std::vector<float>(hyperparameters.begin() + curPos, hyperparameters.begin() + curPos + _trainingNeuralNetwork->_normalizationParameterCount);
  curPos += _normalizationMeans.size();

  // Setting NN normalization variances finally
  _normalizationVariances = std::vector<float>(hyperparameters.begin() + curPos, hyperparameters.end());
}

std::vector<std::vector<float>> DeepGD::getEvaluation(const std::vector<std::vector<float>> &inputBatch)
{
  // Updating NN inputs
  _evaluationNeuralNetwork->setInput(inputBatch);

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues;
}

std::vector<float> DeepGD::getEvaluation(const std::vector<float> &inputBatch)
{
  // Updating NN inputs
  _evaluationNeuralNetwork->setInput({inputBatch});

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues[0];
}

std::vector<float> DeepGD::getGradients(const std::vector<float> &outputDiffs)
{
  // Backward propagating the gradients through the training neural network
  _evaluationNeuralNetwork->backward(outputDiffs);

  // Running backward propagation wrt data
  return _evaluationNeuralNetwork->getDataGradients();
}

void DeepGD::printGenerationAfter()
{
  // Printing results so far
  _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _currentLoss);
  _k->_logger->logInfo("Normal", "Suboptimal Generation Count: %lu\n", _suboptimalGenerationCount);
}

} // namespace learner

} // namespace solver

} // namespace korali

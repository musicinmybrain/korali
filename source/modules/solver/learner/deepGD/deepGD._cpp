#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepGD/deepGD.hpp"

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepGD::initialize()
{

  // If the learner is alread initialized, do not do anything else
  if (_isLearnerInitialized == false)
  {
    // Getting problem pointer
    _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

    /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

    // If we use normalization, inform the underlying NN
    _neuralNetwork["Batch Normalization"]["Enabled"] = _batchNormalizationEnabled;

    // Setting the number of input layer nodes as number of input vector size
    _neuralNetwork["Layers"][0]["Node Count"] = _problem->_inputVectorSize;

    // Setting the number of output layer nodes as number of output vector size
    size_t outputLayerId = _neuralNetwork["Layers"].size() - 1;
    _neuralNetwork["Layers"][outputLayerId]["Node Count"] = _problem->_outputVectorSize;

    // Creating training neural network, setting its batch size as the number of inputs to the problem
    auto neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Batch Size"] = _problem->_inputs.size();
    _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    // Creating inference neural network, setting its batch size as one (evaluation per call) and following the global (mean/variance) stats of the trained NN
    neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Batch Size"] = 1;
    neuralNetworkConfig["Batch Normalization"]["Use Global Stats"] = true;
    _evaluationNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    // If we ran this experiment before, set evaluation hyperparameters and normalization statistics obtained from the previous run
    if (_k->_currentGeneration > 0) setHyperparameters(getHyperparameters());

    /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

    // Creating evaluation lambda function for optimization
    auto fc = [this, nn = &_trainingNeuralNetwork, out = &_problem->_outputs](Sample &sample) { this->evaluateHyperparameters(sample, *nn, *out, true); };

    _optExperiment["Problem"]["Type"] = "Optimization";
    _optExperiment["Problem"]["Objective Function"] = fc;

    std::vector<double> hyperparameters = _trainingNeuralNetwork->getHyperparameters();

    for (size_t i = 0; i < hyperparameters.size(); i++)
    {
      _optExperiment["Variables"][i]["Name"] = std::string("Hyperparameter") + std::to_string(i);
      _optExperiment["Variables"][i]["Initial Value"] = hyperparameters[i];
      _optExperiment["Variables"][i]["Initial Mean"] = hyperparameters[i];
      _optExperiment["Variables"][i]["Initial Standard Deviation"] = abs(hyperparameters[i]) * 0.1;
    }

    _optExperiment["Solver"] = _optimizer;
    _optExperiment["File Output"]["Frequency"] = 0;
    _optExperiment["File Output"]["Enabled"] = false;
    _optExperiment["Console Output"]["Frequency"] = 0;
    _optExperiment["Console Output"]["Verbosity"] = "Silent";
    _optExperiment["Random Seed"] = _k->_randomSeed++;

    // Running initialization to verify that the configuration is correct
    _optEngine.initialize(_optExperiment);

    // Setting learner initialized flag
    _isLearnerInitialized = true;
  }

  // Update/Copy the inputs into the NN
  _trainingNeuralNetwork->setInput(_problem->_inputs);
}

void DeepGD::runGeneration()
{
  _optExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _optExperiment._currentGeneration + _stepsPerGeneration;
  _optEngine.resume(_optExperiment);
  _currentLoss = _optExperiment["Results"]["Best Sample"]["F(x)"].get<double>();
}

void DeepGD::finalize()
{
  _k->_logger->logInfo("Normal", "Current Loss: %.15f\n", _currentLoss);
}

std::vector<double> DeepGD::getHyperparameters()
{
  std::vector<double> parameters;

  // Getting NN's hyperparameters and normalization parameters
  auto networkHyperparameters = _trainingNeuralNetwork->getHyperparameters();
  auto networkNormalizationParameters = _trainingNeuralNetwork->getNormalizationParameters();

  // Concatenating parameters
  parameters.insert(parameters.end(), networkHyperparameters.begin(), networkHyperparameters.end());
  parameters.insert(parameters.end(), networkNormalizationParameters.begin(), networkNormalizationParameters.end());

  return parameters;
}

void DeepGD::setHyperparameters(const std::vector<double> &hyperparameters)
{
  // Setting NN hyperparameters first
  std::vector<double> networkHyperparameters(hyperparameters.begin(), hyperparameters.begin() + _evaluationNeuralNetwork->_hyperparameterCount);
  _evaluationNeuralNetwork->setHyperparameters(networkHyperparameters);

  // Setting NN normalization parameters second
  std::vector<double> networkNormalizationParameters(hyperparameters.begin() + _evaluationNeuralNetwork->_hyperparameterCount, hyperparameters.end());
  _evaluationNeuralNetwork->setNormalizationParameters(networkNormalizationParameters);
}

std::vector<double> DeepGD::getEvaluation(const std::vector<double> &inputBatch)
{
  // Copying hyperparameters and normalization parameters from training to evaluation
  //setHyperparameters(getHyperparameters());

  // Updating NN inputs
  _evaluationNeuralNetwork->setInput({ inputBatch });

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues[0];
}

std::vector<double> DeepGD::getGradients(const std::vector<double> &outputDiffs)
{
  // Converting precision
  std::vector<float> singlePrecisionDiffs(outputDiffs.begin(), outputDiffs.end());

  // Running backward propagation wrt data
  _evaluationNeuralNetwork->backwardData(singlePrecisionDiffs);

  return _evaluationNeuralNetwork->_inputGradient[0];
}

void DeepGD::printGenerationAfter()
{
  // Printing results so far
  _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _currentLoss);
}

void DeepGD::evaluateHyperparameters(Sample &sample, NeuralNetwork *nn, const std::vector<std::vector<double>> &solution, bool getGradients)
{
  // Getting parameters for weight and bias update
  auto parameters = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Updating the training network's weights, biases, and normalization factors
  nn->setHyperparameters(parameters);

  // Running the input values through the training neural network
  nn->forward();

  // Getting NN's dimensions
  size_t batchSize = nn->_outputValues.size();
  size_t outputSize = nn->_outputValues[0].size();

  // Calculating mean square error
  double meanSquaredError = 0.0;

  // Saving values for the last layer's diff
  std::vector<double> outputDiff(batchSize * outputSize);

  for (size_t i = 0; i < batchSize; i++)
    for (size_t j = 0; j < outputSize; j++)
    {
      double diff = solution[i][j] - nn->_outputValues[i][j];
      outputDiff[i * outputSize + j] = diff;
      meanSquaredError += diff * diff;
    }

  meanSquaredError = meanSquaredError / ((double)batchSize * 2);

  // Saving the negative of the error because we want to minimize it
  sample["F(x)"] = -meanSquaredError;

  // If we need the gradients of the weights, we also compute them
  if (getGradients) sample["Gradient"] = nn->getHyperparameterGradients(outputDiff);
}

} // namespace learner

} // namespace solver

} // namespace korali

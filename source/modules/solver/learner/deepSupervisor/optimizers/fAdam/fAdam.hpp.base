#pragma once

#include "modules/solver/learner/deepSupervisor/optimizers/fGradientBasedOptimizers.hpp"
#include <string>
#include <vector>

__startNamespace__;

class __className__ : public __parentClassName__
{
  public:
  // VARIABLES =================================================
  /**
   * @brief Beta for momentum update
   */
  float _beta1{0.9f};
  /**
   * @brief Beta for gradient update
   */
  float _beta2{0.999f};
  /**
   * @brief Weight Decay To add
   */
  // FUNCTIONS =================================================
  // OVERRIDEN FUNCTIONS =======================================
  virtual void initialize() override;
  virtual void processResult(std::vector<float> &gradient) override;
  virtual void reset() override;
  virtual bool _implementsWeightDecay() override { return 1; };
};

__endNamespace__;

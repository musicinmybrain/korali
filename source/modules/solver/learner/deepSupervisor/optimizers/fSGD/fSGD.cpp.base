#include "modules/solver/learner/deepSupervisor/optimizers/fSGD/fSGD.hpp"
#include <vector>

__startNamespace__;

void __className__::initialize() {
  __parentClassName__::initialize();
  reset();
}

void __className__::reset()
{
  _modelEvaluationCount = 0;
  #pragma omp parallel for simd
  for (size_t i = 0; i < _nVars; i++)
    _currentValue[i] = 0.0f;
}

void __className__::processResult(std::vector<float> &gradient)
{
  __parentClassName__::preProcessResult(gradient);

  // clip gradients
  clipGradients(gradient);

  #pragma omp parallel for simd
  for (size_t i = 0; i < _nVars; i++)
  {
    _currentValue[i] += _eta * gradient[i];
  }

  __parentClassName__::postProcessResult(_currentValue);
}

void __className__::clipGradients(std::vector<float> &gradient){
  if(_gradientClipping == "value-clipping"){
    float l2norm = 0;
    #pragma omp parallel for simd reduction(+:l2norm)
    for(size_t i = 0; i < _nVars; i++)
      l2norm += gradient[i]*gradient[i];
    l2norm = sqrt(l2norm);
    if (l2norm > _clippingThreshold){
      #pragma omp parallel for simd
      for(size_t i = 0; i < _nVars; i++)
        gradient[i] = _clippingThreshold*gradient[i]/l2norm;
    }
  };
}

__moduleAutoCode__;

__endNamespace__;

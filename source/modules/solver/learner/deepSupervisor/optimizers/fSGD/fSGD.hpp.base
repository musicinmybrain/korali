#pragma once

#include "modules/solver/learner/deepSupervisor/optimizers/fGradientBasedOptimizers.hpp"
#include <string>
#include <vector>

__startNamespace__;

class __className__ : public __parentClassName__
{
  public:
  // VARIABLES =================================================
  /**
   * @brief Maximum gradient size before clipping
   */
  float _clippingThreshold{10000};
  // FUNCTIONS =================================================
  /**
   * @brief wether to perform graddient clipping and what kind of.
   * @details
   * - value-clipping: clip each gradient value individually
   */
  std::string _gradientClipping{"value-clipping"};
  /**
   * @brief Takes the gradients and clips them if they become too large
   * @param gradient The gradient of the objective function at the current set of parameters
   */
  void clipGradients(std::vector<float> &gradient);
  // OVERRIDEN FUNCTIONS =======================================
  virtual void initialize() override;
  virtual void processResult(std::vector<float> &gradient) override;
  virtual void reset() override;
};

__endNamespace__;

{

 "Module Data":
 {
   "Class Name": "DeepSupervisor",
   "Namespace": ["korali", "solver", "learner"],
   "Parent Class Name": "Learner"
 },

 "Available Operations":
 [
  {
    "Name": "Run Training On Worker",
    "Description": "Asks a Korali worker to run the forward and backward pipeline of the network given an batch for the current worker and return the hyperparameter gradients and their loss.",
    "Function": "runTrainingOnWorker"
  },
  {
    "Name": "Forward Data",
    "Description": "Runs forward pipeline on input data.",
    "Function": "runForwardData"
  }
 ],
 
 "Configuration Settings":
 [
  {
   "Name": [ "Mode" ],
   "Type": "std::string",
   "Options": [
               { "Value": "Automatic Training", "Description": "Trains a neural network for the supervised learning problem." },
               { "Value": "Training", "Description": "Trains a one epoch of a neural network for the supervised learning problem." },
               { "Value": "Predict", "Description": "Predicts using the neural network." },
               { "Value": "Testing", "Description": "Predicts using the neural network and calculates the loss for a solution test set." }
              ],
   "Description": "Specifies the operation mode for the learner."
  },
  {
    "Name": [ "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "Sets the configuration of the hidden layers for the neural network."
  },
  {
    "Name": [ "Neural Network", "Output Activation" ],
    "Type": "knlohmann::json",
    "Description": "Allows setting an aditional activation for the output layer."
  },
  {
    "Name": [ "Neural Network", "Output Layer" ],
    "Type": "knlohmann::json",
    "Description": "Sets any additional configuration (e.g., masks) for the output NN layer."
  },
  {
   "Name": [ "Neural Network", "Engine" ],
   "Type": "std::string",
   "Description": "Specifies which Neural Network backend engine to use."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "std::vector<float>",
   "Description": "Stores the training neural network hyperparameters (weights and biases)."
  },
  {
   "Name": [ "Loss Function"],
   "Type": "std::string",
   "Options": [
      { "Value": "Direct Gradient", "Description": "The given solution represents the gradients of the loss with respect to the network-output. Note that Korali uses the gradients to maximize the objective." },
      { "Value": "DG", "Description": "See 'Direct Gradient'" },
      { "Value": "Mean Squared Error", "Description": "The loss is calculated as the negative mean of square errors, one per input in the batch. Note that Korali maximizes the negative MSE." },
      { "Value": "MSE", "Description": "See 'Mean Squared Error'" },
      { "Value": "Cross Entropy", "Description": "Calculates the Cross Entropy loss and expect softmax activation." },
      { "Value": "CE", "Description": "See 'Cross Entropy'" },
      { "Value": "Negative Log Likelihood", "Description": "Calculates the Negative Log Liklihood loss and expects logsoftmax activation." },
      { "Value": "NLL", "Description": "See 'Negative Log Liklihood'" }
     ],
   "Description": "Function to calculate the difference (loss) between the NN inference and the exact solution and its gradients for optimization."
  },
  {
   "Name": [ "Regularizer" , "Type"],
   "Type": "std::string",
   "Options": [
      { "Value": "", "Description": "No Regularizer."},
      { "Value": "None", "Description": "No Regularizer."},
      { "Value": "L1", "Description": "L1 regularization."},
      { "Value": "L2", "Description": "L2 regularization." }
     ],
   "Description": "Function to calculate the difference (loss) between the NN inference and the exact solution and its gradients for optimization."
  },
  {
   "Name": [ "Regularizer", "Coefficient" ],
   "Type": "float",
   "Description": "Coefficient of the regularizer"
  },
  {
   "Name": [ "Learning Rate" ],
   "Type": "float",
   "Description": "Learning rate for the underlying gradient based optimizers."
  },
  {
   "Name": [ "Learning Rate Type"],
   "Type": "std::string",
   "Options": [
      { "Value": "", "Description": "Constant"},
      { "Value": "Const", "Description": "Constant"},
      { "Value": "Step Based", "Description": "Step based learning rate."},
      { "Value": "Time Based", "Description": "Time based learning rate."}
     ],
   "Description": "Type of the learning rate."
  },
  {
   "Name": [ "Metrics", "Type"],
   "Type": "std::string",
   "Options": [
      { "Value": "", "Description": "None" },
      { "Value": "Accuracy", "Description": "Calcualtes the fraction of correctly classified examples " }
     ],
   "Description": "Function to calculate different metrics on the validation set."
  },
  {
   "Name": [ "Learning Rate Decay Factor" ],
   "Type": "float",
   "Description": "Factor how fast the learning rate decays."
  },
  {
   "Name": [ "Learning Rate Lower Bound" ],
   "Type": "float",
   "Description": "Smallest value the learning rate can obtain."
  },
  {
   "Name": [ "Learning Rate Steps" ],
   "Type": "float",
   "Description": "At what point to divide the 'Step Based' learning rate."
  },
  {
   "Name": [ "Learning Rate Save" ],
   "Type": "bool",
   "Description": "Whether to save the learning rate in the Results."
  },
  {
   "Name": [ "L2 Regularization", "Enabled" ],
   "Type": "bool",
   "Description": "Regulates if l2 regularization will be applied to the neural network."
  },
  {
   "Name": [ "L2 Regularization", "Importance" ],
   "Type": "bool",
   "Description": "Importance weight of l2 regularization."
  },
  {
   "Name": [ "Output Weights Scaling" ],
   "Type": "float",
   "Description": "Specified by how much will the weights of the last linear transformation of the NN be scaled. A value of < 1.0 is useful for a more deterministic start."
  },
  {
   "Name": [ "Batch Concurrency" ],
   "Type": "size_t",
   "Description": "Specifies in how many parts will the mini batch be split for concurrent processing. It must divide the training mini batch size perfectly."
  },
  {
   "Name": [ "Data", "Input", "Shuffel" ],
   "Type": "bool",
   "Description": "Specifies whether to shuffel the input data during initialization."
  },
  {
   "Name": [ "Data", "Training", "Shuffel" ],
   "Type": "bool",
   "Description": "Specifies whether to shuffel the training data for each epoch."
  }
 ],
 "Results":
 [
   {
    "Name": [ "Epoch" ],
    "Type": "size_t",
    "Description": "Training iterration."
   },
   {
    "Name": [ "Training Loss" ],
    "Type": "std::vector<float>",
    "Description": "Traing loss of the previous generation as an averages of the mini-batches"
   },
   {
    "Name": [ "Validation Loss" ],
    "Type": "std::vector<float>",
    "Description": "Validation loss of the previous generation as an averages of the mini-batches"
   },
   {
    "Name": [ "Loss Function" ],
    "Type": "std::string",
    "Description": "Type of the loss function we use."
   },
   {
    "Name": [ "Metrics" ],
    "Type": "std::vector<float>",
    "Description": "Type of the loss function we use."
   }
 ],
 "Termination Criteria":
 [
   {
    "Name": [ "Epochs" ],
    "Type": "size_t",
    "Criteria": "(_epochCount >= _epochs) && (_mode == \"Automatic Training\")",
    "Description": "Specifies the maximum number of epochs to run when in training mode"
   },
   {
    "Name": [ "Target Loss" ],
    "Type": "float",
    "Criteria": "(_epochCount > 1) && (_targetLoss > 0.0) && (_currentValidationLoss <= _targetLoss) && (_mode == \"Training\") && (_mode == \"Training\")",
    "Description": "Specifies the maximum number of epochs to run when in training mode"
   },
   {
    "Name": [ "Is One Epoch Finished" ],
    "Type": "bool",
    "Criteria": "_isOneEpochFinished && (_mode == \"Predict\" || _mode == \"Testing\" || _mode == \"Training\")",
    "Description": "Stops prediction after the first generation by setting _isOneEpochFinished to bool"
   }
 ],

 "Variables Configuration":
 [

 ],


 "Internal Settings":
 [
  {
   "Name": [ "Evaluation" ],
   "Type": "std::vector<std::vector<float>>",
   "Description": "The output of the neural network if running on testing mode (Npred x OC)."
  },
  {
    "Name": [ "Data", "Validation", "Split" ],
    "Type": "float",
    "Description": "If given indicates to split the training data into training and validation data. 0<given<1: percentage for validation data; given>1: nb. of samples for validation split."
  },
  {
    "Name": [ "Validation Set", "Solution" ],
    "Type": "std::vector<float>",
    "Description": "Provides the solution for one-step ahead prediction with layout NV*OC, where N is the batch size and OC is the vector size of the output."
  },
  {
    "Name": [ "Validation Set", "Size" ],
    "Type": "size_t",
    "Description": "Provides the number of samples of the validation set NV."
  },
  {
   "Name": [ "Current Training Loss" ],
   "Type": "float",
   "Description": "Current value of the training loss."
  },
  {
   "Name": [ "Current Validation Loss" ],
   "Type": "float",
   "Description": "Current value of the loss on the validation set."
  },
  {
   "Name": [ "Current Metrics" ],
   "Type": "float",
   "Description": "Current value of calculated Metrics"
  },
  {
   "Name": [ "Training Loss" ],
   "Type": "std::vector<float>",
   "Description": "Current value of the traing loss for all epochs."
  },
  {
   "Name": [ "Validation Loss" ],
   "Type": "std::vector<float>",
   "Description": "Current value of the validation loss for all epochs."
  },
  {
   "Name": [ "Total Metrics" ],
   "Type": "std::vector<float>",
   "Description": "Value of the metrics for all epochs."
  },
  {
   "Name": [ "Testing Loss" ],
   "Type": "float",
   "Description": "Value of the testing loss."
  },
  {
   "Name": [ "Normalization Means" ],
   "Type": "std::vector<float>",
   "Description": "Stores the current neural network normalization mean parameters."
  },
  {
   "Name": [ "Normalization Variances" ],
   "Type": "std::vector<float>",
   "Description": "Stores the current neural network normalization variance parameters."
  },
  {
   "Name": [ "Epoch Count" ],
   "Type": "size_t",
   "Description": "Stores the current epoch number."
   },
  {
    "Name": [ "Optimizer" ],
    "Type": "korali::solver::learner::optimizer::FastGradientBasedOptimizer*",
    "Description": "Gradient-based solver pointer to access directly (for performance)",
    "Options": [
        { "Value": "learner/deepSupervisor/optimizers/fSGD", "Description": "Uses the Stochastic Gradient Descent algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fMomentum", "Description": "Uses the Momentum algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fAdam", "Description": "Uses the Adam algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fAdaBelief", "Description": "Uses the AdaBelief algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fMADGRAD", "Description": "Uses the MADGRAD algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fRMSProp", "Description": "Uses the RMSProp algorithm." },
        { "Value": "learner/deepSupervisor/optimizers/fAdaGrad", "Description": "Uses the AdaGrad algorithm." }
    ]
  },
  {
   "Name": [ "Total Learning Rate" ],
   "Type": "std::vector<float>",
   "Description": "Value of the learning rate for all epochs."
  }
 ],

 "Module Defaults":
 {
  "L2 Regularization": 
   {
     "Enabled": false,
     "Importance": 1e-4
   },
  "Regularizer":
  {
    "Coefficient": 1e-4,
    "Type": "None"
  },
  "Loss Function": "Direct Gradient",
  "Learning Rate Type": "Const",
  "Learning Rate Save": true,
  "Learning Rate Decay Factor": 100,
  "Learning Rate Steps": 0,
  "Learning Rate Lower Bound": -10000000000,
  "Neural Network":
  {
   "Output Activation": "Identity",
   "Output Layer": { },
   "Hidden Layers": {  }
  },
  "Metrics": {
    "Type": ""
  },
  "Termination Criteria":
   { 
     "Epochs": 10000000000,
     "Is One Epoch Finished": false,
     "Target Loss": -1.0,
     "Max Generations": 10000000000
   },
  "Hyperparameters": [],
  "Output Weights Scaling": 1.0,
  "Batch Concurrency": 1,
  "Epoch Count": 0,
  "Data":
  {
    "Validation": {
        "Split": 0.0
    },
    "Training": {
        "Shuffel": true
    },
    "Input": {
        "Shuffel": false
    }
  },
  "Optimizer":
  {
    "Type": "learner/deepSupervisor/optimizers/fSGD"
  }
 },
 "Variable Defaults":
 {

 }
}

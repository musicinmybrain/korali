#pragma once

#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include "modules/problem/supervisedLearning/supervisedLearning.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fAdaBelief.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fAdagrad.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fAdam.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fGradientBasedOptimizer.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fMadGrad.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fRMSProp.hpp"
#include "modules/solver/learner/learner.hpp"

__startNamespace__;

class __className__ : public __parentClassName__
{
  public:
  /**
   * @brief Korali Problem for optimizing NN weights and biases
   */
  problem::SupervisedLearning *_problem;

  /**
   * @brief Korali Experiment for optimizing the NN's weights and biases
   */
  korali::Experiment _optExperiment;

  /**
   * @brief Gradient-based solver pointer to access directly (for performance)
   */
  korali::fGradientBasedOptimizer *_optimizer;

  /**
   * @brief A neural network to be trained based on inputs and solutions
   */
  NeuralNetwork *_neuralNetwork;

  std::vector<std::vector<float>> &getEvaluation(const std::vector<std::vector<std::vector<float>>> &input) override;
  std::vector<float> getHyperparameters() override;
  void setHyperparameters(const std::vector<float> &hyperparameters) override;

  void initialize() override;
  void runGeneration() override;
  void runTrainingGeneration();
  /**
  * @brief splits up the test input - further into minibatches and runs just the forward pass.
  */
  void runTestingGeneration();
  void printGenerationBefore() override;
  void printGenerationAfter() override;
  /**
  * @brief Calculates the loss function.
  * @param y the ground truth y of size N*OC
  * @param yhat of our model N*OC.
  * @return the loss function of size N*OC
  */
  /**
  * @brief Calculates the neural network hyperparamter gradients.
  * @details dloss is a wrapper function that runs the backward pipeline of the neural network by passing
  *          it the derivative of the output loss function and returns the gradients of the hyperparameters.
  * @param dloss BatchSize x OC derivatives of the output loss function phi'(z^{(L+1)})
  * @return hyperparamter gradients
  */
  std::vector<float> backwardGradients(const std::vector<std::vector<float>> &dloss);
  /**
   * @brief Run the training pipeline of the network given an input and return the output.
   * @param sample A sample containing the NN's input BxTxIC (B: Batch Size, T: Time steps, IC: Input channels) and solution BxOC data (B: Batch Size, OC: Output channels)
   */
  void runTrainingOnWorker(korali::Sample &sample);

  /**
   * @brief Run the forward evaluation pipeline of the network given an input and return the output.
   * @param sample A sample containing the NN's input BxTxIC (B: Batch Size, T: Time steps, IC: Input channels)
   */
  void runEvaluationOnWorker(korali::Sample &sample);

  /**
   * @brief Run one iteration of forward backward loop on a worker
   * @param sample A sample containing the NN's input BxTxIC (B: Batch Size, T: Time steps, IC: Input channels)
   */
  void runIteration(korali::Sample &sample);
  /**
   * @brief Update the hyperparameters for the neural network after an update for every worker.
   * @param sample A sample containing the new NN's hyperparameters
   */
  void updateHyperparametersOnWorker(korali::Sample &sample);
};

__endNamespace__;

#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <omp.h>

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepSupervisor::initialize()
{
  // If the learner is alread initialized, do not do anything else
  if (_isLearnerInitialized == false)
  {
    // Getting problem pointer
    _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

    /*****************************************************************
    * Obtaining parallel configuration
    *****************************************************************/
    _evaluationNeuralNetworks.resize(_maxThreads);

    /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

    // Setting the number of input layer nodes as number of input vector size
    _neuralNetwork["Layers"][0]["Node Count"] = _problem->_inputVectorSize;

    // Setting the number of output layer nodes as number of output vector size
    size_t outputLayerId = _neuralNetwork["Layers"].size() - 1;
    _neuralNetwork["Layers"][outputLayerId]["Node Count"] = _problem->_solutionVectorSize;

    // Scale down weights of output layer
    _neuralNetwork["Layers"][outputLayerId]["Weight Scaling"] = 0.1;

    auto neuralNetworkConfig = _neuralNetwork;
    _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));
    _trainingNeuralNetwork->initialize();

    // Creating inference neural network
    for (size_t i = 0; i < _maxThreads; i++)
    {
      auto neuralNetworkConfig = _neuralNetwork;
      _evaluationNeuralNetworks[i] = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));
      _evaluationNeuralNetworks[i]->initialize();
    }

    /*****************************************************************
   * Initializing NN hyperparameters
   *****************************************************************/

    // If the hyperparameters have not been specified, produce new initial hyperparameters
    if (_hyperparameters.size() == 0) _hyperparameters = _trainingNeuralNetwork->generateInitialHyperparameters();

    // Creating and setting hyperparameter structures
    _trainingNeuralNetwork->setHyperparameters(_hyperparameters);

    // Copying hyperparameters and normalization parameters from training to evaluation
    for (size_t i = 0; i < _maxThreads; i++)
      _evaluationNeuralNetworks[i]->setHyperparameters(_hyperparameters);

    /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

    auto weightAndBiasParameters = _trainingNeuralNetwork->getHyperparameters();

    if (_optimizer == "Adam") _adamSolver = new korali::fAdam(weightAndBiasParameters.size());
    if (_optimizer == "AdaBelief") _adamSolver = new korali::fAdaBelief(weightAndBiasParameters.size());

    // Setting learning rate
    _adamSolver->_eta = _learningRate;

    for (size_t i = 0; i < weightAndBiasParameters.size(); i++)
      _adamSolver->_initialValues[i] = weightAndBiasParameters[i];

    // Resetting
    _adamSolver->reset();

    // Setting learner initialized flag
    _isLearnerInitialized = true;
  }

  // Initialize statistics
  if (_k->_currentGeneration == 0)
  {
    _minimumLoss = korali::Inf;
    _suboptimalGenerationCount = 0;
  }

  // Update/Copy the inputs into the NN
  _trainingNeuralNetwork->setInput(_problem->_inputs);
}

void DeepSupervisor::runGeneration()
{
  for (size_t step = 0; step < _stepsPerGeneration; step++)
  {
    // Forward propagating the input values through the training neural network
    _trainingNeuralNetwork->forward();

    // Saving values for the loss function and its gradient
    _currentLoss = 0.0;

    // Allocating space for the loss
    std::vector<std::vector<std::vector<float>>> lossDiff(_problem->_timesteps);
    for (size_t t = 0; t < _problem->_timesteps; t++)
    {
     lossDiff[t].resize(_problem->_batchSize);
     for (size_t i = 0; i < _problem->_batchSize; i++) lossDiff[t][i].resize(_problem->_solutionVectorSize);
    }

    // Assigning values to evaluation and gradients, depending on the loss function selected
    for (size_t t = 0; t < _problem->_timesteps; t++)
     for (size_t i = 0; i < _problem->_batchSize; i++)
      for (size_t j = 0; j < _problem->_solutionVectorSize; j++)
      {
        float diff = 0.0;
        if (_lossFunction == "Direct Gradient") diff = _problem->_solution[t][i][j];
        if (_lossFunction == "Mean Squared Error") diff = _problem->_solution[t][i][j] - _trainingNeuralNetwork->_outputValues[t][i][j];
        lossDiff[t][i][j] = diff;
        _currentLoss += diff * diff;
      }

    // Averaging the loss across the batch size
    _currentLoss = _currentLoss / ((float)_problem->_timesteps * (float)_problem->_batchSize * 2.0f);

    // Backward propagating the gradients through the training neural network
    _trainingNeuralNetwork->backward(lossDiff);

    // Passing hyperparameter gradients through an ADAM update
    _adamSolver->processResult(_currentLoss, _trainingNeuralNetwork->_hyperparameterGradients);

    // Updating best loss if better, or increasing suboptimal generation count
    if (_currentLoss < _minimumLoss) _minimumLoss = _currentLoss;
    else _suboptimalGenerationCount++;

    // Getting new set of hyperparameters from Adam
    _trainingNeuralNetwork->setHyperparameters(_adamSolver->_currentValue);
  }
}

void DeepSupervisor::finalize()
{
  // Copying hyperparameters and normalization parameters from training to evaluation
  auto hyperparameters = _trainingNeuralNetwork->getHyperparameters();

  for (size_t i = 0; i < _maxThreads; i++)
    _evaluationNeuralNetworks[i]->setHyperparameters(hyperparameters);

  _hyperparameters = hyperparameters;
}

std::vector<float> DeepSupervisor::getTrainingHyperparameters()
{
  return _trainingNeuralNetwork->getHyperparameters();
}

void DeepSupervisor::setTrainingHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _trainingNeuralNetwork->setHyperparameters(hyperparameters);

  // Need to update Adam to continue from there
  for (size_t i = 0; i < hyperparameters.size(); i++)
    _adamSolver->_initialValues[i] = hyperparameters[i];

  // Resetting
  _adamSolver->reset();
}

std::vector<float> DeepSupervisor::getInferenceHyperparameters()
{
  return _evaluationNeuralNetworks[0]->getHyperparameters();
}

void DeepSupervisor::setInferenceHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  for (size_t i = 0; i < _maxThreads; i++)
    _evaluationNeuralNetworks[i]->setHyperparameters(hyperparameters);
}

std::vector<std::vector<std::vector<float>>> DeepSupervisor::getEvaluation(const std::vector<std::vector<std::vector<float>>> &input)
{
  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  // Updating NN inputs
  _evaluationNeuralNetworks[curThread]->setInput(input);

  // Running the input values through the neural network
  _evaluationNeuralNetworks[curThread]->forward();

  return _evaluationNeuralNetworks[curThread]->_outputValues;
}

std::vector<std::vector<std::vector<float>>> DeepSupervisor::getGradients(const std::vector<std::vector<std::vector<float>>> &outputDiffs)
{
  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  // Backward propagating the gradients through the training neural network
  _evaluationNeuralNetworks[curThread]->backward(outputDiffs);

  // Running backward propagation wrt data
  return _evaluationNeuralNetworks[curThread]->_inputGradients;
}

void DeepSupervisor::printGenerationAfter()
{
  // Printing results so far
  if (_lossFunction == "Mean Squared Error")
  {
    _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentLoss);
    _k->_logger->logInfo("Detailed", " + Suboptimal Generation Count: %lu\n", _suboptimalGenerationCount);
  }
  else if (_lossFunction == "Direct Gradient")
    _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f\n", sqrt(_currentLoss));
}

} // namespace learner

} // namespace solver

} // namespace korali

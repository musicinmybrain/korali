#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepSupervisor::initialize()
{
  // If the learner is alread initialized, do not do anything else
  if (_isLearnerInitialized == false)
  {
    // Getting problem pointer
    _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

    /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

    // Setting the number of input layer nodes as number of input vector size
    _neuralNetwork["Layers"][0]["Node Count"] = _problem->_inputVectorSize;

    // Setting the number of output layer nodes as number of output vector size
    size_t outputLayerId = _neuralNetwork["Layers"].size() - 1;
    _neuralNetwork["Layers"][outputLayerId]["Node Count"] = _problem->_solutionVectorSize;

    // Creating training neural network, setting its batch size as the number of inputs to the problem
    auto neuralNetworkConfig = _neuralNetwork;
    _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    // Creating inference neural network, setting its batch size as one (evaluation per call) and following the global (mean/variance) stats of the trained NN
    neuralNetworkConfig = _neuralNetwork;
    neuralNetworkConfig["Use Preloaded Normalization Data"] = true;
    _evaluationNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(neuralNetworkConfig, _k));

    /*****************************************************************
   * Initializing NN hyperparameters
   *****************************************************************/

    _hyperparameters = _trainingNeuralNetwork->generateInitialHyperparameters();

    // Creating and setting hyperparameter structures
    _trainingNeuralNetwork->setHyperparameters(_hyperparameters);

    // Copying hyperparameters and normalization parameters from training to evaluation
    _evaluationNeuralNetwork->setHyperparameters(_hyperparameters);

    /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

    auto weightAndBiasParameters = _trainingNeuralNetwork->getWeightsAndBiases();

    if (_optimizer == "Adam") _adamSolver = new korali::fAdam(weightAndBiasParameters.size());
    if (_optimizer == "AdaBelief") _adamSolver = new korali::fAdaBelief(weightAndBiasParameters.size());

    // Setting learning rate
    _adamSolver->_eta = _learningRate;

    for (size_t i = 0; i < weightAndBiasParameters.size(); i++)
      _adamSolver->_initialValues[i] = weightAndBiasParameters[i];

    // Resetting
    _adamSolver->reset();

    // Setting learner initialized flag
    _isLearnerInitialized = true;
  }

  // Initialize statistics
  if (_k->_currentGeneration == 0)
  {
    _minimumLoss = korali::Inf;
    _suboptimalGenerationCount = 0;
  }

  // Update/Copy the inputs into the NN
  _trainingNeuralNetwork->setInput(_problem->_inputs);
}

void DeepSupervisor::runGeneration()
{
  for (size_t step = 0; step < _stepsPerGeneration; step++)
  {
    // Forward propagating the input values through the training neural network
    _trainingNeuralNetwork->forward();

    // Getting solution's dimensions
    size_t batchSize = _problem->_solution.size();
    size_t solutionSize = _problem->_solution[0].size();

    // Saving values for the loss function and its gradient
    _currentLoss = 0.0;
    std::vector<float> lossDiff(batchSize * solutionSize);

    // Assigning values to evaluation and gradients, depending on the loss function selected
    for (size_t i = 0; i < batchSize; i++)
      for (size_t j = 0; j < solutionSize; j++)
      {
        float diff = 0.0;
        if (_lossFunction == "Direct Gradient") diff = _problem->_solution[i][j];
        if (_lossFunction == "Mean Squared Error") diff = _problem->_solution[i][j] - _trainingNeuralNetwork->_outputValues[i][j];
        lossDiff[i * solutionSize + j] = diff;
        _currentLoss += diff * diff;
      }

    // Averaging the loss across the batch size
    _currentLoss = _currentLoss / ((float)batchSize * 2.0f);

    // Backward propagating the gradients through the training neural network
    _trainingNeuralNetwork->backward(lossDiff);

    // If we need the gradients of the weights, we also compute them
    auto gradient = _trainingNeuralNetwork->getWeightAndBiasesGradients();

    // Passing loss and gradient through an ADAM update
    _adamSolver->processResult(_currentLoss, gradient);

    // Updating best loss if better, or increasing suboptimal generation count
    if (_currentLoss < _minimumLoss)
      _minimumLoss = _currentLoss;
    else
      _suboptimalGenerationCount++;

    // Getting new set of hyperparameters from Adam
    _trainingNeuralNetwork->setWeightsAndBiases(_adamSolver->_currentValue);
  }
}

void DeepSupervisor::finalize()
{
  _hyperparameters = _trainingNeuralNetwork->getHyperparameters();

  // Copying hyperparameters and normalization parameters from training to evaluation
  _evaluationNeuralNetwork->setHyperparameters(_hyperparameters);
}

std::vector<float> DeepSupervisor::getHyperparameters()
{
  return _hyperparameters;
}

void DeepSupervisor::setHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _evaluationNeuralNetwork->setHyperparameters(hyperparameters);
}

std::vector<std::vector<float>> DeepSupervisor::getEvaluation(const std::vector<std::vector<float>> &inputBatch)
{
  // Updating NN inputs
  _evaluationNeuralNetwork->setInput(inputBatch);

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues;
}

std::vector<float> DeepSupervisor::getEvaluation(const std::vector<float> &inputBatch)
{
  // Updating NN inputs
  _evaluationNeuralNetwork->setInput({inputBatch});

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues[0];
}

std::vector<float> DeepSupervisor::getGradients(const std::vector<float> &outputDiffs)
{
  // Backward propagating the gradients through the training neural network
  _evaluationNeuralNetwork->backward(outputDiffs);

  // Running backward propagation wrt data
  return _evaluationNeuralNetwork->getDataGradients();
}

void DeepSupervisor::printGenerationAfter()
{
  // Printing results so far
  if (_lossFunction == "Mean Squared Error")
  {
    _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentLoss);
    _k->_logger->logInfo("Detailed", " + Suboptimal Generation Count: %lu\n", _suboptimalGenerationCount);
  }
  else if (_lossFunction == "Direct Gradient")
    _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f\n", sqrt(_currentLoss));
}

} // namespace learner

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <omp.h>

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepSupervisor::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

  /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

  // Configuring neural network's inputs
  knlohmann::json neuralNetworkConfig;
  neuralNetworkConfig["Type"] = "Neural Network";
  neuralNetworkConfig["Engine"] = _neuralNetworkEngine;
  neuralNetworkConfig["Timestep Count"] = _problem->_maxTimesteps;

  // Iterator for the current layer id
  size_t curLayer = 0;

  // Setting the number of input layer nodes as number of input vector size
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Input";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_inputSize;
  curLayer++;

  // Adding user-defined hidden layers
  for (size_t i = 0; i < _neuralNetworkHiddenLayers.size(); i++) neuralNetworkConfig["Layers"][curLayer++] = _neuralNetworkHiddenLayers[i];

  // Adding linear transformation layer to convert hidden state to match output channels
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Linear";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_solutionSize;
  neuralNetworkConfig["Layers"][curLayer]["Weight Scaling"] = _outputWeightsScaling;
  curLayer++;

  // Applying a user-defined pre-activation function
  if (_neuralNetworkOutputActivation != "Identity")
  {
    neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Activation";
    neuralNetworkConfig["Layers"][curLayer]["Function"] = _neuralNetworkOutputActivation;
    curLayer++;
  }

  // Applying output layer configuration
  neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkOutputLayer;
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Output";

  // Instancing training neural network
  auto trainingNeuralNetworkConfig = neuralNetworkConfig;
  trainingNeuralNetworkConfig["Batch Size"] = _problem->_trainingBatchSize;
  trainingNeuralNetworkConfig["Mode"] = "Training";
  _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(trainingNeuralNetworkConfig, _k));
  _trainingNeuralNetwork->applyModuleDefaults(trainingNeuralNetworkConfig);
  _trainingNeuralNetwork->setConfiguration(trainingNeuralNetworkConfig);
  _trainingNeuralNetwork->initialize();

  // Creating inference neural networks (one per thread)
  _inferenceNeuralNetworks.resize(_maxThreads);

#pragma omp parallel
  {
    size_t curThread = omp_get_thread_num();
    auto inferenceNeuralNetworkConfig = neuralNetworkConfig;
    inferenceNeuralNetworkConfig["Batch Size"] = _problem->_inferenceBatchSize;
    inferenceNeuralNetworkConfig["Mode"] = "Training";
    _inferenceNeuralNetworks[curThread] = dynamic_cast<NeuralNetwork *>(getModule(inferenceNeuralNetworkConfig, _k));
    _inferenceNeuralNetworks[curThread]->applyModuleDefaults(inferenceNeuralNetworkConfig);
    _inferenceNeuralNetworks[curThread]->setConfiguration(inferenceNeuralNetworkConfig);
    _inferenceNeuralNetworks[curThread]->initialize();
  }

  /*****************************************************************
  * Initializing NN hyperparameters
  *****************************************************************/

  // If the hyperparameters have not been specified, produce new initial ones
  if (_hyperparameters.size() == 0) _hyperparameters = _trainingNeuralNetwork->generateInitialHyperparameters();

  // Creating and setting hyperparameter structures
  _trainingNeuralNetwork->setHyperparameters(_hyperparameters);

// Copying hyperparameters and normalization parameters from training to evaluation
#pragma omp parallel
  {
    size_t curThread = omp_get_thread_num();
    _inferenceNeuralNetworks[curThread]->setHyperparameters(_hyperparameters);
  }

  /*****************************************************************
  * Setting up weight and bias optimization experiment
  *****************************************************************/

  auto weightAndBiasParameters = _trainingNeuralNetwork->getHyperparameters();

  if (_optimizer == "Adam") _adamSolver = new korali::fAdam(weightAndBiasParameters.size());
  if (_optimizer == "AdaBelief") _adamSolver = new korali::fAdaBelief(weightAndBiasParameters.size());

  // Setting initial guesses as the current weight and bias parameters
  _adamSolver->_initialValues = weightAndBiasParameters;

  // Resetting solver before using it
  _adamSolver->reset();

  _currentLoss = 0.0f;
  _l2Hyperparameter = 0.0f;
}

void DeepSupervisor::runGeneration()
{
  // Grabbing constants
  size_t N = _problem->_trainingBatchSize;
  size_t OC = _problem->_solutionSize;

  // Checking that incoming data has a correct format
  _problem->verifyData();

  // Updating optimizer's learning rate, in case it changed
  _adamSolver->_eta = _learningRate;
  for (size_t step = 0; step < _stepsPerGeneration; step++)
  {
    // Forward propagating the input values through the training neural network
    _trainingNeuralNetwork->forward(_problem->_inputData);

    // Creating gradient vector
    auto gradientVector = _problem->_solutionData;

    // If we use an MSE loss function, we need to update the gradient vector with its difference with each of batch's last timestep of the NN output
    if (_lossFunction == "Mean Squared Error")
      for (size_t b = 0; b < N; b++)
        for (size_t i = 0; i < OC; i++)
          gradientVector[b][i] = gradientVector[b][i] - _trainingNeuralNetwork->_outputValues[b][i];

    // Calculating loss across the batch size
    _currentLoss = 0.0;
    for (size_t b = 0; b < N; b++)
      for (size_t i = 0; i < OC; i++)
        _currentLoss += gradientVector[b][i] * gradientVector[b][i];
    _currentLoss = _currentLoss / ((float)N * 2.0f);

    // Backward propagating the gradients through the training neural network
    _trainingNeuralNetwork->backward(gradientVector);

    // Apply gradient of L2 regularizer
    if (_l2RegularizationEnabled)
    {
      auto nnHyperparameters = _trainingNeuralNetwork->getHyperparameters();
      for (size_t i = 0; i < _trainingNeuralNetwork->_hyperparameterCount; ++i)
        _trainingNeuralNetwork->_hyperparameterGradients[i] -= _l2RegularizationImportance * nnHyperparameters[i];
    }

    // Passing hyperparameter gradients through an ADAM update
    _adamSolver->processResult(_currentLoss, _trainingNeuralNetwork->_hyperparameterGradients);

    // Getting new set of hyperparameters from Adam
    _trainingNeuralNetwork->setHyperparameters(_adamSolver->_currentValue);
  }

  // Copying hyperparameters and normalization parameters from training to evaluation
  _hyperparameters = _trainingNeuralNetwork->getHyperparameters();

  #pragma omp parallel
  {
    size_t curThread = omp_get_thread_num();
    _inferenceNeuralNetworks[curThread]->setHyperparameters(_hyperparameters);
  }

  _l2Hyperparameter = 0.;
  for (float param : _hyperparameters) _l2Hyperparameter += param * param;
  _l2Hyperparameter = std::sqrt(_l2Hyperparameter);
}

std::vector<float> DeepSupervisor::getTrainingHyperparameters()
{
  return _trainingNeuralNetwork->getHyperparameters();
}

void DeepSupervisor::setTrainingHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _trainingNeuralNetwork->setHyperparameters(hyperparameters);

  // Need to update Adam to continue from there
  for (size_t i = 0; i < hyperparameters.size(); i++)
    _adamSolver->_initialValues[i] = hyperparameters[i];

  // Resetting
  _adamSolver->reset();
}

std::vector<float> DeepSupervisor::getInferenceHyperparameters()
{
  return _inferenceNeuralNetworks[0]->getHyperparameters();
}

void DeepSupervisor::setInferenceHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  #pragma omp parallel
 {
  size_t curThread = omp_get_thread_num();
  _inferenceNeuralNetworks[curThread]->setHyperparameters(hyperparameters);
 }
}

std::vector<std::vector<float>> DeepSupervisor::getEvaluation(const std::vector<std::vector<std::vector<float>>> &input)
{
  // Grabbing constants
  size_t N = _problem->_inferenceBatchSize;
  size_t IC = _problem->_inputSize;

  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  // Check whether the input has the correct shape
  if (input.size() != _problem->_inferenceBatchSize)
    KORALI_LOG_ERROR("A different batch size (%lu) passed than the one in the configuration.\n", input.size(), _problem->_inferenceBatchSize);

  for (size_t b = 0; b < N; b++)
    if (input[b].size() > _problem->_maxTimesteps)
      KORALI_LOG_ERROR("More timesteps (%lu) passed for batch %lu for inference than maximum (%lu).\n", input[b].size(), b, _problem->_maxTimesteps);

  for (size_t b = 0; b < N; b++)
    for (size_t t = 0; t < input[b].size(); t++)
      if (input[b][t].size() > IC)
        KORALI_LOG_ERROR("A different vector size (%lu) passed for batch %lu, timestep %lu for inference than expected (%lu).\n", input[b][t].size(), b, t, IC);

  // Running the input values through the neural network
  _inferenceNeuralNetworks[curThread]->forward(input);

  // Returning the output values for the last given timestep
  return _inferenceNeuralNetworks[curThread]->_outputValues;
}

std::vector<std::vector<float>> DeepSupervisor::getDataGradients(const std::vector<std::vector<std::vector<float>>> &input, const std::vector<std::vector<float>> &outputGradients)
{
  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  auto forwardEvaluation = getEvaluation(input);

  // Running the input values through the neural network
  _inferenceNeuralNetworks[curThread]->backward(outputGradients);

  // Returning the input data gradients
  return _inferenceNeuralNetworks[curThread]->_inputGradients;
}

void DeepSupervisor::printGenerationAfter()
{
  // Printing results so far
  if (_lossFunction == "Mean Squared Error") _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentLoss);
  if (_lossFunction == "Direct Gradient") _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f\n", std::sqrt(_currentLoss));
  _k->_logger->logInfo("Normal", " + Hyperparameter L2-Norm: %.15f\n", _l2Hyperparameter);
}

} // namespace learner

} // namespace solver

} // namespace korali

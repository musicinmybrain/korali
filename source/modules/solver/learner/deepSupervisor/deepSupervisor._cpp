#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <omp.h>

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{
void DeepSupervisor::initialize()
{
   // Getting problem pointer
   _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

   /*****************************************************************
   * Obtaining parallel configuration
   *****************************************************************/
   _inferenceNeuralNetworks.resize(_maxThreads);

   /*****************************************************************
  * Setting up Neural Networks
  *****************************************************************/

   // Configuring neural network's inputs
    knlohmann::json neuralNetworkConfig;
    neuralNetworkConfig["Type"] = "Neural Network";
    neuralNetworkConfig["Engine"] = _neuralNetworkEngine;
    neuralNetworkConfig["Timestep Count"] = _problem->_maxTimesteps;

   // Iterator for the current layer id
    size_t curLayer = 0;

   // Setting the number of input layer nodes as number of input vector size
   neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Input";
   neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_inputSize;
   curLayer++;

   // Adding user-defined hidden layers
   for (size_t i = 0; i < _neuralNetworkHiddenLayers.size(); i++)  neuralNetworkConfig["Layers"][curLayer++] = _neuralNetworkHiddenLayers[i];

   // Adding linear transformation layer to convert hidden state to match output channels
   neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Linear";
   neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_solutionSize;
   curLayer++;

   // Applying a user-defined pre-activation function
   if (_neuralNetworkOutputActivation != "Identity")
   {
    neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Activation";
    neuralNetworkConfig["Layers"][curLayer]["Function"] = _neuralNetworkOutputActivation;
    curLayer++;
   }

   // Applying output layer configuration
   neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkOutputLayer;
   neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Output";

   // Instancing training neural network
   auto trainingNeuralNetworkConfig = neuralNetworkConfig;
   trainingNeuralNetworkConfig["Batch Size"] = _problem->_trainingBatchSize;
   _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(trainingNeuralNetworkConfig, _k));
   _trainingNeuralNetwork->initialize();

   // Creating inference neural networks (one per thread)
   for (size_t i = 0; i < _maxThreads; i++)
   {
     auto inferenceNeuralNetworkConfig = neuralNetworkConfig;
     inferenceNeuralNetworkConfig["Batch Size"] = _problem->_inferenceBatchSize;
     _inferenceNeuralNetworks[i] = dynamic_cast<NeuralNetwork *>(getModule(inferenceNeuralNetworkConfig, _k));
     _inferenceNeuralNetworks[i]->initialize();
   }

   /*****************************************************************
  * Initializing NN hyperparameters
  *****************************************************************/

   // If the hyperparameters have not been specified, produce new initial ones
   if (_hyperparameters.size() == 0) _hyperparameters = _trainingNeuralNetwork->generateInitialHyperparameters();

   // Creating and setting hyperparameter structures
   _trainingNeuralNetwork->setHyperparameters(_hyperparameters);

   // Copying hyperparameters and normalization parameters from training to evaluation
   for (size_t i = 0; i < _maxThreads; i++)
     _inferenceNeuralNetworks[i]->setHyperparameters(_hyperparameters);

   /*****************************************************************
  * Setting up weight and bias optimization experiment
  *****************************************************************/

   auto weightAndBiasParameters = _trainingNeuralNetwork->getHyperparameters();

   if (_optimizer == "Adam") _adamSolver = new korali::fAdam(weightAndBiasParameters.size());
   if (_optimizer == "AdaBelief") _adamSolver = new korali::fAdaBelief(weightAndBiasParameters.size());

   // Setting learning rate
   _adamSolver->_eta = _learningRate;

   // Setting initial guesses as the current weight and bias parameters
   for (size_t i = 0; i < weightAndBiasParameters.size(); i++)
     _adamSolver->_initialValues[i] = weightAndBiasParameters[i];

   // Resetting solver before using it
   _adamSolver->reset();

  // Initialize statistics
  if (_k->_currentGeneration == 0)
  {
    _minimumLoss = korali::Inf;
    _suboptimalGenerationCount = 0;
  }
}

void DeepSupervisor::runGeneration()
{
  // Checking that incoming data has a correct format
  _problem->verifyData();

  for (size_t step = 0; step < _stepsPerGeneration; step++)
  {
    // Forward propagating the input values through the training neural network
    _trainingNeuralNetwork->forward(_problem->_inputData);

    // Saving values for the loss function and its gradient
    _currentLoss = 0.0;

    // Allocating space for the loss calculation
    std::vector<std::vector<std::vector<float>>> lossDiff(_problem->_maxTimesteps);
    for (size_t t = 0; t < _problem->_maxTimesteps; t++)
    {
     lossDiff[t].resize(_problem->_trainingBatchSize);
     for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
      lossDiff[t][i].resize(_problem->_solutionSize);
    }

    // Assigning values to evaluation and gradients, depending on the loss function selected
    for (size_t t = 0; t < _problem->_maxTimesteps; t++)
     for (size_t i = 0; i < _problem->_trainingBatchSize; i++)
      for (size_t j = 0; j < _problem->_solutionSize; j++)
      {
        float diff = 0.0;
        if (_lossFunction == "Direct Gradient") diff = _problem->_solutionData[t][i][j];
        if (_lossFunction == "Mean Squared Error") diff = _problem->_solutionData[t][i][j] - _trainingNeuralNetwork->_outputValues[t][i][j];
        lossDiff[t][i][j] = diff;
        _currentLoss += diff * diff;
      }

    // Averaging the loss across the batch size
    _currentLoss = _currentLoss / ((float)_problem->_maxTimesteps * (float)_problem->_trainingBatchSize * 2.0f);

    // Backward propagating the gradients through the training neural network
    _trainingNeuralNetwork->backward(lossDiff);

    // Passing hyperparameter gradients through an ADAM update
    _adamSolver->processResult(_currentLoss, _trainingNeuralNetwork->_hyperparameterGradients);

    // Updating best loss if better, or increasing suboptimal generation count
    if (_currentLoss < _minimumLoss) _minimumLoss = _currentLoss;
    else _suboptimalGenerationCount++;

    // Getting new set of hyperparameters from Adam
    _trainingNeuralNetwork->setHyperparameters(_adamSolver->_currentValue);
  }
}

void DeepSupervisor::finalize()
{
  // Copying hyperparameters and normalization parameters from training to evaluation
  auto hyperparameters = _trainingNeuralNetwork->getHyperparameters();

  for (size_t i = 0; i < _maxThreads; i++)
    _inferenceNeuralNetworks[i]->setHyperparameters(hyperparameters);

  _hyperparameters = hyperparameters;
}

std::vector<float> DeepSupervisor::getTrainingHyperparameters()
{
  return _trainingNeuralNetwork->getHyperparameters();
}

void DeepSupervisor::setTrainingHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _trainingNeuralNetwork->setHyperparameters(hyperparameters);

  // Need to update Adam to continue from there
  for (size_t i = 0; i < hyperparameters.size(); i++)
    _adamSolver->_initialValues[i] = hyperparameters[i];

  // Resetting
  _adamSolver->reset();
}

std::vector<float> DeepSupervisor::getInferenceHyperparameters()
{
  return _inferenceNeuralNetworks[0]->getHyperparameters();
}

void DeepSupervisor::setInferenceHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  for (size_t i = 0; i < _maxThreads; i++)
    _inferenceNeuralNetworks[i]->setHyperparameters(hyperparameters);
}

std::vector<std::vector<std::vector<float>>> DeepSupervisor::getEvaluation(const std::vector<std::vector<std::vector<float>>> &input)
{
  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  // Running the input values through the neural network
  _inferenceNeuralNetworks[curThread]->forward(input);

  return _inferenceNeuralNetworks[curThread]->_outputValues;
}

std::vector<std::vector<std::vector<float>>> DeepSupervisor::getGradients(const std::vector<std::vector<std::vector<float>>> &outputDiffs)
{
  // Getting current thread ID
  size_t curThread = omp_get_thread_num();

  // Backward propagating the gradients through the training neural network
  _inferenceNeuralNetworks[curThread]->backward(outputDiffs);

  // Running backward propagation wrt data
  return _inferenceNeuralNetworks[curThread]->_inputGradients;
}

void DeepSupervisor::printGenerationAfter()
{
  // Printing results so far
  if (_lossFunction == "Mean Squared Error")
  {
    _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentLoss);
    _k->_logger->logInfo("Detailed", " + Suboptimal Generation Count: %lu\n", _suboptimalGenerationCount);
  }
  else if (_lossFunction == "Direct Gradient")
    _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f\n", sqrt(_currentLoss));
}

} // namespace learner

} // namespace solver

} // namespace korali

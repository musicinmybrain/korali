#ifndef _KORALI_SOLVER_DEEPSUPERVISOR_HPP_
#define _KORALI_SOLVER_DEEPSUPERVISOR_HPP_

#include "modules/solver/learner/deepSupervisor/optimizers/fAdaBelief.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fAdam.hpp"

#include "modules/experiment/experiment.hpp"
#include "modules/neuralNetwork/neuralNetwork.hpp"
#include "modules/problem/supervisedLearning/supervisedLearning.hpp"
#include "modules/solver/learner/learner.hpp"

namespace korali
{
namespace solver
{
namespace learner
{
class DeepSupervisor : public Learner
{
  public:
  /**
 * @brief Korali Problem for optimizing NN weights and biases
 */
  problem::SupervisedLearning *_problem;

  /**
 * @brief Korali Experiment for optimizing the NN's weights and biases
 */
  korali::Experiment _optExperiment;

  /**
 * @brief ADAM solver pointer to access directly (for performance)
 */
  korali::fAdam *_adamSolver;

  /**
   * @brief A neural network to be trained based on inputs and solutions
   */
  NeuralNetwork *_trainingNeuralNetwork;

  /**
   * @brief A neural network to perform inference of inputs and obtain gradients, one per thread.
   */
  std::vector<NeuralNetwork *> _inferenceNeuralNetworks;

  std::vector<std::vector<std::vector<float>>> getEvaluation(const std::vector<std::vector<std::vector<float>>> &input) override;
  std::vector<std::vector<std::vector<float>>> getGradients(const std::vector<std::vector<std::vector<float>>> &gradients) override;

  std::vector<float> getTrainingHyperparameters() override;
  void setTrainingHyperparameters(const std::vector<float> &hyperparameters) override;
  std::vector<float> getInferenceHyperparameters() override;
  void setInferenceHyperparameters(const std::vector<float> &hyperparameters) override;

  void finalize() override;
  void initialize() override;
  void runGeneration() override;
  void printGenerationAfter() override;
};

} // namespace learner
} // namespace solver
} // namespace korali

#endif // _KORALI_SOLVER_DEEP_DEEPSUPERVISOR_HPP_

#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepLearner/deepLearner.hpp"

/******************************************************************
 * Performance considerations:
 * + Do not require gradient if the solver is purely stochastic
 *****************************************************************/

namespace korali
{
namespace solver
{
namespace learner
{

void DeepLearner::initialize()
{
 if (_isLearnerInitialized == false)
 {
  // Indicate that the evaluation NN is not yet initialized
  _isEvaluatioNNForwardInitialized = false;
  _isEvaluatioNNBackwardInitialized = false;

  // Getting problem pointer
  _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

  /*****************************************************************
   * Setting up Training Neural Network
   *****************************************************************/

  // Setting NN's input and output dimensions
  size_t outputLayerId = _neuralNetwork->_layers.size() - 1;
  _neuralNetwork->_layers[0]->_nodeCount = _problem->_inputVectorSize;
  _neuralNetwork->_layers[outputLayerId]->_nodeCount = _problem->_solutionVectorSize;

  // Setting training NN
  _trainingNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_neuralNetwork));

  // Creating Training Neural Network internal structures
  _trainingNeuralNetwork->createForwardPipeline(_problem->_trainingBatchSize);
  _trainingNeuralNetwork->createBackwardPipeline();
 }

 // Setting input training data
 _trainingNeuralNetwork->setInput(_problem->_trainingInput);

 /*****************************************************************
 * Setting up Optional Validation Neural Network
 *****************************************************************/
 if (_isLearnerInitialized == false)
 {

  // Initialize validation NN only if validation data has been provided
  if (_problem->_validationBatchSize > 0)
  {
   // Creating Validation NN
   _validationNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_trainingNeuralNetwork));

   // Creating Validation Neural Network internal structures, keeping normalization from training NN
   _validationNeuralNetwork->createForwardPipeline(_problem->_validationBatchSize);
  }
 }

 // Initialize validation NN only if validation data has been provided
 if (_problem->_validationBatchSize > 0)
 {
  // Setting input validation data
  _validationNeuralNetwork->setInput(_problem->_validationInput);
 }

  /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

 if (_isLearnerInitialized == false)
 {
  // Creating evaluation lambda function for optimization
  auto fc = [this, nn = _trainingNeuralNetwork, sol = _problem->_trainingSolution](Sample &sample) { this->evaluateWeightsAndBiases(sample, nn, sol, true); };

  _optExperiment["Problem"]["Type"] = "Optimization";
  _optExperiment["Problem"]["Objective Function"] = fc;

  _hyperparameters["Weights"][0] = { { 0 } };
  _hyperparameters["Bias"][0] = { 0 };

  size_t currentVariable = 0;
  for (size_t i = 1; i < _trainingNeuralNetwork->_layers.size(); i++)
  {
    // Setting value for this layer's xavier constant
    double xavierConstant = sqrt(6.0) / sqrt(_trainingNeuralNetwork->_layers[i]->_nodeCount + _trainingNeuralNetwork->_layers[i - 1]->_nodeCount);

    // Adding layer's weights
    for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
      for (size_t k = 0; k < _trainingNeuralNetwork->_layers[i - 1]->_nodeCount; k++)
      {
        char varName[512];
        sprintf(varName, "Weight [%lu] %lu->%lu", i, j, k);
        std::string varNameString(varName);
        _optExperiment["Variables"][currentVariable]["Name"] = varNameString;

        // Setting initial weight values
        double initialGuess = 0.0;
        if (_weightInitialization == "Xavier")
         initialGuess = xavierConstant * _uniformGenerator->getRandomNumber();

        _hyperparameters["Weights"][i][j][k] = initialGuess;
        _optExperiment["Variables"][currentVariable]["Initial Value"] = initialGuess;
        _optExperiment["Variables"][currentVariable]["Initial Mean"] = initialGuess;
        _optExperiment["Variables"][currentVariable]["Initial Standard Deviation"] = abs(initialGuess)*0.1;
        currentVariable++;
      }

    // Adding layer's biases
    for (size_t j = 0; j < _trainingNeuralNetwork->_layers[i]->_nodeCount; j++)
    {
      char varName[512];
      sprintf(varName, "Bias [%lu] %lu", i, j);
      std::string varNameString(varName);
      _optExperiment["Variables"][currentVariable]["Name"] = varNameString;

      // Setting initial biases values
      double initialGuess = 0.0;
      if (_weightInitialization == "Xavier") initialGuess = xavierConstant * _uniformGenerator->getRandomNumber();

      _hyperparameters["Bias"][i][j] = initialGuess;
      _optExperiment["Variables"][currentVariable]["Initial Value"] = initialGuess;
      _optExperiment["Variables"][currentVariable]["Initial Mean"] = initialGuess;
      _optExperiment["Variables"][currentVariable]["Initial Standard Deviation"] = abs(initialGuess)*0.1;
      currentVariable++;
    }
  }

  _optExperiment["Solver"] = _optimizer;

  _optExperiment["File Output"]["Frequency"] = 0;
  _optExperiment["File Output"]["Enabled"] = false;
  _optExperiment["Console Output"]["Frequency"] = 0;
  _optExperiment["Console Output"]["Verbosity"] = "Silent";
  _optExperiment["Random Seed"] = _k->_randomSeed++;

  // Running initialization to verify that the configuration is correct
  _optEngine.initialize(_optExperiment);
  printf("Initializing Experiment\n");

  // Resetting termination counters
   _currentInactiveSteps = 0;
   _lowestValidationLoss = korali::Inf;
 }

  // Storing input normalization hyperparameters
  _hyperparameters["Input Normalization Means"] = _trainingNeuralNetwork->_inputNormalizationMeans;
  _hyperparameters["Input Normalization Variances"] = _trainingNeuralNetwork->_inputNormalizationVariances;

  _isLearnerInitialized = true;
}

void DeepLearner::runGeneration()
{
  /**************************************************************
  * Training Stage
  *************************************************************/

  // Performs the training phase of the NN, given the solution and the NN's configuration.
  _optExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _optExperiment._currentGeneration + _stepsPerGeneration;
  _optEngine.resume(_optExperiment);

  // Getting results of optimization
  _currentTrainingLoss = _optExperiment["Results"]["Best Sample"]["F(x)"].get<double>();

  // If validation hasn't been provided, store best training NN so far
  if (_problem->_validationBatchSize == 0)
  {
   // Setting input validation data
   Sample newSample;

   // Getting the (so far) optimal training parameters
   newSample["Parameters"] = _optExperiment["Results"]["Best Sample"]["Parameters"];

   for (size_t i = 0; i < _trainingNeuralNetwork->_layers.size(); i++)
   {
    _hyperparameters["Weights"][i] = _trainingNeuralNetwork->_layers[i]->_weights;
    _hyperparameters["Bias"][i] = _trainingNeuralNetwork->_layers[i]->_bias;
   }
  }

  /**************************************************************
  * Validation Stage (Optional)
  *************************************************************/

  // Initialize validation NN only if validation data has been provided
  if (_problem->_validationBatchSize > 0)
  {
   // Setting input validation data
   Sample validationSample;
   validationSample["Parameters"] = _optExperiment["Results"]["Best Sample"]["Parameters"];
   evaluateWeightsAndBiases(validationSample, _validationNeuralNetwork, _problem->_validationSolution);

   // Getting results of optimization
   _currentValidationLoss = -KORALI_GET(double, validationSample, "F(x)");

   // If validation is better, saving it as the best network for use as for test batch later.
   _currentInactiveSteps++;
   if (_currentValidationLoss < _lowestValidationLoss)
   {
     // Reseting inactive counter
     _currentInactiveSteps = 0;

     // Saving lowest validation loss
     _lowestValidationLoss = _currentValidationLoss;

     // Storing best NN
     for (size_t i = 0; i < _validationNeuralNetwork->_layers.size(); i++)
     {
      _hyperparameters["Weights"][i] = _validationNeuralNetwork->_layers[i]->_weights;
      _hyperparameters["Bias"][i] = _validationNeuralNetwork->_layers[i]->_bias;
     }
   }
  }
}

void DeepLearner::finalize()
{
 _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _currentTrainingLoss);
}

std::vector<std::vector<double>> DeepLearner::evaluate(const std::vector<std::vector<double>> &inputBatch)
{
  // Building evaluation NN's forward pipeline if not yet initialized
  if (_isEvaluatioNNForwardInitialized == false)
  {
   _evaluationNeuralNetwork = dynamic_cast<NeuralNetwork *>(Module::duplicate(_neuralNetwork));
   _evaluationNeuralNetwork->_inputNormalizationMeans = _hyperparameters["Input Normalization Means"].get<std::vector<double>>();
   _evaluationNeuralNetwork->_inputNormalizationVariances = _hyperparameters["Input Normalization Variances"].get<std::vector<double>>();
   _evaluationNeuralNetwork->createForwardPipeline(inputBatch.size());
   _isEvaluatioNNForwardInitialized = true;
  }

  for (size_t i = 0; i < _evaluationNeuralNetwork->_layers.size(); i++)
  {
   _evaluationNeuralNetwork->_layers[i]->_weights = _hyperparameters["Weights"][i].get<std::vector<std::vector<double>>>();
   _evaluationNeuralNetwork->_layers[i]->_bias = _hyperparameters["Bias"][i].get<std::vector<double>>();
  }

  _evaluationNeuralNetwork->updateWeightsAndBias();

  // Updating NN inputs
  _evaluationNeuralNetwork->setInput(inputBatch);

  // Running the input values through the neural network
  _evaluationNeuralNetwork->forward();

  return _evaluationNeuralNetwork->_outputValues;
}

std::vector<std::vector<double>> DeepLearner::getGradients(const std::vector<std::vector<double>> &outputDiffs)
{
 // Building evaluation NN's backward pipeline if not yet initialized
 if (_isEvaluatioNNBackwardInitialized == false)
 {
  if (_isEvaluatioNNForwardInitialized == false)
   KORALI_LOG_ERROR("Attempting to get gradients but forward inference has not yet been performed.\n");

  _evaluationNeuralNetwork->createBackwardPipeline();

  _isEvaluatioNNBackwardInitialized = true;
 }

 size_t batchSize = _evaluationNeuralNetwork->_outputValues.size();
 size_t outputSize = _evaluationNeuralNetwork->_outputValues[0].size();

 // Converting precision
 std::vector<float> singlePrecisionDiffs(batchSize * outputSize);
 for (size_t i = 0; i < batchSize; i++)
   for (size_t j = 0; j < outputSize; j++)
    singlePrecisionDiffs[i*outputSize + j] = outputDiffs[i][j];

 // Running backward propagation wrt data
 _evaluationNeuralNetwork->backwardData(singlePrecisionDiffs);

  return _evaluationNeuralNetwork->_inputGradient;
}

void DeepLearner::printGenerationAfter()
{
  // Printing results so far
  _k->_logger->logInfo("Normal", "Training Loss: %.15f\n", _currentTrainingLoss);
  if (_problem->_validationBatchSize > 0)
  {
   _k->_logger->logInfo("Normal", "Current Validation Loss: %.15f\n", _currentValidationLoss);
   _k->_logger->logInfo("Normal", "Lowest Validation Loss: %.15f\n", _lowestValidationLoss);
   _k->_logger->logInfo("Normal", "Inactive Step Counter: %lu\n", _currentInactiveSteps);
  }
}

void DeepLearner::evaluateWeightsAndBiases(Sample &sample, NeuralNetwork* nn, const std::vector<std::vector<double>>& solution, bool getGradients)
{
  // Setting weights and biases
  auto parameters = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Updating training network's weights and biases
  size_t currentVariable = 0;
  for (size_t i = 1; i < nn->_layers.size(); i++)
  {
   nn->_layers[i]->_weights.resize(nn->_layers[i]->_nodeCount);
   nn->_layers[i]->_bias.resize(nn->_layers[i]->_nodeCount);

    // Adding layer's weights
    for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
    {
      nn->_layers[i]->_weights[j].resize(nn->_layers[i - 1]->_nodeCount);

      for (size_t k = 0; k < nn->_layers[i - 1]->_nodeCount; k++)
        nn->_layers[i]->_weights[j][k] = parameters[currentVariable++];
    }

    // Adding layer's biases
    for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
      nn->_layers[i]->_bias[j] = parameters[currentVariable++];
  }

  // Updating the trainingnetwork's weights and biases
  nn->updateWeightsAndBias();

  // Running the input values through the training neural network
  nn->forward();

  // Getting NN's dimensions
  size_t batchSize = nn->_outputValues.size();
  size_t outputSize = nn->_outputValues[0].size();

  // Calculating mean square error
  double meanSquaredError = 0.0;

  // Saving values for the last layer's diff
  std::vector<float> outputDiff(batchSize * outputSize);

  for (size_t i = 0; i < batchSize; i++)
    for (size_t j = 0; j < outputSize; j++)
    {
      double diff = solution[i][j] - nn->_outputValues[i][j];
      outputDiff[i * outputSize + j] = diff;
      meanSquaredError += diff * diff;
    }

  meanSquaredError = meanSquaredError / ((double)batchSize * 2);

  // Saving the negative of the error because we want to minimize it
  sample["F(x)"] = -meanSquaredError;

  if (getGradients)
  {
   // Running backward propagation
   nn->backwardWeightsAndBias(outputDiff);

   // Copying back the gradients and biases back
   std::vector<double> gradientVector(currentVariable);
   currentVariable = 0;
   for (size_t i = 1; i < nn->_layers.size(); i++)
   {
     // Adding layer's weights
     for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
       for (size_t k = 0; k < nn->_layers[i - 1]->_nodeCount; k++)
         gradientVector[currentVariable++] = nn->_layers[i]->_weights[j][k];

     // Adding layer's biases
     for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
       gradientVector[currentVariable++] = nn->_layers[i]->_bias[j];
   }

   sample["Gradient"] = gradientVector;
  }
}

} // namespace learner

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/optimizer/LMCMAES/LMCMAES.hpp"
#include "sample/sample.hpp"

#include <algorithm> // std::sort
#include <chrono>
#include <numeric> // std::iota
#include <stdio.h>
#include <unistd.h>

@startNamespace

void @className::setInitialConfiguration()
{
  _variableCount = _k->_variables.size();

  // Establishing optimization goal
  _bestEverValue = -std::numeric_limits<double>::infinity();

  _previousBestValue = _bestEverValue;
  _currentBestValue = _bestEverValue;

  if (_targetDistanceCoefficients.size() == 0) _targetDistanceCoefficients = {double(_variableCount), 0.0, 0.0};

  if (_targetDistanceCoefficients.size() != 3)
    KORALI_LOG_ERROR("LMCMAES requires 3 parameters for 'Target Distance Coefficients' (%zu provided).\n", _targetDistanceCoefficients.size());

  if (_muValue == 0) _muValue = _populationSize / 2;
  if (_subsetSize == 0) _subsetSize = 4 + std::floor(3 * std::log(double(_variableCount)));
  if (_cumulativeCovariance == 0.0) _cumulativeCovariance = 1.0 / ((double)_subsetSize);
  if (_choleskyMatrixLearningRate == 0.0) _choleskyMatrixLearningRate = 1.0 / (10.0 * std::log((double)_variableCount + 1.0));
  if (_targetDistanceCoefficients.empty()) _targetDistanceCoefficients = {(double)_variableCount, 0.0, 0.0};
  if (_setUpdateInterval == 0) _setUpdateInterval = std::max(std::floor(std::log(_variableCount)), 1.0);

  _chiSquareNumber = sqrt((double)_variableCount) * (1. - 1. / (4. * _variableCount) + 1. / (21. * _variableCount * _variableCount));
  _sigmaExponentFactor = 0.0;
  _conjugateEvolutionPathL2Norm = 0.0;

  // Allocating Memory
  _samplePopulation.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; i++) _samplePopulation[i].resize(_variableCount);

  _evolutionPath.resize(_variableCount);
  _meanUpdate.resize(_variableCount);
  _currentMean.resize(_variableCount);
  _previousMean.resize(_variableCount);
  _bestEverVariables.resize(_variableCount);
  _currentBestVariables.resize(_variableCount);
  _randomVector.resize(_variableCount);
  _choleskyFactorVectorProduct.resize(_variableCount);
  _standardDeviation.resize(_variableCount);

  _muWeights.resize(_muValue);

  _sortingIndex.resize(_populationSize);
  _valueVector.resize(_populationSize);

  _evolutionPathWeights.resize(_subsetSize);
  _subsetHistory.resize(_subsetSize);
  _subsetUpdateTimes.resize(_subsetSize);

  std::fill(_evolutionPathWeights.begin(), _evolutionPathWeights.end(), 0.0);
  std::fill(_subsetHistory.begin(), _subsetHistory.end(), 0);
  std::fill(_subsetUpdateTimes.begin(), _subsetUpdateTimes.end(), 0);

  _inverseVectors.resize(_subsetSize);
  _evolutionPathHistory.resize(_subsetSize);
  for (size_t i = 0; i < _subsetSize; ++i)
  {
    _inverseVectors[i].resize(_variableCount);
    _evolutionPathHistory[i].resize(_variableCount);
    std::fill(_inverseVectors[i].begin(), _inverseVectors[i].end(), 0.0);
    std::fill(_evolutionPathHistory[i].begin(), _evolutionPathHistory[i].end(), 0.0);
  }

  // Initializing variable defaults
  for (size_t i = 0; i < _variableCount; i++)
  {
    /* init mean if not defined */
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialValue = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound) * 0.5;
    }

    /* calculate stddevs */
    if (std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
      if ((std::isfinite(_k->_variables[i]->_lowerBound) && std::isfinite(_k->_variables[i]->_upperBound)) == false)
        KORALI_LOG_ERROR("Either Lower/Upper Bound or Initial Value of variable \'%s\' must be defined.\n", _k->_variables[i]->_name.c_str());
      _standardDeviation[i] = 0.3 * (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound);
    }
    else
      _standardDeviation[i] = _k->_variables[i]->_initialStandardDeviation;
  }

  _sigma = _initialSigma;

  if (_muType == "Linear")
    for (size_t i = 0; i < _muValue; i++) _muWeights[i] = _muValue - i;
  else if (_muType == "Equal")
    for (size_t i = 0; i < _muValue; i++) _muWeights[i] = 1.;
  else if (_muType == "Logarithmic")
    for (size_t i = 0; i < _muValue; i++) _muWeights[i] = log(std::max((double)_muValue, 0.5 * _populationSize) + 0.5) - log(i + 1.);
  else
    KORALI_LOG_ERROR("Invalid setting of Mu Type (%s) (Linear, Equal, and Logarithmic accepted).", _muType.c_str());

  if ((_randomNumberDistribution != "Normal") && (_randomNumberDistribution != "Uniform")) KORALI_LOG_ERROR("Invalid setting of Random Number Distribution (%s) (Normal and Uniform accepted).", _randomNumberDistribution.c_str());
  _normalRandomNumbers = (_randomNumberDistribution == "Normal");

  // Normalize weights vector and set mueff
  double s1 = 0.0;
  double s2 = 0.0;

  for (size_t i = 0; i < _muValue; i++)
  {
    s1 += _muWeights[i];
    s2 += _muWeights[i] * _muWeights[i];
  }
  _effectiveMu = s1 * s1 / s2;

  for (size_t i = 0; i < _muValue; i++) _muWeights[i] /= s1;

  if (_initialSigma <= 0.0)
    KORALI_LOG_ERROR("Invalid Initial Sigma (must be greater 0.0, is %lf).", _initialSigma);
  if (_cumulativeCovariance <= 0.0)
    KORALI_LOG_ERROR("Invalid Initial Cumulative Covariance (must be greater 0.0).");
  if (_sigmaCumulationFactor <= 0.0)
    KORALI_LOG_ERROR("Invalid Sigma Cumulative Covariance (must be greater 0.0).");
  if (_dampFactor <= 0.0)
    KORALI_LOG_ERROR("Invalid Damp Factor (must be greater 0.0).");
  if (_choleskyMatrixLearningRate <= 0.0 || _choleskyMatrixLearningRate > 1.0)
    KORALI_LOG_ERROR("Invalid Cholesky Matrix Learning Rate (must be in (0, 1], is %lf).", _choleskyMatrixLearningRate);
  if (_setUpdateInterval <= 0.0)
    KORALI_LOG_ERROR("Invalid Set Update Interval(must be greater 0, is %zu).", _setUpdateInterval);

  _infeasibleSampleCount = 0;
  _sqrtInverseCholeskyRate = std::sqrt(1.0 - _choleskyMatrixLearningRate);

  for (size_t i = 0; i < _variableCount; i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialValue;
}

void @className::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  prepareGeneration();

  // Initializing Sample Evaluation
  std::vector<Sample> samples(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    samples[i]["Module"] = "Problem";
    samples[i]["Operation"] = "Evaluate";
    samples[i]["Parameters"] = _samplePopulation[i];
    samples[i]["Sample Id"] = i;
    _modelEvaluationCount++;
    KORALI_START(samples[i]);
  }

  // Waiting for samples to finish
  KORALI_WAITALL(samples);
  updateDistribution(samples);
}

void @className::prepareGeneration()
{
  for (size_t i = 0; i < _populationSize; ++i)
  {
    bool isFeasible;
    do
    {
      sampleSingle(i);
      isFeasible = isSampleFeasible(_samplePopulation[i]);

      if (isFeasible == false) _infeasibleSampleCount++;

    } while (isFeasible == false);
  }
}

void @className::sampleSingle(size_t sampleIdx)
{
  if (_symmetricSampling || (sampleIdx % 2) == 0)
  {
    choleskyFactorUpdate(sampleIdx);
    for (size_t d = 0; d < _variableCount; ++d)
    {
      _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma * _standardDeviation[d] * _choleskyFactorVectorProduct[d];
    }
  }
  else
  {
    for (size_t d = 0; d < _variableCount; ++d)
      //_samplePopulation[sampleIdx][d] = 2.0*_currentMean[d] - _samplePopulation[sampleIdx-1][d]; // version from [Loshchilov2015]
      _samplePopulation[sampleIdx][d] = _currentMean[d] - _sigma * _standardDeviation[d] * _choleskyFactorVectorProduct[d]; // version from Loshchilov's code
  }
}

void @className::updateDistribution(std::vector<Sample> &samples)
{
  // Processing results
  for (size_t i = 0; i < _populationSize; i++)
    _valueVector[i] = KORALI_GET(double, samples[i], "F(x)");

  /* Generate _sortingIndex */
  sort_index(_valueVector, _sortingIndex);

  /* update current best */
  _previousBestValue = _currentBestValue;
  _currentBestValue = _valueVector[0];
  for (size_t d = 0; d < _variableCount; ++d) _currentBestVariables[d] = _samplePopulation[_sortingIndex[0]][d];

  /* update xbestever */
  if (_currentBestValue > _bestEverValue)
  {
    _bestEverValue = _currentBestValue;

    for (size_t d = 0; d < _variableCount; ++d) _bestEverVariables[d] = _currentBestVariables[d];
  }

  /* set weights */
  for (size_t d = 0; d < _variableCount; ++d)
  {
    _previousMean[d] = _currentMean[d];
    _currentMean[d] = 0.;
    for (size_t i = 0; i < _muValue; ++i)
      _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];

    _meanUpdate[d] = (_currentMean[d] - _previousMean[d]) / (_sigma * _standardDeviation[d]);
  }

  /* update evolution path */
  _conjugateEvolutionPathL2Norm = 0.0;
  for (size_t d = 0; d < _variableCount; ++d)
  {
    _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + sqrt(_cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu) * _meanUpdate[d];
    _conjugateEvolutionPathL2Norm += std::pow(_evolutionPath[d], 2);
  }
  _conjugateEvolutionPathL2Norm = std::sqrt(_conjugateEvolutionPathL2Norm);

  /* update stored paths */
  if ((_k->_currentGeneration - 1) % _setUpdateInterval == 0)
  {
    updateSet();
    updateInverseVectors();
  }

  /* update sigma */
  updateSigma();

  /* numerical error management */
  numericalErrorTreatment();
}

void @className::choleskyFactorUpdate(size_t sampleIdx)
{
  /* randomly select subsetStartIndex */
  double ms = 4.0;
  if (sampleIdx == 0) ms *= 10;
  size_t subsetStartIndex = _subsetSize - std::min((size_t)std::floor(ms * std::abs(_normalGenerator->getRandomNumber())) + 1, _subsetSize);

  if (_normalRandomNumbers)
    for (size_t d = 0; d < _variableCount; ++d) _randomVector[d] = _normalGenerator->getRandomNumber();
  else /* (_randomNumberDistribution == "Uniform" */
    for (size_t d = 0; d < _variableCount; ++d) _randomVector[d] = 2 * _uniformGenerator->getRandomNumber() - 1.0;

  for (size_t d = 0; d < _variableCount; ++d) _choleskyFactorVectorProduct[d] = _randomVector[d];

  for (size_t i = subsetStartIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];

    double k = 0.0;
    for (size_t d = 0; d < _variableCount; ++d) k += _inverseVectors[idx][d] * _randomVector[d];
    k *= _evolutionPathWeights[idx];

    _minCholeskyFactorVectorProductEntry = std::numeric_limits<double>::infinity();
    _maxCholeskyFactorVectorProductEntry = -std::numeric_limits<double>::infinity();
    for (size_t d = 0; d < _variableCount; ++d)
    {
      _choleskyFactorVectorProduct[d] = _sqrtInverseCholeskyRate * _choleskyFactorVectorProduct[d] + k * _evolutionPathHistory[idx][d];
      if (_choleskyFactorVectorProduct[d] < _minCholeskyFactorVectorProductEntry) _minCholeskyFactorVectorProductEntry = _choleskyFactorVectorProduct[d];
      if (_choleskyFactorVectorProduct[d] > _maxCholeskyFactorVectorProductEntry) _maxCholeskyFactorVectorProductEntry = _choleskyFactorVectorProduct[d];
    }
  }
}

void @className::updateSet()
{
  size_t t = std::floor(double(_k->_currentGeneration - 1.0) / double(_setUpdateInterval));

  if (t < _subsetSize)
  {
    _replacementIndex = t;
    _subsetHistory[t] = t;
    _subsetUpdateTimes[t] = t * _setUpdateInterval + 1;
  }
  else
  {
    double tmparg = 0.0, minarg, target;
    minarg = std::numeric_limits<double>::max();
    for (size_t i = 1; i < _subsetSize; ++i)
    {
      /* `target` by default equals _variableCount */
      target = _targetDistanceCoefficients[0] + _targetDistanceCoefficients[1] * std::pow(double(i + 1.) / double(_subsetSize), _targetDistanceCoefficients[2]);
      tmparg = _subsetUpdateTimes[_subsetHistory[i]] - _subsetUpdateTimes[_subsetHistory[i - 1]] - target;
      if (tmparg < minarg)
      {
        minarg = tmparg;
        _replacementIndex = i;
      }
    }
    if (tmparg > 0) _replacementIndex = 0; /* if all evolution paths at a distance of `target` or larger, update oldest */
    size_t jtmp = _subsetHistory[_replacementIndex];
    for (size_t i = _replacementIndex; i < _subsetSize - 1; ++i) _subsetHistory[i] = _subsetHistory[i + 1];

    _subsetHistory[_subsetSize - 1] = jtmp;
    _subsetUpdateTimes[jtmp] = t * _setUpdateInterval + 1;
  }

  /* insert new evolution path */
  std::copy(_evolutionPath.begin(), _evolutionPath.end(), _evolutionPathHistory[_subsetHistory[_replacementIndex]].begin());
}

void @className::updateInverseVectors()
{
  double djt, k;
  double fac = std::sqrt(1.0 + _choleskyMatrixLearningRate / (1.0 - _choleskyMatrixLearningRate));

  /* update all inverse vectors and evolution path weights onwards from replacement index */
  for (size_t i = _replacementIndex; i < _subsetSize; ++i)
  {
    size_t idx = _subsetHistory[i];

    double v2L2 = 0.0;
    for (size_t d = 0; d < _variableCount; ++d) v2L2 += _inverseVectors[idx][d] * _inverseVectors[idx][d];

    k = 0.0;
    if (v2L2 > 0.0)
    {
      djt = _sqrtInverseCholeskyRate / v2L2 * (1.0 - 1.0 / (fac * std::sqrt(v2L2)));

      k = 0.0;
      for (size_t d = 0; d < _variableCount; ++d) k += _inverseVectors[idx][d] * _evolutionPathHistory[idx][d];
      k *= djt;

      _evolutionPathWeights[idx] = _sqrtInverseCholeskyRate / v2L2 * (std::sqrt(1.0 + _choleskyMatrixLearningRate / (1.0 - _choleskyMatrixLearningRate) * v2L2) - 1.0);
    }

    for (size_t d = 0; d < _variableCount; ++d)
      _inverseVectors[idx][d] = _sqrtInverseCholeskyRate * _evolutionPathHistory[idx][d] - k * _inverseVectors[idx][d];
  }
}

void @className::updateSigma()
{
  _sigma *= exp(_sigmaCumulationFactor / _dampFactor * (_conjugateEvolutionPathL2Norm / _chiSquareNumber - 1.));

  /* escape flat evaluation */
  if (_currentBestValue == _valueVector[_sortingIndex[(int)_muValue]])
  {
    _sigma *= exp(0.2 + _sigmaCumulationFactor / _dampFactor);
    _k->_logger->logWarning("Detailed", "Sigma increased due to equal function values.\n");
  }

  /* upper bound check for _sigma */
  if (_sigma > 2.0 * _initialSigma)
  {
    _k->_logger->logInfo("Detailed", "Sigma exceeding initial sigma by a factor of two (%f > %f), increase value of Initial Sigma.\n", _sigma, 2.0 * _initialSigma);
    if (_isSigmaBounded)
    {
      _sigma = 2.0 * _initialSigma;
      _k->_logger->logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
    }
  }
}

void @className::numericalErrorTreatment()
{
  //treat numerical precision provblems
  //TODO
}

/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/

void @className::sort_index(const std::vector<double> &vec, std::vector<size_t> &sortingIndex) const
{
  // initialize original _sortingIndex locations
  std::iota(std::begin(sortingIndex), std::end(sortingIndex), (size_t)0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(sortingIndex), std::end(sortingIndex), [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; });
}

void @className::printGenerationBefore() { return; }

void @className::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
  _k->_logger->logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);
  _k->_logger->logInfo("Normal", "Cholesky Factor:        Min = %+6.3e -  Max = %+6.3e\n", _minCholeskyFactorVectorProductEntry, _maxCholeskyFactorVectorProductEntry);
  _k->_logger->logInfo("Normal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);

  _k->_logger->logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < _variableCount; d++) _k->_logger->logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  _k->_logger->logInfo("Detailed", "Covariance Matrix:\n");
}

void @className::finalize()
{
  // Updating Results
  (*_k)["Results"]["Best Sample"]["F(x)"] = _bestEverValue;
  (*_k)["Results"]["Best Sample"]["Parameters"] = _bestEverVariables;

  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _variableCount; ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

@moduleAutoCode

@endNamespace

#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/problem.hpp"
#include "modules/solver/optimizer/Rprop/Rprop.hpp"
#include "sample/sample.hpp"

#include <stdio.h>

@startNamespace

void @className::setInitialConfiguration()
{
  _variableCount = _k->_variables.size();

  for (size_t i = 0; i < _variableCount; i++)
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
      KORALI_LOG_ERROR("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  _currentVariable.resize(_variableCount, 0.0);
  for (size_t i = 0; i < _variableCount; i++)
    _currentVariable[i] = _k->_variables[i]->_initialValue;

  _bestEverVariables = _currentVariable;
  _delta.resize(_variableCount, _delta0);
  _currentGradient.resize(_variableCount, 0);
  _bestEverGradient.resize(_variableCount, 0);
  _previousGradient.resize(_variableCount, 0.0);

  _bestEverValue = Inf;
  _xDiff = Inf;
  _maxStallCounter = 0;
  _normPreviousGradient = Inf;
  _previousBestValue = Inf;
}

void @className::evaluateFunctionAndGradient(Sample &sample)
{
  // Initializing Sample Evaluation
  sample["Module"] = "Problem";
  sample["Operation"] = "Evaluate With Gradients";
  sample["Parameters"] = _currentVariable;
  sample["Sample Id"] = 0;
  _modelEvaluationCount++;
  KORALI_START(sample);

  // Waiting for samples to finish
  KORALI_WAIT(sample);

  // Processing results
  // The 'minus' is there because we want Rprop to do Maximization be default.
  _currentBestValue = -KORALI_GET(double, sample, "F(x)");
  _currentGradient = KORALI_GET(std::vector<double>, sample, "Gradient");

  if (_currentGradient.size() != _variableCount)
    KORALI_LOG_ERROR("Size of sample's gradient evaluations vector (%lu) is different from the number of problem variables defined (%lu).\n", _currentGradient.size(), _variableCount);

  for (size_t i = 0; i < _variableCount; i++)
    _currentGradient[i] = -_currentGradient[i];
}

void @className::runGeneration(void)
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  Sample sample;

  evaluateFunctionAndGradient(sample);

  Update_iminus();

  _previousBestValue = _currentBestValue;
  _previousGradient = _currentGradient;
  _normPreviousGradient = vectorNorm(_previousGradient);

  if (_currentBestValue < _bestEverValue)
  {
    _bestEverValue = _currentBestValue;

    std::vector<double> tmp(_variableCount);
    for (size_t j = 0; j < _variableCount; j++) tmp[j] = _bestEverVariables[j] - _currentVariable[j];
    _xDiff = vectorNorm(tmp);
    _bestEverVariables = _currentVariable;
    _bestEverGradient = _currentGradient;
    _maxStallCounter = 0;
  }
  else
  {
    _maxStallCounter++;
  }
}

// Rprop_plus
void @className::Update_plus(void)
{
  for (size_t i = 0; i < _variableCount; i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentVariable[i] += _currentGradient[i];
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      _currentVariable[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else
    {
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentVariable[i] += _currentGradient[i];
    }
  }
}

// Rprop_minus
void @className::Update_minus(void)
{
  for (size_t i = 0; i < _variableCount; i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
    else if (productGradient < 0)
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
    _currentVariable[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

// iRprop_plus
void @className::Update_iplus(void)
{
  for (size_t i = 0; i < _variableCount; i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentVariable[i] += _currentGradient[i];
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      if (_currentBestValue > _previousBestValue) _currentVariable[i] -= _currentGradient[i];
      _currentGradient[i] = 0;
    }
    else
    {
      _currentGradient[i] = -sign(_currentGradient[i]) * _delta[i];
      _currentVariable[i] += _currentGradient[i];
    }
  }
}

// iRprop_minus
void @className::Update_iminus(void)
{
  for (size_t i = 0; i < _variableCount; i++)
  {
    double productGradient = _previousGradient[i] * _currentGradient[i];
    if (productGradient > 0)
    {
      _delta[i] = std::min(_delta[i] * _etaPlus, _deltaMax);
    }
    else if (productGradient < 0)
    {
      _delta[i] = std::max(_delta[i] * _etaMinus, _deltaMin);
      _currentGradient[i] = 0;
    }
    _currentVariable[i] += -sign(_currentGradient[i]) * _delta[i];
  }
}

void @className::printGenerationBefore()
{
  return;
}

void @className::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "X = [ ");
  for (size_t k = 0; k < _variableCount; k++) _k->_logger->logData("Normal", " %.5le  ", _currentVariable[k]);
  _k->_logger->logData("Normal", " ]\n");

  _k->_logger->logInfo("Normal", "F(X) = %le \n", _currentBestValue);

  _k->_logger->logInfo("Normal", "DF(X) = [ ");
  for (size_t k = 0; k < _variableCount; k++) _k->_logger->logData("Normal", " %.5le  ", _currentGradient[k]);
  _k->_logger->logData("Normal", " ]\n");

  _k->_logger->logInfo("Normal", "X_best = [ ");
  for (size_t k = 0; k < _variableCount; k++) _k->_logger->logData("Normal", " %.5le  ", _bestEverVariables[k]);
  _k->_logger->logData("Normal", " ]\n");
}

void @className::finalize()
{
  // Updating Results
  (*_k)["Results"]["Best Sample"]["F(x)"] = _bestEverValue;
  (*_k)["Results"]["Best Sample"]["Gradient(x)"] = _bestEverGradient;
  (*_k)["Results"]["Best Sample"]["Parameters"] = _bestEverVariables;
  return;
}

@moduleAutoCode

@endNamespace

#include "engine.hpp"
#include "modules/solver/optimizer/MOCMAES/MOCMAES.hpp"
#include "sample/sample.hpp"

#include <gsl/gsl_linalg.h> // Cholesky

namespace korali
{
namespace solver
{
namespace optimizer
{
void MOCMAES::setInitialConfiguration()
{
  knlohmann::json problemConfig = (*_k)["Problem"];
  _variableCount = _k->_variables.size();

  // Initializing Population Size
  if (_populationSize == 0) _populationSize = std::ceil(4.0 + floor(3 * log((double)_variableCount)));
  if (_muValue == 0) _muValue = _populationSize / 2.0;

  // Allocating Sample Memory
  _sampleCollection.resize(0);
  _sampleValueCollection.resize(0);
  _samplePopulation.resize(_populationSize);
  _previousSamplePopulation.resize(_populationSize);
  _currentValues.resize(_populationSize);
  _previousValues.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    _samplePopulation[i].resize(_variableCount);
    _previousSamplePopulation[i].resize(_variableCount);
    _currentValues.resize(_numObjectives);
    _previousValues.resize(_numObjectives);
  }

  // Allocating evaluation vectors
  _sortingIndex.resize(_populationSize);

  // Establishing optimization goal
  _bestEverValues.resize(_numObjectives);
  _bestEverVariables.resize(_numObjectives);
  _previousBestValues.resize(_numObjectives);
  _previousBestVariables.resize(_numObjectives);
  _currentBestValues.resize(_numObjectives);
  _currentBestVariables.resize(_numObjectives);
  for (size_t i = 0; i < _numObjectives; ++i)
  {
    _previousValues[i].resize(_numObjectives);
    _currentValues[i].resize(_numObjectives);
    _bestEverValues[i] = -std::numeric_limits<double>::infinity();
    _bestEverVariables[i].resize(_variableCount);
    _previousBestValues[i] = _bestEverValues[i];
    _previousBestVariables.resize(_variableCount);
    _currentBestValues[i] = _bestEverValues[i];
    _currentBestVariables.resize(_variableCount);
  }

  // Initializing variable defaults
  double trace = 0.0;
  double minSdev = std::numeric_limits<double>::infinity();
  double maxSdev = -std::numeric_limits<double>::infinity();
  for (size_t i = 0; i < _variableCount; i++)
  {
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialValue = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound) * 0.5;
    }

    if (std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound) * 0.3;
    }
    trace += _k->_variables[i]->_initialStandardDeviation * _k->_variables[i]->_initialStandardDeviation;
    if (_k->_variables[i]->_initialStandardDeviation < minSdev) minSdev = _k->_variables[i]->_initialStandardDeviation;
    if (_k->_variables[i]->_initialStandardDeviation > maxSdev) maxSdev = _k->_variables[i]->_initialStandardDeviation;
  }

  // Initializing proposal
  _sigma.resize(_populationSize);
  _evolutionPaths.resize(_populationSize);
  _successProbabilities.resize(_populationSize);
  _covarianceMatrix.resize(_populationSize);

  // Init covariance and sigma
  for (size_t i = 0; i < _populationSize; ++i)
  {
    _sigma[i] = sqrt(trace / _k->_variables.size());
    _evolutionPaths[i].resize(_variableCount);
    _successProbabilities[i] = _targetSuccessRate;
    _covarianceMatrix[i].resize(_variableCount * _variableCount);
    for (size_t d = 0; d < _variableCount; ++d)
      _covarianceMatrix[i][d * _k->_variables.size() + d] = _k->_variables[d]->_initialStandardDeviation * _k->_variables[d]->_initialStandardDeviation / (_sigma[i] * _sigma[i]);
  }

  // MOCMAES variables
  if(_evolutionPathAdaptionStrength < 0.0) _evolutionPathAdaptionStrength = 2.0/(_variableCount + 2.0);
  if(_covarianceLearningRate < 0.0) _covarianceLearningRate = 2.0/(_variableCount*_variableCount + 6.0);
  if ((_successLearningRate <= 0.0) || (_successLearningRate > 1.0))
    KORALI_LOG_ERROR("Invalid Global Success Learning Rate (%f), must be greater than 0.0 and less or equal to 1.0\n", _successLearningRate);
  if ((_targetSuccessRate <= 0.0) || (_targetSuccessRate > 1.0))
    KORALI_LOG_ERROR("Invalid Target Success Rate (%f), must be greater than 0.0 and less or equal to 1.0\n", _targetSuccessRate);
  if (_covarianceLearningRate <= 0.0)
    KORALI_LOG_ERROR("Invalid Covariance Learning Rate (%f), must be greater than 0.0\n", _covarianceLearningRate);
  if (_evolutionPathAdaptionStrength <= 0.0 || _evolutionPathAdaptionStrength > 1.0)
    KORALI_LOG_ERROR("Invalid Evolution Path Adaption Strength (%f), must be greater 0.0 and less or equal to 1.0\n", _evolutionPathAdaptionStrength);

  // Termination criteria
  _infeasibleSampleCount = 0;
  _currentMinStandardDeviations.resize(_populationSize);
  _currentMaxStandardDeviations.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; ++i)
  {
    _currentMinStandardDeviations[i] = minSdev;
    _currentMaxStandardDeviations[i] = maxSdev;
    _currentMinValueDifferences[i] = std::numeric_limits<double>::infinity();
  }
}

void MOCMAES::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  prepareGeneration();

  // Initializing Sample Evaluation
  std::vector<Sample> samples(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    samples[i]["Module"] = "Problem";
    samples[i]["Operation"] = "Evaluate";
    samples[i]["Parameters"] = _samplePopulation[i];
    samples[i]["Sample Id"] = i;
    _modelEvaluationCount++;

    KORALI_START(samples[i]);
  }

  // Waiting for samples to finish
  KORALI_WAITALL(samples);

  // Gathering evaluations
  for (size_t i = 0; i < _populationSize; i++)
    _currentValues[i] = KORALI_GET(std::vector<double>, samples[i], "F(x)");

  sortSampleIndices();
  updateDistribution();
}

void MOCMAES::prepareGeneration()
{
    for(size_t i = 0; i < _populationSize; ++i)
    {   
        _previousValues[i] = _currentValues[i];
        _previousSamplePopulation[i] = _samplePopulation[i];
        sampleSingle(i);
    }

}

void MOCMAES::sampleSingle(size_t sampleIdx)
{
    // Set mean
    _multinormalGenerator->_meanVector = _previousSamplePopulation[sampleIdx];

    // Sett covariance
    _multinormalGenerator->_sigma = _covarianceMatrix[sampleIdx];
    gsl_matrix_view sig = gsl_matrix_view_array(_multinormalGenerator->_sigma.data(), _variableCount, _variableCount);

    // Calculate Cholesky decomp and scale with sigma
    int err = gsl_linalg_cholesky_decomp1(&sig.matrix);
    err = gsl_matrix_scale(&sig.matrix, _sigma[sampleIdx]);

    // generate new sample
    _multinormalGenerator->updateDistribution();
    _multinormalGenerator->getRandomVector(_samplePopulation[sampleIdx].data(), _variableCount);
}


void MOCMAES::sortSampleIndices()
{

    // Find rank based on non-dominance
    std::vector<size_t> rank(_populationSize, 0);
    std::vector<size_t> maxNBetter(_populationSize, 0);
    for(size_t r = _populationSize; r <= _populationSize; ++r) // start with the best ranks
    {
        size_t minMaxNBetter = _numObjectives;
        for(size_t i = 0; i < _populationSize; ++i) if(rank[i] == 0) // sample i not yet ranked
        {
            for(size_t j = 0; j < _populationSize; ++j) if(i != j && rank[j] == 0) // compare not with yourself and not with already ranked samples
            {
                // Compare with other sample and count how many times it is better
                size_t nBetter = 0;
                for(size_t k = 0; k < _numObjectives; ++k) if (_currentValues[i][k] < _currentValues[j][k]) nBetter++;
                if (nBetter > maxNBetter[i]) maxNBetter[i] = nBetter;
            }
            if(maxNBetter[i] < minMaxNBetter) minMaxNBetter = maxNBetter[i];
        }

        for(size_t i = 0; i < _populationSize; ++i) if(rank[i] == 0 && maxNBetter[i] == minMaxNBetter) // sample i not yet ranked and amongst best performing
                rank[i] = r;
    }
     
    // Find reference point
    std::vector<double> reference(_numObjectives, std::numeric_limits<double>::infinity());
    for(size_t i = 0; i < _populationSize; ++i)
        for(size_t k = 0; k < _numObjectives; ++k) if(_currentValues[i][k] < reference[k]) 
            reference[k] = _currentValues[i][k];
     
    // sort samples based on contributing hypervolume
    std::vector<std::vector<double>> exclusiveUpperbound(_populationSize);
    for(size_t i = 0; i <_populationSize; ++i) exclusiveUpperbound[i] = std::vector<double>(_numObjectives, -std::numeric_limits<double>::infinity());

    for(size_t r = 1; r <= _populationSize; ++r)
    {
        // Find rank based hyper volume upper bound
        std::vector<double> upperBound(_numObjectives, -std::numeric_limits<double>::infinity());
        for(size_t i = 0; i < _populationSize; ++i) if(rank[i] == r)
        for(size_t k = 0; k < _numObjectives; ++k) if(_currentValues[i][k] > upperBound[k]) 
            upperBound[k] = _currentValues[i][k];
 
        // Find exclusive upper bound
        for(size_t i = 0; i < _populationSize; ++i) if(rank[i] == r)
        {
            for(size_t j = 0; j < _populationSize; ++j) if(rank[j] == r)
            if(i != j)
            for(size_t k = 0; k < _numObjectives; ++k)
                if(_currentValues[i][k] > exclusiveUpperbound[j][k]) exclusiveUpperbound[j][k] = _currentValues[i][k];
        }

        // Calculate rank based hypervolume
        double hyperVol = 0.0;
        for(size_t k = 0; k < _numObjectives; ++k) hyperVol += (upperBound[k] - reference[k]);

        // Sort samples ascending
        size_t sIdx;
        double previousMinContribution = -std::numeric_limits<double>::infinity();
        for(size_t i = 0; i < _populationSize; ++i) if(rank[i] == r)
        {
            double currentMinContribution = std::numeric_limits<double>::infinity();
            for(size_t j = 0; j < _populationSize; ++j) if(rank[j] == r) // compare only with samples of equal rank
            {
                double contribution = 0.0;
                for(size_t k = 0; k < _numObjectives; ++k)
                    contribution += (exclusiveUpperbound[j][k] - reference[k]);
                if (contribution > previousMinContribution && contribution < currentMinContribution)
                { 
                    currentMinContribution = contribution;  
                    sIdx = j;
                }
            }
            _sortingIndex[i] = sIdx;
            previousMinContribution = currentMinContribution;
        }
    }
}

void MOCMAES::updateDistribution()
{
    // Define mu best values 
    std::vector<bool> success(_populationSize, false);
    for(size_t i = 0; i < _muValue; ++i) success[_sortingIndex[_populationSize-i]] = true;
    
    // TODO : Consider mu best samples (include previous samples)
    
    double evolutionPathFactor = std::sqrt( _evolutionPathAdaptionStrength* (2.0 - _evolutionPathAdaptionStrength));
    for(size_t i = 0; i < _populationSize; ++i)
    {
        // Update Success rate
        _successProbabilities[i] *= (1.0 - _successLearningRate);
        if(success[i] == true) _successProbabilities[i] + _successLearningRate;

        // Update Evolutionpath
        for(size_t d = 0; d < _variableCount; ++d)
            _evolutionPaths[i][d] = (1. - _evolutionPathAdaptionStrength) * _evolutionPaths[i][d];
         
        // Update Covariance
        for(size_t d = 0; d < _variableCount; ++d)
        for(size_t e = 0; e < _variableCount; ++e)
            _covarianceMatrix[i][d*_variableCount + e] = (1.0 - _covarianceLearningRate) * _covarianceMatrix[i][d*_variableCount + e] + _covarianceLearningRate * _evolutionPaths[i][d] * _evolutionPaths[i][e];


        if(_successProbabilities[i] < _targetSuccessRate)
        {
            // Update Evolutionpath
            for(size_t d = 0; d < _variableCount; ++d)
                _evolutionPaths[i][d] += evolutionPathFactor * (_samplePopulation[i][d] - _previousSamplePopulation[i][d]) / _sigma[i];
        }
        else
        {
            // Update Covariance
            for(size_t d = 0; d < _variableCount; ++d)
            for(size_t e = 0; e < _variableCount; ++e)
                _covarianceMatrix[i][d*_variableCount + e] += _covarianceLearningRate * evolutionPathFactor * evolutionPathFactor  * _covarianceMatrix[i][d*_variableCount + e];
        }

        // Update Sigma
        _sigma[i] *= std::exp( 2.0 * _successProbabilities[i]-_targetSuccessRate / ( (2.0 + _variableCount) * (1. - _targetSuccessRate) ) );
    }
}

void MOCMAES::printGenerationBefore() { return; }

void MOCMAES::printGenerationAfter()
{

  double maxminSdev = *std::max_element(_currentMinStandardDeviations.begin(), _currentMinStandardDeviations.end());
  double minmaxSdev = *std::min_element(_currentMaxStandardDeviations.begin(), _currentMaxStandardDeviations.end());
  _k->_logger->logInfo("Normal", "Standard Devs:          Min = %+6.3e - Max = %+6.3e\n", maxminSdev, minmaxSdev);

  /*
  _k->_logger->logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);
  _k->_logger->logInfo("Normal", "Diagonal Covariance:    Min = %+6.3e -  Max = %+6.3e\n", _minimumDiagonalCovarianceMatrixElement, _maximumDiagonalCovarianceMatrixElement);
  _k->_logger->logInfo("Normal", "Covariance Eigenvalues: Min = %+6.3e -  Max = %+6.3e\n", _minimumCovarianceEigenvalue, _maximumCovarianceEigenvalue);

  _k->_logger->logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < _k->_variables.size(); d++) _k->_logger->logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  _k->_logger->logInfo("Detailed", "Constraint Evaluation at Current Function Value:\n");
  if (_areConstraintsDefined)
  {
    if (_bestValidSample >= 0)
      for (size_t c = 0; c < _constraintEvaluations.size(); c++) _k->_logger->logData("Detailed", "         ( %+6.3e )\n", _constraintEvaluations[c][_bestValidSample]);
    else
      for (size_t c = 0; c < _constraintEvaluations.size(); c++) _k->_logger->logData("Detailed", "         ( %+6.3e )\n", _constraintEvaluations[c][0]);
  }

  _k->_logger->logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < _k->_variables.size(); d++)
  {
    for (size_t e = 0; e <= d; e++) _k->_logger->logData("Detailed", "   %+6.3e  ", _covarianceMatrix[d * _k->_variables.size() + e]);
    _k->_logger->logInfo("Detailed", "\n");
  }

  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
  if (_areConstraintsDefined)
  {
    _k->_logger->logInfo("Detailed", "Number of Constraint Evaluations: %zu\n", _constraintEvaluationCount);
    _k->_logger->logInfo("Detailed", "Number of Matrix Corrections: %zu\n", _covarianceMatrixAdaptationCount);
  }*/
}

void MOCMAES::finalize()
{
    // TODO: find all pareto samples
    // copy them and fevals
  
    /*
  // Updating Results
  (*_k)["Results"]["Best Sample"]["F(x)"] = _bestEverValue;
  (*_k)["Results"]["Best Sample"]["Parameters"] = _bestEverVariables;
    

  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _k->_variables.size(); ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
  */
}

} // namespace optimizer

} // namespace solver

} // namespace korali

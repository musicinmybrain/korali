#include "engine.hpp"
#include "modules/problem/optimization/optimization.hpp"
#include "modules/solver/optimizer/MOCMAES/MOCMAES.hpp"
#include "sample/sample.hpp"

#include <gsl/gsl_linalg.h> // Cholesky

@startNamespace

void @className::setInitialConfiguration()
{
  knlohmann::json problemConfig = (*_k)["Problem"];
  _variableCount = _k->_variables.size();

  auto problemPtr = dynamic_cast<korali::problem::Optimization *>(_k->_problem);
  if (problemPtr == nullptr)
    KORALI_LOG_ERROR("Korali problem incompatible with MO-CMAES, must be of type 'Optimization'.\n");

  _numObjectives = problemPtr->_numObjectives;
  if (_numObjectives < 2)
    KORALI_LOG_ERROR("Problem requires multiple objectives, 'Num Objectives' is set to %zu\n.", _numObjectives);

  // Initializing Population Size
  if (_populationSize == 0) _populationSize = std::ceil(4. + floor(3. * log((double)_variableCount)));
  if (_muValue == 0) _muValue = _populationSize / 2.;
  if (_muValue > _populationSize)
    KORALI_LOG_ERROR("Number of parents ('Mu Value' %zu) must be smaller or equal with population size (%zu).\n", _muValue, _populationSize);

  _currentNonDominatedSampleCount = 1;

  // Allocating Sample and Value Memory
  _sampleCollection.resize(0);
  _sampleValueCollection.resize(0);
  _currentSamplePopulation.resize(_populationSize);
  _previousSamplePopulation.resize(_populationSize);
  _currentValues.resize(_populationSize);
  _previousValues.resize(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    _currentSamplePopulation[i].resize(_variableCount);
    _previousSamplePopulation[i].resize(_variableCount);
    _currentValues[i].resize(_numObjectives, -std::numeric_limits<double>::infinity());
    _previousValues[i].resize(_numObjectives, -std::numeric_limits<double>::infinity());
  }

  // Establishing optimization goal
  _bestEverValues.resize(_numObjectives);
  _bestEverVariables.resize(_numObjectives);
  _previousBestValues.resize(_numObjectives);
  _previousBestVariables.resize(_numObjectives);
  _currentBestValues.resize(_numObjectives);
  _currentBestVariables.resize(_numObjectives);
  for (size_t k = 0; k < _numObjectives; ++k)
  {
    _bestEverValues[k] = -std::numeric_limits<double>::infinity();
    _bestEverVariables[k].resize(_variableCount);
    _previousBestValues[k] = -std::numeric_limits<double>::infinity();
    _previousBestVariables[k].resize(_variableCount);
    _currentBestValues[k] = -std::numeric_limits<double>::infinity();
    _currentBestVariables[k].resize(_variableCount);
  }

  // Disable general termination criterion
  _currentBestValue = +Inf;
  _previousBestValue = -Inf;

  // Initializing variable defaults
  double trace = 0.;
  double minSdev = std::numeric_limits<double>::infinity();
  double maxSdev = -std::numeric_limits<double>::infinity();
  for (size_t i = 0; i < _variableCount; i++)
  {
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialValue = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound) * 0.5;
    }

    if (std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound) * 0.3;
    }
    trace += _k->_variables[i]->_initialStandardDeviation * _k->_variables[i]->_initialStandardDeviation;
    if (_k->_variables[i]->_initialStandardDeviation < minSdev) minSdev = _k->_variables[i]->_initialStandardDeviation;
    if (_k->_variables[i]->_initialStandardDeviation > maxSdev) maxSdev = _k->_variables[i]->_initialStandardDeviation;
  }

  // Initializing proposal
  _currentSigma.resize(_populationSize, 0.);
  _previousSigma.resize(_populationSize, 0.);
  _currentSuccessProbabilities.resize(_populationSize, 0.);
  _previousSuccessProbabilities.resize(_populationSize, 0.);
  _currentEvolutionPaths.resize(_populationSize);
  _previousEvolutionPaths.resize(_populationSize);
  _currentCovarianceMatrix.resize(_populationSize);
  _previousCovarianceMatrix.resize(_populationSize);

  // Init covariance and sigma
  for (size_t i = 0; i < _populationSize; ++i)
  {
    _currentEvolutionPaths[i].resize(_variableCount, 0.);
    _previousEvolutionPaths[i].resize(_variableCount, 0.);
    _currentCovarianceMatrix[i].resize(_variableCount * _variableCount, 0.);
    _previousCovarianceMatrix[i].resize(_variableCount * _variableCount, 0.);
  }

  // Init parent
  _parentIndex.resize(_populationSize, 0);
  _parentSuccessProbabilities.resize(_muValue, _targetSuccessRate);
  _parentSigma.resize(_muValue, std::sqrt(trace / _variableCount));
  _parentSamplePopulation.resize(_muValue);
  _parentEvolutionPaths.resize(_muValue);
  _parentCovarianceMatrix.resize(_muValue);
  for (size_t i = 0; i < _muValue; ++i)
  {
    _parentSamplePopulation[i].resize(_variableCount, 0.);
    _parentEvolutionPaths[i].resize(_variableCount, 0.);
    _parentCovarianceMatrix[i].resize(_variableCount * _variableCount, 0.);
    for (size_t d = 0; d < _variableCount; ++d)
      _parentCovarianceMatrix[i][d * _variableCount + d] = _k->_variables[d]->_initialStandardDeviation * _k->_variables[d]->_initialStandardDeviation / (_parentSigma[i] * _parentSigma[i]);
  }

  // MOCMAES variables
  if (_evolutionPathAdaptionStrength < 0.) _evolutionPathAdaptionStrength = 2. / (_variableCount + 2.);
  if (_covarianceLearningRate < 0.) _covarianceLearningRate = 2. / (_variableCount * _variableCount + 6.);
  if ((_successLearningRate <= 0.) || (_successLearningRate > 1.))
    KORALI_LOG_ERROR("Invalid Global Success Learning Rate (%f), must be greater than 0.0 and less or equal to 1.0\n", _successLearningRate);
  if ((_targetSuccessRate <= 0.) || (_targetSuccessRate > 1.))
    KORALI_LOG_ERROR("Invalid Target Success Rate (%f), must be greater than 0.0 and less or equal to 1.0\n", _targetSuccessRate);
  if (_covarianceLearningRate <= 0.)
    KORALI_LOG_ERROR("Invalid Covariance Learning Rate (%f), must be greater than 0.0\n", _covarianceLearningRate);
  if (_evolutionPathAdaptionStrength <= 0. || _evolutionPathAdaptionStrength > 1.)
    KORALI_LOG_ERROR("Invalid Evolution Path Adaption Strength (%f), must be greater 0.0 and less or equal to 1.0\n", _evolutionPathAdaptionStrength);

  // Termination criteria
  _infeasibleSampleCount = 0;
  _currentMinStandardDeviations.resize(_populationSize, minSdev);
  _currentMaxStandardDeviations.resize(_populationSize, maxSdev);
  _currentBestValueDifferences.resize(_numObjectives, std::numeric_limits<double>::infinity());
  _currentBestVariableDifferences.resize(_numObjectives, std::numeric_limits<double>::infinity());
}

void @className::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  prepareGeneration();

  // Initializing Sample Evaluation
  std::vector<Sample> samples(_populationSize);
  for (size_t i = 0; i < _populationSize; i++)
  {
    samples[i]["Module"] = "Problem";
    samples[i]["Operation"] = "Evaluate Multiple";
    samples[i]["Parameters"] = _currentSamplePopulation[i];
    samples[i]["Sample Id"] = i;
    _modelEvaluationCount++;

    KORALI_START(samples[i]);
  }

  // Waiting for samples to finish
  KORALI_WAITALL(samples);

  // Gathering evaluations
  for (size_t i = 0; i < _populationSize; i++)
    _currentValues[i] = KORALI_GET(std::vector<double>, samples[i], "F(x)");

  updateDistribution();
  updateStatistics();
}

void @className::prepareGeneration()
{
  for (size_t i = 0; i < _populationSize; ++i)
  {
    _previousValues[i] = _currentValues[i];
    _previousSamplePopulation[i] = _currentSamplePopulation[i];
    _previousSigma[i] = _currentSigma[i];
    _previousCovarianceMatrix[i] = _currentCovarianceMatrix[i];
    _previousEvolutionPaths[i] = _currentEvolutionPaths[i];
    _previousSuccessProbabilities[i] = _currentSuccessProbabilities[i];
    sampleSingle(i);
  }
}

void @className::sampleSingle(size_t sampleIdx)
{
  size_t parentIdx;
  if (_muValue == _populationSize)
    parentIdx = sampleIdx;
  else
    parentIdx = std::floor(std::min(_muValue, _currentNonDominatedSampleCount) * _uniformGenerator->getRandomNumber());

  // Kepp track of parent
  _parentIndex[sampleIdx] = parentIdx;

  // Set distribution
  _multinormalGenerator->_meanVector = _parentSamplePopulation[parentIdx];
  _multinormalGenerator->_sigma = _parentCovarianceMatrix[parentIdx];
  gsl_matrix_view sig = gsl_matrix_view_array(_multinormalGenerator->_sigma.data(), _variableCount, _variableCount);

  // Calculate Cholesky decomp and scale with sigma
  int err = gsl_linalg_cholesky_decomp1(&sig.matrix);
  err = gsl_matrix_scale(&sig.matrix, _parentSigma[parentIdx]);

  if (err > 0) KORALI_LOG_ERROR("Error during Cholesky decomposition of covariance matrix.\n");
  _multinormalGenerator->updateDistribution();

  // Generate new sample
  bool isFeasible;
  do
  {
    _multinormalGenerator->getRandomVector(_currentSamplePopulation[sampleIdx].data(), _variableCount);
    isFeasible = isSampleFeasible(_currentSamplePopulation[sampleIdx]);
    //if (isFeasible == false) _infeasibleSampleCount++;

  } while (isFeasible == false);

  // Set Covariance
  _currentCovarianceMatrix[sampleIdx] = _parentCovarianceMatrix[parentIdx];

  // Set Sigma
  _currentSigma[sampleIdx] = _parentSigma[parentIdx];

  // Set success probability and evolution path
  _currentEvolutionPaths[sampleIdx] = _parentEvolutionPaths[parentIdx];
  _currentSuccessProbabilities[sampleIdx] = _parentSuccessProbabilities[parentIdx];
}

std::vector<int> @className::sortSampleIndices(const std::vector<std::vector<double>> &values) const
{
  const size_t numValues = values.size();

  // find rank based on non-dominance
  size_t minRank = numValues;
  std::vector<size_t> rank(numValues, 0);
  for (size_t r = numValues; r >= 1; --r) // start with the best ranks
  {
    size_t minMaxNBetter = _numObjectives;
    std::vector<size_t> maxNBetter(numValues, 0);
    for (size_t i = 0; i < numValues; ++i)
      if (rank[i] == 0) // sample i not yet ranked
      {
        for (size_t j = 0; j < numValues; ++j)
          if (i != j && rank[j] == 0) // compare not with yourself and not with already ranked samples
          {
            // Compare with other sample and count how many times it is better
            size_t nBetter = 0;
            for (size_t k = 0; k < _numObjectives; ++k)
              if (values[i][k] < values[j][k]) nBetter++;
            if (nBetter > maxNBetter[i]) maxNBetter[i] = nBetter;
          }

        if (maxNBetter[i] < minMaxNBetter) minMaxNBetter = maxNBetter[i];
      }

    for (size_t i = 0; i < numValues; ++i)
      if (rank[i] == 0 && maxNBetter[i] == minMaxNBetter) // sample i not yet ranked and amongst best performing
      {
        rank[i] = r;
        if (r < minRank) minRank = r;
      }
  }

  // remove offset
  const size_t maxRank = numValues - minRank;
  for (size_t i = 0; i < numValues; ++i)
    rank[i] -= minRank;

  // find reference point
  std::vector<double> reference(_numObjectives, std::numeric_limits<double>::infinity());
  for (size_t i = 0; i < numValues; ++i)
    for (size_t k = 0; k < _numObjectives; ++k)
      if (values[i][k] < reference[k])
        reference[k] = values[i][k];

  std::vector<int> sortedIndeces(numValues, -1);

  // sort samples based on contributing hypervolume
  int order = 0;
  for (size_t r = 0; r <= maxRank; ++r)
  {
    // next samples to sort
    std::vector<size_t> unsorted;
    for (size_t i = 0; i < numValues; ++i)
      if (rank[i] == r && sortedIndeces[i] == -1) unsorted.push_back(i);

    while (unsorted.size() > 0)
    {
      const size_t numUnsorted = unsorted.size();

      // init exclusive upper bounds
      std::vector<std::vector<double>> exclusiveUpperbound(numUnsorted);
      for (size_t i = 0; i < numUnsorted; ++i) exclusiveUpperbound[i] = std::vector<double>(_numObjectives, -std::numeric_limits<double>::infinity());

      // calculate exclusive upper bound
      for (size_t i = 0; i < numUnsorted; ++i)
        for (size_t j = 0; j < numUnsorted; ++j)
          if (i != j)
            for (size_t k = 0; k < _numObjectives; ++k)
              if (values[unsorted[j]][k] > exclusiveUpperbound[i][k]) exclusiveUpperbound[i][k] = values[unsorted[j]][k];

      // calculate (negative) contributing hypervolume
      std::vector<double> contributingHypervolume(numUnsorted, 0.0);
      for (size_t i = 0; i < numUnsorted; ++i)
        for (size_t k = 0; k < _numObjectives; ++k)
          contributingHypervolume[i] += (exclusiveUpperbound[i][k] - reference[k]);

      // sort samples ascending based on rank (primary) and descending based on contributing hypervolume (secondary)
      // descending because we use a different (negative) interpretation of the contributing hypervolume
      size_t nextSample = unsorted[0];
      double currentMaxContribution = contributingHypervolume[0];
      for (size_t i = 1; i < numUnsorted; ++i)
      {
        if (contributingHypervolume[i] > currentMaxContribution)
        {
          currentMaxContribution = contributingHypervolume[i];
          nextSample = unsorted[i];
        }
      }

      sortedIndeces[nextSample] = (int)order;

      unsorted.clear();
      for (size_t i = 0; i < numValues; ++i)
        if (rank[i] == r && sortedIndeces[i] == -1) unsorted.push_back(i);
      order++;
    }
  }

  return sortedIndeces;
}

void @className::updateDistribution()
{
  auto values = _currentValues;
  values.insert(std::end(values), std::begin(_previousValues), std::end(_previousValues));

  // Sort current and previous evlauations
  auto sortedIndices = sortSampleIndices(values);

  // Precompute factors
  const double evolutionPathFactor = std::sqrt(_evolutionPathAdaptionStrength * (2. - _evolutionPathAdaptionStrength));
  const double d = 1.0 + 2.0 * std::max(0.0, std::sqrt((_muValue - 1.) / (_variableCount + 1.)) - 1.0) + _evolutionPathAdaptionStrength;
  const double chiN = std::sqrt(_variableCount) * (1. - 1. / (4. * _variableCount) + 1. / (21. * _variableCount * _variableCount));

  for (size_t i = 0; i < _populationSize; ++i)
  {
    // Update Success rate
    _currentSuccessProbabilities[i] *= (1. - _successLearningRate);
    if ((size_t)sortedIndices[i] >= values.size() - _muValue) _currentSuccessProbabilities[i] += _successLearningRate; // sucess, considered a parent
    _currentSuccessProbabilities[i] = std::min(_currentSuccessProbabilities[i], 1.0);

    // Update Evolutionpath
    for (size_t d = 0; d < _variableCount; ++d)
      _currentEvolutionPaths[i][d] = (1. - _evolutionPathAdaptionStrength) * _currentEvolutionPaths[i][d];
    const auto &parent = _parentSamplePopulation[_parentIndex[i]];
    for (size_t d = 0; d < _variableCount; ++d)
      _currentEvolutionPaths[i][d] += evolutionPathFactor / std::sqrt(_currentCovarianceMatrix[i][d * _variableCount + d]) * (_currentSamplePopulation[i][d] - parent[d]) / _currentSigma[i];
    double evolutionPathLength = 0.;
    for (size_t d = 0; d < _variableCount; ++d) evolutionPathLength += _currentEvolutionPaths[i][d] * _currentEvolutionPaths[i][d];
    evolutionPathLength = std::sqrt(evolutionPathLength);

    // Update Covariance
    for (size_t d = 0; d < _variableCount; ++d)
      for (size_t e = 0; e < _variableCount; ++e)
        _currentCovarianceMatrix[i][d * _variableCount + e] = (1. - _covarianceLearningRate) * _currentCovarianceMatrix[i][d * _variableCount + e] + _covarianceLearningRate * _currentEvolutionPaths[i][d] * _currentEvolutionPaths[i][e];

    if (_currentSuccessProbabilities[i] >= _targetSuccessRate)
    {
      for (size_t d = 0; d < _variableCount; ++d)
        for (size_t e = 0; e < _variableCount; ++e)
          _currentCovarianceMatrix[i][d * _variableCount + e] += _covarianceLearningRate * evolutionPathFactor * evolutionPathFactor * _currentCovarianceMatrix[i][d * _variableCount + e];
    }

    // Update Sigma
    _currentSigma[i] *= std::exp(_evolutionPathAdaptionStrength / d * (evolutionPathLength / chiN - 1.0));
  }

  // Update parents
  size_t pidx = 0;
  for (size_t i = 0; i < sortedIndices.size(); ++i)
    if (sortedIndices[i] >= (int)(values.size() - _muValue))
    {
      if (i < _populationSize)
      {
        _parentSamplePopulation[pidx] = _currentSamplePopulation[i];
        _parentSigma[pidx] = _currentSigma[i];
        _parentCovarianceMatrix[pidx] = _currentCovarianceMatrix[i];
        _parentEvolutionPaths[pidx] = _currentEvolutionPaths[i];
        _parentSuccessProbabilities[pidx] = _currentSuccessProbabilities[i];
      }
      else
      {
        const size_t j = i - _populationSize;
        _parentSamplePopulation[pidx] = _previousSamplePopulation[j];
        _parentSigma[pidx] = _previousSigma[j];
        _parentCovarianceMatrix[pidx] = _previousCovarianceMatrix[j];
        _parentEvolutionPaths[pidx] = _previousEvolutionPaths[j];
        _parentSuccessProbabilities[pidx] = _previousSuccessProbabilities[j];
      }
      pidx++;
    }
}

void @className::updateStatistics()
{
  _previousBestValues = _currentBestValues;
  for (size_t k = 0; k < _numObjectives; ++k)
    _previousBestVariables[k] = _currentBestVariables[k];

  std::fill(_currentBestValues.begin(), _currentBestValues.end(), -std::numeric_limits<double>::infinity());
  std::fill(_currentMinStandardDeviations.begin(), _currentMinStandardDeviations.end(), std::numeric_limits<double>::infinity());
  std::fill(_currentMaxStandardDeviations.begin(), _currentMaxStandardDeviations.end(), -std::numeric_limits<double>::infinity());

  // Keep track of current best objectives and differences to previous generation
  for (size_t i = 0; i < _populationSize; ++i)
  {
    for (size_t k = 0; k < _numObjectives; ++k)
    {
      if (_currentValues[i][k] > _currentBestValues[k])
      {
        _currentBestValues[k] = _currentValues[i][k];
        _currentBestVariables[k] = _currentSamplePopulation[i];
        _currentBestValueDifferences[k] = _currentBestValues[k] - _previousBestValues[k];
        double l2norm2 = 0.;
        for (size_t d = 0; d < _variableCount; ++d)
        {
          l2norm2 += std::pow(_previousBestVariables[k][d] - _currentBestVariables[k][d], 2.);
        }
        _currentBestVariableDifferences[k] = std::sqrt(l2norm2);
      }
    }
  }

  // Keep track of best obtained result for each objective
  for (size_t k = 0; k < _numObjectives; ++k)
  {
    if (_currentBestValues[k] > _bestEverValues[k])
    {
      _bestEverValues[k] = _currentBestValues[k];
      _bestEverVariables[k] = _currentBestVariables[k];
    }
  }

  // Store min / max standard deviations of offspring (sdev approximated)
  for (size_t i = 0; i < _populationSize; ++i)
  {
    for (size_t d = 0; d < _variableCount; ++d)
    {
      double sdev = _currentSigma[d] * std::sqrt(_currentCovarianceMatrix[i][d * _variableCount + d]);
      if (sdev > _currentMaxStandardDeviations[i]) _currentMaxStandardDeviations[i] = sdev;
      if (sdev < _currentMinStandardDeviations[i]) _currentMinStandardDeviations[i] = sdev;
    }
  }

  // Find non dominated samples of current generation
  std::vector<size_t> candidates;

  for (size_t i = 0; i < _populationSize; ++i)
  {
    bool dominated = false;
    for (size_t j = 0; j < _populationSize && dominated == false; ++j)
      if (j != i)
      {
        size_t numdom = 0;
        for (size_t k = 0; k < _numObjectives; ++k)
          if (_currentValues[j][k] > _currentValues[i][k]) numdom++;
        if (numdom == _numObjectives) dominated = true;
      }

    if (dominated == false) candidates.push_back(i);
  }

  _currentNonDominatedSampleCount = candidates.size();

  // Merge new candiates with sample collection
  std::vector<bool> keepCandiate(candidates.size(), true);
  std::vector<bool> keepSample(_sampleCollection.size(), true);

  for (size_t i = 0; i < candidates.size(); ++i)
  {
    for (size_t j = 0; j < _sampleCollection.size(); ++j)
    {
      size_t cdom = 0;
      size_t sdom = 0;

      for (size_t k = 0; k < _numObjectives; ++k)
      {
        if (_currentValues[candidates[i]][k] > _sampleValueCollection[j][k]) cdom++;
        if (_currentValues[candidates[i]][k] < _sampleValueCollection[j][k]) sdom++;
      }
      if (cdom == _numObjectives) keepSample[j] = false;
      if (sdom == _numObjectives) keepCandiate[i] = false;
    }
  }

  auto tmpSampleCollection = _sampleCollection;
  auto tmpSampleValueCollection = _sampleValueCollection;

  _sampleCollection.clear();
  _sampleValueCollection.clear();
  for (size_t i = 0; i < tmpSampleCollection.size(); ++i)
    if (keepSample[i])
    {
      _sampleCollection.push_back(tmpSampleCollection[i]);
      _sampleValueCollection.push_back(tmpSampleValueCollection[i]);
    }

  for (size_t i = 0; i < candidates.size(); ++i)
    if (keepCandiate[i])
    {
      _sampleCollection.push_back(_currentSamplePopulation[candidates[i]]);
      _sampleValueCollection.push_back(_currentValues[candidates[i]]);
    }
}

void @className::printGenerationBefore() { return; }

void @className::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Current Function Values = (Max, Best):\n");
  for (size_t k = 0; k < _numObjectives; ++k) _k->_logger->logData("Normal", "                              = (%+6.3e, %+6.3e)\n", _currentBestValues[k], _bestEverValues[k]);

  double minSdev = *std::min_element(_currentMinStandardDeviations.begin(), _currentMinStandardDeviations.end());
  double maxSdev = *std::max_element(_currentMaxStandardDeviations.begin(), _currentMaxStandardDeviations.end());
  _k->_logger->logInfo("Normal", "Standard Devs:            Min = %+6.3e - Max = %+6.3e\n", minSdev, maxSdev);

  _k->_logger->logInfo("Normal", "Non Dominated Samples:   Current = %zu - Overall = %zu\n", _currentNonDominatedSampleCount, _sampleCollection.size());
  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

void @className::finalize()
{
  // Updating Results
  (*_k)["Results"]["Pareto Optimal Samples"]["F(x)"] = _sampleValueCollection;
  (*_k)["Results"]["Pareto Optimal Samples"]["Parameters"] = _sampleCollection;

  /*
  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);s
  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _k->_variables.size(); ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
  */
}

@moduleAutoCode

@endNamespace

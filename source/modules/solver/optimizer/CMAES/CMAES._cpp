#include "engine.hpp"
#include "modules/solver/optimizer/CMAES/CMAES.hpp"
#include "sample/sample.hpp"

#include <algorithm> // std::sort
#include <chrono>
#include <numeric> // std::iota
#include <stdio.h>
#include <unistd.h>
#include <gsl/gsl_eigen.h>
//#include <gsl/gsl_vector.h>
//#include <gsl/gsl_matrix.h>
//#include <gsl/gsl_blas.h>

@startNamespace

void @className::setInitialConfiguration()
{
  knlohmann::json problemConfig = (*_k)["Problem"];
  _variableCount = _k->_variables.size();

  // Establishing optimization goal
  _bestEverValue = -std::numeric_limits<double>::infinity();

  _previousBestEverValue = _bestEverValue;
  _previousBestValue = _bestEverValue;
  _currentBestValue = _bestEverValue;

  if (_populationSize == 0) _populationSize = ceil(4.0 + floor(3 * log((double)_variableCount)));
  if (_muValue == 0) _muValue = _populationSize / 2;
  if (_viabilityMuValue == 0) _viabilityMuValue = _viabilityPopulationSize / 2;

  size_t s_max = std::max(_populationSize, _viabilityPopulationSize);
  size_t mu_max = std::max(_muValue, _viabilityMuValue);

  _chiSquareNumber = sqrt((double)_variableCount) * (1. - 1. / (4. * _variableCount) + 1. / (21. * _variableCount * _variableCount));
  _chiSquareNumberDiscreteMutations = sqrt((double)_variableCount) * (1. - 1. / (4. * _variableCount) + 1. / (21. * _variableCount * _variableCount));

  _areConstraintsDefined = false;
  if (isDefined(problemConfig, "Constraints"))
  {
    std::vector<size_t> problemConstraints = problemConfig["Constraints"];
    if (problemConstraints.size() > 0) _areConstraintsDefined = true;
    _constraintEvaluations.resize(problemConstraints.size());
  }

  _hasDiscreteVariables = false;
  if (isDefined(problemConfig, "Has Discrete Variables"))
    _hasDiscreteVariables = problemConfig["Has Discrete Variables"];

  _isViabilityRegime = _areConstraintsDefined;

  if (_isViabilityRegime)
  {
    _currentPopulationSize = _viabilityPopulationSize;
    _currentMuValue = _viabilityMuValue;
  }
  else
  {
    _currentPopulationSize = _populationSize;
    _currentMuValue = _muValue;
  }

  // Allocating Memory
  _samplePopulation.resize(s_max);
  for (size_t i = 0; i < s_max; i++) _samplePopulation[i].resize(_variableCount);

  _evolutionPath.resize(_variableCount);
  _conjugateEvolutionPath.resize(_variableCount);
  _auxiliarBDZMatrix.resize(_variableCount);
  _meanUpdate.resize(_variableCount);
  _currentMean.resize(_variableCount);
  _previousMean.resize(_variableCount);
  _bestEverVariables.resize(_variableCount);
  _axisLengths.resize(_variableCount);
  _auxiliarAxisLengths.resize(_variableCount);
  _currentBestVariables.resize(_variableCount);

  _sortingIndex.resize(s_max);
  _valueVector.resize(s_max);

  if (_useGradientInformation)
  {
    _gradients.resize(s_max);
    for (size_t i = 0; i < s_max; ++i) _gradients[i].resize(_variableCount);
    if (_gradientStepSize <= 0.) KORALI_LOG_ERROR("Gradient Step Size must be larger than 0.0 (is %f)", _gradientStepSize);
  }

  _covarianceMatrix.resize(_variableCount * _variableCount);
  _auxiliarCovarianceMatrix.resize(_variableCount * _variableCount);
  _covarianceEigenvectorMatrix.resize(_variableCount * _variableCount);
  _auxiliarCovarianceEigenvectorMatrix.resize(_variableCount * _variableCount);
  _bDZMatrix.resize(s_max * _variableCount);

  _maskingMatrix.resize(_variableCount);
  _maskingMatrixSigma.resize(_variableCount);
  _discreteMutations.resize(_variableCount * _populationSize);
  std::fill(std::begin(_discreteMutations), std::end(_discreteMutations), 0.0);

  _numberMaskingMatrixEntries = 0;
  _numberOfDiscreteMutations = 0;
  _muWeights.resize(mu_max);

  // Initializing variable defaults
  for (size_t i = 0; i < _variableCount; ++i)
  {
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialValue = (_k->_variables[i]->_upperBound + _k->_variables[i]->_lowerBound) * 0.5;
    }

    if (std::isfinite(_k->_variables[i]->_initialStandardDeviation) == false)
    {
      if (std::isfinite(_k->_variables[i]->_lowerBound) == false) KORALI_LOG_ERROR("Initial (Mean) Value of variable \'%s\' not defined, and cannot be inferred because variable lower bound is not finite.\n", _k->_variables[i]->_name.c_str());
      if (std::isfinite(_k->_variables[i]->_upperBound) == false) KORALI_LOG_ERROR("Initial Standard Deviation \'%s\' not defined, and cannot be inferred because variable upper bound is not finite.\n", _k->_variables[i]->_name.c_str());
      _k->_variables[i]->_initialStandardDeviation = (_k->_variables[i]->_upperBound - _k->_variables[i]->_lowerBound) * 0.3;
    }
  }

  // CMAES variables
  if (_areConstraintsDefined)
  {
    if ((_globalSuccessLearningRate <= 0.0) || (_globalSuccessLearningRate > 1.0))
      KORALI_LOG_ERROR("Invalid Global Success Learning Rate (%f), must be greater than 0.0 and less than 1.0\n", _globalSuccessLearningRate);
    if ((_targetSuccessRate <= 0.0) || (_targetSuccessRate > 1.0))
      KORALI_LOG_ERROR("Invalid Target Success Rate (%f), must be greater than 0.0 and less than 1.0\n", _targetSuccessRate);
    if (_covarianceMatrixAdaptionStrength <= 0.0)
      KORALI_LOG_ERROR("Invalid Adaption Size (%f), must be greater than 0.0\n", _covarianceMatrixAdaptionStrength);

    _globalSuccessRate = 0.5;
    _bestValidSample = -1;
    _sampleConstraintViolationCounts.resize(s_max);
    _viabilityBoundaries.resize(_constraintEvaluations.size());

    _viabilityImprovement.resize(s_max);
    _viabilityIndicator.resize(_constraintEvaluations.size());

    for (size_t c = 0; c < _constraintEvaluations.size(); c++) _viabilityIndicator[c].resize(s_max);
    for (size_t c = 0; c < _constraintEvaluations.size(); c++) _constraintEvaluations[c].resize(s_max);

    _normalConstraintApproximation.resize(_constraintEvaluations.size());
    for (size_t i = 0; i < _constraintEvaluations.size(); i++) _normalConstraintApproximation[i].resize(_variableCount);

    _bestConstraintEvaluations.resize(_constraintEvaluations.size());

    _normalVectorLearningRate = 1.0 / (2.0 + _variableCount);
    _covarianceMatrixAdaptionFactor = _covarianceMatrixAdaptionStrength / (_variableCount + 2.);
  }
  else
  {
    _globalSuccessRate = -1.0;
    _covarianceMatrixAdaptionFactor = -1.0;
    _bestValidSample = 0;
  }

  _covarianceMatrixAdaptationCount = 0;
  _maxConstraintViolationCount = 0;

  // Setting algorithm internal variables
  if (_areConstraintsDefined)
    initMuWeights(_viabilityMuValue);
  else
    initMuWeights(_muValue);

  initCovariance();

  _infeasibleSampleCount = 0;
  _resampledParameterCount = 0;

  _conjugateEvolutionPathL2Norm = 0.0;

  for (size_t i = 0; i < _variableCount; i++) _currentMean[i] = _previousMean[i] = _k->_variables[i]->_initialValue;

  _currentMinStandardDeviation = +std::numeric_limits<double>::infinity();
  _currentMaxStandardDeviation = -std::numeric_limits<double>::infinity();
}

void @className::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  if (_areConstraintsDefined) checkMeanAndSetRegime();
  prepareGeneration();
  if (_areConstraintsDefined)
  {
    updateConstraints();
    handleConstraints();
  }

  std::string operation;
  if (_useGradientInformation)
    operation = "Evaluate With Gradients";
  else
    operation = "Evaluate";

  // Initializing Sample Evaluation
  std::vector<Sample> samples(_currentPopulationSize);
  for (size_t i = 0; i < _currentPopulationSize; i++)
  {
    if (_hasDiscreteVariables) discretize(_samplePopulation[i]);

    samples[i]["Module"] = "Problem";
    samples[i]["Operation"] = operation;
    samples[i]["Parameters"] = _samplePopulation[i];
    samples[i]["Sample Id"] = i;
    _modelEvaluationCount++;

    KORALI_START(samples[i]);
  }

  // Waiting for samples to finish
  KORALI_WAITALL(samples);

  // Gathering evaluations
  for (size_t i = 0; i < _currentPopulationSize; i++)
    _valueVector[i] = KORALI_GET(double, samples[i], "F(x)");

  if (_useGradientInformation)
    for (size_t i = 0; i < _currentPopulationSize; i++)
      _gradients[i] = KORALI_GET(std::vector<double>, samples[i], "Gradient");

  updateDistribution();
}

void @className::initMuWeights(size_t numsamplesmu)
{
  // Initializing Mu Weights
  if (_muType == "Linear")
    for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = numsamplesmu - i;
  else if (_muType == "Equal")
    for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = 1.;
  else if (_muType == "Logarithmic")
    for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] = log(std::max((double)numsamplesmu, 0.5 * _currentPopulationSize) + 0.5) - log(i + 1.);
  else
    KORALI_LOG_ERROR("Invalid setting of Mu Type (%s) (Linear, Equal, or Logarithmic accepted).", _muType.c_str());

  // Normalize weights vector and set mueff
  double s1 = 0.0;
  double s2 = 0.0;

  for (size_t i = 0; i < numsamplesmu; i++)
  {
    s1 += _muWeights[i];
    s2 += _muWeights[i] * _muWeights[i];
  }
  _effectiveMu = s1 * s1 / s2;

  for (size_t i = 0; i < numsamplesmu; i++) _muWeights[i] /= s1;

  // Setting Cumulative Covariancea
  if ((_initialCumulativeCovariance <= 0) || (_initialCumulativeCovariance > 1))
    _cumulativeCovariance = (4.0 + _effectiveMu / (1.0 * _variableCount)) / (_variableCount + 4.0 + 2.0 * _effectiveMu / (1.0 * _variableCount));
  else
    _cumulativeCovariance = _initialCumulativeCovariance;

  // Setting Sigma Cumulation Factor
  _sigmaCumulationFactor = _initialSigmaCumulationFactor;
  if (_sigmaCumulationFactor <= 0 || _sigmaCumulationFactor >= 1)
  {
    if (_areConstraintsDefined)
    {
      _sigmaCumulationFactor = sqrt(_effectiveMu) / (sqrt(_effectiveMu) + sqrt(_variableCount));
    }
    else
    {
      _sigmaCumulationFactor = (_effectiveMu + 2.0) / (_variableCount + _effectiveMu + 3.0);
    }
  }

  // Setting Damping Factor
  _dampFactor = _initialDampFactor;
  if (_dampFactor <= 0.0)
    _dampFactor = (1.0 + 2 * std::max(0.0, sqrt((_effectiveMu - 1.0) / (_variableCount + 1.0)) - 1)) + _sigmaCumulationFactor;
}

void @className::initCovariance()
{
  // Setting Sigma
  _trace = 0.0;
  for (size_t i = 0; i < _variableCount; ++i) _trace += _k->_variables[i]->_initialStandardDeviation * _k->_variables[i]->_initialStandardDeviation;
  _sigma = sqrt(_trace / _variableCount);

  // Setting B, C and _axisD
  for (size_t i = 0; i < _variableCount; ++i)
  {
    _covarianceEigenvectorMatrix[i * _variableCount + i] = 1.0;
    _covarianceMatrix[i * _variableCount + i] = _axisLengths[i] = _k->_variables[i]->_initialStandardDeviation * sqrt(_variableCount / _trace);
    _covarianceMatrix[i * _variableCount + i] *= _covarianceMatrix[i * _variableCount + i];
  }

  _minimumCovarianceEigenvalue = *std::min_element(std::begin(_axisLengths), std::end(_axisLengths));
  _maximumCovarianceEigenvalue = *std::max_element(std::begin(_axisLengths), std::end(_axisLengths));

  _minimumCovarianceEigenvalue = _minimumCovarianceEigenvalue * _minimumCovarianceEigenvalue;
  _maximumCovarianceEigenvalue = _maximumCovarianceEigenvalue * _maximumCovarianceEigenvalue;

  _maximumDiagonalCovarianceMatrixElement = _covarianceMatrix[0];
  for (size_t i = 1; i < _variableCount; ++i)
    if (_maximumDiagonalCovarianceMatrixElement < _covarianceMatrix[i * _variableCount + i]) _maximumDiagonalCovarianceMatrixElement = _covarianceMatrix[i * _variableCount + i];
  _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[0];
  for (size_t i = 1; i < _variableCount; ++i)
    if (_minimumDiagonalCovarianceMatrixElement > _covarianceMatrix[i * _variableCount + i]) _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[i * _variableCount + i];
}

void @className::checkMeanAndSetRegime()
{
  if (_isViabilityRegime == false) return; /* mean already inside valid domain, no udpates */

  if (_hasDiscreteVariables) discretize(_currentMean);

  Sample sample;
  sample["Sample Id"] = 0;
  sample["Parameters"] = _currentMean;
  sample["Module"] = "Problem";
  sample["Operation"] = "Evaluate Constraints";

  KORALI_START(sample);
  KORALI_WAIT(sample);
  _constraintEvaluationCount++;

  auto cEvals = KORALI_GET(std::vector<double>, sample, "Constraint Evaluations");

  if (cEvals.size() != _constraintEvaluations.size())
    KORALI_LOG_ERROR("Size of sample's sample evaluations vector (%lu) is different from the number of constraints defined (%lu).\n", cEvals.size(), _constraintEvaluations.size());

  for (size_t c = 0; c < _constraintEvaluations.size(); c++)
    if (cEvals[c] > 0.0) return; /* mean violates constraint, do nothing */

  /* mean inside domain, switch regime and update internal variables */
  _isViabilityRegime = false;

  for (size_t c = 0; c < _constraintEvaluations.size(); c++) { _viabilityBoundaries[c] = 0; }
  _currentPopulationSize = _populationSize;
  _currentMuValue = _muValue;

  initMuWeights(_currentMuValue);
  initCovariance();
}

void @className::updateConstraints()
{
  for (size_t i = 0; i < _currentPopulationSize; i++)
  {
    _sampleConstraintViolationCounts[i] = 0;

    if (_hasDiscreteVariables) discretize(_samplePopulation[i]);

    Sample sample;
    sample["Sample Id"] = 0;
    sample["Parameters"] = _samplePopulation[i];
    sample["Module"] = "Problem";
    sample["Operation"] = "Evaluate Constraints";

    KORALI_START(sample);
    KORALI_WAIT(sample);
    _constraintEvaluationCount++;

    auto cEvals = KORALI_GET(std::vector<double>, sample, "Constraint Evaluations");

    if (cEvals.size() != _constraintEvaluations.size())
      KORALI_LOG_ERROR("Size of sample's sample evaluations vector (%lu) is different from the number of constraints defined (%lu).\n", cEvals.size(), _constraintEvaluations.size());

    for (size_t c = 0; c < _constraintEvaluations.size(); c++)
      _constraintEvaluations[c][i] = cEvals[c];
  }

  _maxConstraintViolationCount = 0;

  for (size_t c = 0; c < _constraintEvaluations.size(); c++)
  {
    double maxviolation = 0.0;
    for (size_t i = 0; i < _currentPopulationSize; ++i)
    {
      if (_constraintEvaluations[c][i] > maxviolation) maxviolation = _constraintEvaluations[c][i];
      if (_k->_currentGeneration == 1 && _isViabilityRegime) _viabilityBoundaries[c] = maxviolation;

      if (_constraintEvaluations[c][i] > _viabilityBoundaries[c] + 1e-12) _sampleConstraintViolationCounts[i]++;
      if (_sampleConstraintViolationCounts[i] > _maxConstraintViolationCount) _maxConstraintViolationCount = _sampleConstraintViolationCounts[i];
    }
  }
}

void @className::reEvaluateConstraints()
{
  _maxConstraintViolationCount = 0;

  for (size_t i = 0; i < _currentPopulationSize; ++i)
    if (_sampleConstraintViolationCounts[i] > 0)
    {
      if (_hasDiscreteVariables) discretize(_samplePopulation[i]);

      Sample sample;
      sample["Sample Id"] = 0;
      sample["Parameters"] = _samplePopulation[i];
      sample["Module"] = "Problem";
      sample["Operation"] = "Evaluate Constraints";

      KORALI_START(sample);
      KORALI_WAIT(sample);
      _constraintEvaluationCount++;

      _sampleConstraintViolationCounts[i] = 0;

      auto cEvals = KORALI_GET(std::vector<double>, sample, "Constraint Evaluations");

      if (cEvals.size() != _constraintEvaluations.size())
        KORALI_LOG_ERROR("Size of sample's sample evaluations vector (%lu) is different from the number of constraints defined (%lu).\n", cEvals.size(), _constraintEvaluations.size());

      for (size_t c = 0; c < _constraintEvaluations.size(); c++)
      {
        _constraintEvaluations[c][i] = cEvals[c];

        if (_constraintEvaluations[c][i] > _viabilityBoundaries[c] + 1e-12)
        {
          _viabilityIndicator[c][i] = true;
          _sampleConstraintViolationCounts[i]++;
        }
        else
          _viabilityIndicator[c][i] = false;
      }
      if (_sampleConstraintViolationCounts[i] > _maxConstraintViolationCount) _maxConstraintViolationCount = _sampleConstraintViolationCounts[i];
    }
}

void @className::updateViabilityBoundaries()
{
  for (size_t c = 0; c < _constraintEvaluations.size(); c++)
  {
    double maxviolation = 0.0;
    for (size_t i = 0; i < _currentMuValue /* _currentPopulationSize alternative */; ++i)
      if (_constraintEvaluations[c][_sortingIndex[i]] > maxviolation)
        maxviolation = _constraintEvaluations[c][_sortingIndex[i]];

    _viabilityBoundaries[c] = std::max(0.0, std::min(_viabilityBoundaries[c], 0.5 * (maxviolation + _viabilityBoundaries[c])));
  }
}

void @className::prepareGeneration()
{
  updateEigensystem(_covarianceMatrix);

  for (size_t i = 0; i < _currentPopulationSize; ++i)
  {
    bool isFeasible;
    do
    {
      sampleSingle(i);

      if (_hasDiscreteVariables) discretize(_samplePopulation[i]);

      isFeasible = isSampleFeasible(_samplePopulation[i]);

      if (isFeasible == false) _infeasibleSampleCount++;

    } while (isFeasible == false && (_infeasibleSampleCount < _maxInfeasibleResamplings));
  }
}

void @className::sampleSingle(size_t sampleIdx)
{
  for (size_t d = 0; d < _variableCount; ++d)
  {
    double randomNumber = _normalGenerator->getRandomNumber();

    if (_isDiagonal)
    {
      _bDZMatrix[sampleIdx * _variableCount + d] = _axisLengths[d] * randomNumber;
      _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma * _bDZMatrix[sampleIdx * _variableCount + d];
    }
    else
      _auxiliarBDZMatrix[d] = _axisLengths[d] * randomNumber;
  }

  if (!_isDiagonal)
    for (size_t d = 0; d < _variableCount; ++d)
    {
      _bDZMatrix[sampleIdx * _variableCount + d] = 0.0;
      for (size_t e = 0; e < _variableCount; ++e) _bDZMatrix[sampleIdx * _variableCount + d] += _covarianceEigenvectorMatrix[d * _variableCount + e] * _auxiliarBDZMatrix[e];
      _samplePopulation[sampleIdx][d] = _currentMean[d] + _sigma * _bDZMatrix[sampleIdx * _variableCount + d];
    }

  if (_hasDiscreteVariables)
  {
    if ((sampleIdx + 1) < _numberOfDiscreteMutations)
    {
      double p_geom = std::pow(0.7, 1.0 / _numberMaskingMatrixEntries);
      size_t select = std::floor(_uniformGenerator->getRandomNumber() * _numberMaskingMatrixEntries);

      for (size_t d = 0; d < _variableCount; ++d)
        if ((_maskingMatrix[d] == 1.0) && (select-- == 0))
        {
          double dmutation = 1.0;
          while (_uniformGenerator->getRandomNumber() > p_geom) dmutation += 1.0;
          dmutation *= _k->_variables[d]->_granularity;

          if (_uniformGenerator->getRandomNumber() > 0.5) dmutation *= -1.0;
          _discreteMutations[sampleIdx * _variableCount + d] = dmutation;
          _samplePopulation[sampleIdx][d] += dmutation;
        }
    }
    else if ((sampleIdx + 1) == _numberOfDiscreteMutations)
    {
      for (size_t d = 0; d < _variableCount; ++d)
        if (_k->_variables[d]->_granularity != 0.0)
        {
          double dmutation = std::round(_bestEverVariables[d] / _k->_variables[d]->_granularity) * _k->_variables[d]->_granularity - _samplePopulation[sampleIdx][d];
          _discreteMutations[sampleIdx * _variableCount + d] = dmutation;
          _samplePopulation[sampleIdx][d] += dmutation;
        }
    }
  }
}

void @className::updateDistribution()
{
  /* Generate _sortingIndex */
  sort_index(_valueVector, _sortingIndex, _currentPopulationSize);

  if ((_areConstraintsDefined == false) || _isViabilityRegime)
    _bestValidSample = _sortingIndex[0];
  else
  {
    _bestValidSample = -1;
    for (size_t i = 0; i < _currentPopulationSize; i++)
      if (_sampleConstraintViolationCounts[_sortingIndex[i]] == 0) _bestValidSample = _sortingIndex[i];
    if (_bestValidSample == -1)
    {
      _k->_logger->logWarning("Detailed", "All samples violate constraints, no updates taking place.\n");
      return;
    }
  }

  /* update function value history */
  _previousBestValue = _currentBestValue;

  /* update current best */
  _currentBestValue = _valueVector[_bestValidSample];

  for (size_t d = 0; d < _variableCount; ++d) _currentBestVariables[d] = _samplePopulation[_bestValidSample][d];

  /* update xbestever */
  if (_currentBestValue > _bestEverValue || _k->_currentGeneration == 1)
  {
    _previousBestEverValue = _bestEverValue;
    _bestEverValue = _currentBestValue;

    for (size_t d = 0; d < _variableCount; ++d)
      _bestEverVariables[d] = _currentBestVariables[d];

    if (_areConstraintsDefined)
      for (size_t c = 0; c < _constraintEvaluations.size(); c++)
        _bestConstraintEvaluations[c] = _constraintEvaluations[c][_bestValidSample];
  }

  /* update mean */
  for (size_t d = 0; d < _variableCount; ++d)
  {
    _previousMean[d] = _currentMean[d];
    _currentMean[d] = 0.;
    for (size_t i = 0; i < _currentMuValue; ++i)
      _currentMean[d] += _muWeights[i] * _samplePopulation[_sortingIndex[i]][d];
  }

  if (_useGradientInformation)
  {
    double l2update = 0.;
    for (size_t d = 0; d < _variableCount; ++d)
      l2update += (_currentMean[d] - _previousMean[d]) * (_currentMean[d] - _previousMean[d]);
    l2update = std::sqrt(l2update / _variableCount);

    for (size_t d = 0; d < _variableCount; ++d)
      for (size_t i = 0; i < _currentMuValue; ++i)
        _currentMean[d] += _muWeights[i] * _gradientStepSize / std::sqrt(_variableCount) * _gradients[_sortingIndex[i]][d];
  }

  for (size_t d = 0; d < _variableCount; ++d)
    _meanUpdate[d] = (_currentMean[d] - _previousMean[d]) / _sigma;

  /* calculate z := D^(-1) * B^(T) * _meanUpdate into _auxiliarBDZMatrix */
  for (size_t d = 0; d < _variableCount; ++d)
  {
    double sum = 0.0;
    if (_isDiagonal)
      sum = _meanUpdate[d];
    else
      for (size_t e = 0; e < _variableCount; ++e) sum += _covarianceEigenvectorMatrix[e * _variableCount + d] * _meanUpdate[e]; /* B^(T) * _meanUpdate ( iterating B[e][d] = B^(T) ) */

    _auxiliarBDZMatrix[d] = sum / _axisLengths[d]; /* D^(-1) * B^(T) * _meanUpdate */
  }

  _conjugateEvolutionPathL2Norm = 0.0;

  /* cumulation for _sigma (ps) using B*z */
  for (size_t d = 0; d < _variableCount; ++d)
  {
    double sum = 0.0;
    if (_isDiagonal)
      sum = _auxiliarBDZMatrix[d];
    else
      for (size_t e = 0; e < _variableCount; ++e) sum += _covarianceEigenvectorMatrix[d * _variableCount + e] * _auxiliarBDZMatrix[e];

    _conjugateEvolutionPath[d] = (1. - _sigmaCumulationFactor) * _conjugateEvolutionPath[d] + sqrt(_sigmaCumulationFactor * (2. - _sigmaCumulationFactor) * _effectiveMu) * sum;

    /* calculate norm(ps)^2 */
    _conjugateEvolutionPathL2Norm += std::pow(_conjugateEvolutionPath[d], 2.0);
  }

  /* calculate norm(ps) */
  _conjugateEvolutionPathL2Norm = std::sqrt(_conjugateEvolutionPathL2Norm);

  int hsig = (1.4 + 2.0 / (_variableCount + 1) > _conjugateEvolutionPathL2Norm / sqrt(1. - pow(1. - _sigmaCumulationFactor, 2.0 * (1.0 + _k->_currentGeneration))) / _chiSquareNumber);

  /* cumulation for covariance matrix (pc) using B*D*z~_variableCount(0,C) */
  for (size_t d = 0; d < _variableCount; ++d)
    _evolutionPath[d] = (1. - _cumulativeCovariance) * _evolutionPath[d] + hsig * sqrt(_cumulativeCovariance * (2. - _cumulativeCovariance) * _effectiveMu) * _meanUpdate[d];

  /* update covariance matrix  */
  adaptC(hsig);

  /* update masking matrix */
  if (_hasDiscreteVariables) updateDiscreteMutationMatrix();

  /* update viability bounds */
  if (_areConstraintsDefined && (_isViabilityRegime == true)) updateViabilityBoundaries();

  /* update sigma */
  updateSigma();

  /* numerical error management */
  numericalErrorTreatment();

  _currentMinStandardDeviation = std::numeric_limits<double>::infinity();
  _currentMaxStandardDeviation = -std::numeric_limits<double>::infinity();

  // Calculating current Minimum and Maximum STD Devs
  for (size_t i = 0; i < _variableCount; ++i)
  {
    _currentMinStandardDeviation = std::min(_currentMinStandardDeviation, _sigma * sqrt(_covarianceMatrix[i * _variableCount + i]));
    _currentMaxStandardDeviation = std::max(_currentMaxStandardDeviation, _sigma * sqrt(_covarianceMatrix[i * _variableCount + i]));
  }
}

void @className::adaptC(int hsig)
{
  /* definitions for speeding up inner-most loop */
  //double ccov1  = std::min(_covarianceMatrixLearningRate * (1./_muCovariance) * (_isDiagonal ? (_variableCount+1.5) / 3. : 1.), 1.); (orig, alternative)
  //double ccovmu = std::min(_covarianceMatrixLearningRate * (1-1./_muCovariance) * (_isDiagonal ? (_variableCount+1.5) / 3. : 1.), 1.-ccov1); (orig, alternative)
  double ccov1 = 2.0 / (std::pow(_variableCount + 1.3, 2) + _effectiveMu);
  double ccovmu = std::min(1.0 - ccov1, 2.0 * (_effectiveMu - 2 + 1 / _effectiveMu) / (std::pow(_variableCount + 2.0, 2) + _effectiveMu));
  double sigmasquare = _sigma * _sigma;

  /* update covariance matrix */
  for (size_t d = 0; d < _variableCount; ++d)
    for (size_t e = _isDiagonal ? d : 0; e <= d; ++e)
    {
      _covarianceMatrix[d * _variableCount + e] = (1 - ccov1 - ccovmu) * _covarianceMatrix[d * _variableCount + e] + ccov1 * (_evolutionPath[d] * _evolutionPath[e] + (1 - hsig) * _cumulativeCovariance * (2. - _cumulativeCovariance) * _covarianceMatrix[d * _variableCount + e]);

      for (size_t k = 0; k < _currentMuValue; ++k)
        _covarianceMatrix[d * _variableCount + e] += ccovmu * _muWeights[k] * (_samplePopulation[_sortingIndex[k]][d] - _previousMean[d]) * (_samplePopulation[_sortingIndex[k]][e] - _previousMean[e]) / sigmasquare;

      if (e < d) _covarianceMatrix[e * _variableCount + d] = _covarianceMatrix[d * _variableCount + e];
    }

  /* update maximal and minimal diagonal value */
  _maximumDiagonalCovarianceMatrixElement = _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[0];
  for (size_t d = 1; d < _variableCount; ++d)
  {
    if (_maximumDiagonalCovarianceMatrixElement < _covarianceMatrix[d * _variableCount + d])
      _maximumDiagonalCovarianceMatrixElement = _covarianceMatrix[d * _variableCount + d];
    else if (_minimumDiagonalCovarianceMatrixElement > _covarianceMatrix[d * _variableCount + d])
      _minimumDiagonalCovarianceMatrixElement = _covarianceMatrix[d * _variableCount + d];
  }
}

void @className::updateSigma()
{
  /* update for non-viable region */
  if (_areConstraintsDefined && (_isViabilityRegime == true))
  {
    _globalSuccessRate = (1 - _globalSuccessLearningRate) * _globalSuccessRate;

    _sigma *= exp((_globalSuccessRate - (_targetSuccessRate / (1.0 - _targetSuccessRate)) * (1 - _globalSuccessRate)) / _dampFactor);
  }
  /* update for discrte variables */
  else if (_hasDiscreteVariables)
  {
    double pathL2 = 0.0;
    for (size_t d = 0; d < _variableCount; ++d) pathL2 += _maskingMatrixSigma[d] * _conjugateEvolutionPath[d] * _conjugateEvolutionPath[d];
    _sigma *= exp(_sigmaCumulationFactor / _dampFactor * (sqrt(pathL2) / _chiSquareNumberDiscreteMutations - 1.));
  }
  /* standard update */
  else
  {
    // _sigma *= exp(min(1.0, _sigmaCumulationFactor/_dampFactor*((_conjugateEvolutionPathL2Norm/_chiSquareNumber)-1.))); (alternative)
    _sigma *= exp(_sigmaCumulationFactor / _dampFactor * (_conjugateEvolutionPathL2Norm / _chiSquareNumber - 1.));
  }

  /* escape flat evaluation */
  if (_currentBestValue == _valueVector[_sortingIndex[(int)_currentMuValue]])
  {
    _sigma *= exp(0.2 + _sigmaCumulationFactor / _dampFactor);
    _k->_logger->logWarning("Detailed", "Sigma increased due to equal function values.\n");
  }

  /* upper bound check for _sigma */
  double _upperBound = sqrt(_trace / _variableCount);

  if (_sigma > _upperBound)
  {
    _k->_logger->logInfo("Detailed", "Sigma exceeding inital value of _sigma (%f > %f), increase Initial Standard Deviation of variables.\n", _sigma, _upperBound);
    if (_isSigmaBounded)
    {
      _sigma = _upperBound;
      _k->_logger->logInfo("Detailed", "Sigma set to upper bound (%f) due to solver configuration 'Is Sigma Bounded' = 'true'.\n", _sigma);
    }
  }
}

void @className::numericalErrorTreatment()
{
  //treat maximal standard deviations
  //TODO

  //treat minimal standard deviations
  for (size_t d = 0; d < _variableCount; ++d)
    if (_sigma * sqrt(_covarianceMatrix[d * _variableCount + d]) < _k->_variables[d]->_minimumStandardDeviationUpdate)
    {
      _sigma = (_k->_variables[d]->_minimumStandardDeviationUpdate) / sqrt(_covarianceMatrix[d * _variableCount + d]) * exp(0.05 + _sigmaCumulationFactor / _dampFactor);
      _k->_logger->logWarning("Detailed", "Sigma increased due to minimal standard deviation.\n");
    }

  //too low coordinate axis deviations
  //TODO

  //treat numerical precision provblems
  //TODO
}

void @className::handleConstraints()
{
  size_t initial_resampled = _resampledParameterCount;
  size_t initial_corrections = _covarianceMatrixAdaptationCount;

  while (_maxConstraintViolationCount > 0)
  {
    _auxiliarCovarianceMatrix = _covarianceMatrix;

    for (size_t i = 0; i < _currentPopulationSize; ++i)
      if (_sampleConstraintViolationCounts[i] > 0)
      {
        //update constraint normal
        for (size_t c = 0; c < _constraintEvaluations.size(); c++)
          if (_viabilityIndicator[c][i] == true)
          {
            _covarianceMatrixAdaptationCount++;

            double v2 = 0;
            for (size_t d = 0; d < _variableCount; ++d)
            {
              _normalConstraintApproximation[c][d] = (1.0 - _normalVectorLearningRate) * _normalConstraintApproximation[c][d] + _normalVectorLearningRate * _bDZMatrix[i * _variableCount + d];
              v2 += _normalConstraintApproximation[c][d] * _normalConstraintApproximation[c][d];
            }
            for (size_t d = 0; d < _variableCount; ++d)
              for (size_t e = 0; e < _variableCount; ++e)
                _auxiliarCovarianceMatrix[d * _variableCount + e] = _auxiliarCovarianceMatrix[d * _variableCount + e] - ((_covarianceMatrixAdaptionFactor * _covarianceMatrixAdaptionFactor * _normalConstraintApproximation[c][d] * _normalConstraintApproximation[c][e]) / (v2 * _sampleConstraintViolationCounts[i] * _sampleConstraintViolationCounts[i]));
          }
      }

    updateEigensystem(_auxiliarCovarianceMatrix);

    /* in original some stopping criterion (TOLX) */
    // TODO

    //resample invalid points
    for (size_t i = 0; i < _currentPopulationSize; ++i)
      if (_sampleConstraintViolationCounts[i] > 0)
      {
        Sample sample;
        bool isFeasible;

        do
        {
          _resampledParameterCount++;
          sampleSingle(i);

          if (_resampledParameterCount - initial_resampled > _maxInfeasibleResamplings)
          {
            _k->_logger->logWarning("Detailed", "Exiting resampling loop, max resamplings (%zu) reached.\n", _maxInfeasibleResamplings);
            reEvaluateConstraints();
            return;
          }

          if (_hasDiscreteVariables) discretize(_samplePopulation[i]);
          isFeasible = isSampleFeasible(_samplePopulation[i]);

        } while (isFeasible == false);
      }

    reEvaluateConstraints();

    if (_covarianceMatrixAdaptationCount - initial_corrections > _maxCovarianceMatrixCorrections)
    {
      _k->_logger->logWarning("Detailed", "Exiting adaption loop, max adaptions (%zu) reached.\n", _maxCovarianceMatrixCorrections);
      return;
    }

  } //while _maxConstraintViolationCount > 0
}

void @className::updateDiscreteMutationMatrix()
{
  // implemented based on 'A CMA-ES for Mixed-Integer Nonlinear Optimization' by
  // Hansen2011

  size_t entries = _variableCount + 1; // +1 to prevent 0-ness
  std::fill(std::begin(_maskingMatrixSigma), std::end(_maskingMatrixSigma), 1.0);
  for (size_t d = 0; d < _variableCount; ++d)
    if (_sigma * std::sqrt(_covarianceMatrix[d * _variableCount + d]) / std::sqrt(_sigmaCumulationFactor) < 0.2 * _k->_variables[d]->_granularity)
    {
      _maskingMatrixSigma[d] = 0.0;
      entries--;
    }
  _chiSquareNumberDiscreteMutations = sqrt((double)entries) * (1. - 1. / (4. * entries) + 1. / (21. * entries * entries));

  _numberMaskingMatrixEntries = 0;
  std::fill(std::begin(_maskingMatrix), std::end(_maskingMatrix), 0.0);
  for (size_t d = 0; d < _variableCount; ++d)
    if (2.0 * _sigma * std::sqrt(_covarianceMatrix[d * _variableCount + d]) < _k->_variables[d]->_granularity)
    {
      _maskingMatrix[d] = 1.0;
      _numberMaskingMatrixEntries++;
    }

  _numberOfDiscreteMutations = std::min(std::round(_populationSize / 10.0 + _numberMaskingMatrixEntries + 1), std::floor(_populationSize / 2.0) - 1);
  std::fill(std::begin(_discreteMutations), std::end(_discreteMutations), 0.0);
}

void @className::discretize(std::vector<double> &sample)
{
  if (sample.size() != _variableCount)
    KORALI_LOG_ERROR("Size of sample's parameter vector (%lu) is different from the number of problem variables.\n", sample.size(), _variableCount);

  for (size_t d = 0; d < _variableCount; ++d)
    if (_k->_variables[d]->_granularity != 0.0)
      sample[d] = std::round(sample[d] / _k->_variables[d]->_granularity) * _k->_variables[d]->_granularity;
}

void @className::updateEigensystem(const std::vector<double> &M)
{
  eigen(_variableCount, M, _auxiliarAxisLengths, _auxiliarCovarianceEigenvectorMatrix);

  /* find largest and smallest eigenvalue, they are supposed to be sorted anyway */
  double minCovEV = *std::min_element(std::begin(_auxiliarAxisLengths), std::end(_auxiliarAxisLengths));
  double maxCovEV = *std::max_element(std::begin(_auxiliarAxisLengths), std::end(_auxiliarAxisLengths));
  if (minCovEV <= 0.0)
  {
    _k->_logger->logWarning("Detailed", "Min Eigenvalue smaller or equal 0.0 (%+6.3e) after Eigen decomp (no update possible).\n", _minimumCovarianceEigenvalue);
    return;
  }

  for (size_t d = 0; d < _variableCount; ++d)
  {
    _auxiliarAxisLengths[d] = sqrt(_auxiliarAxisLengths[d]);
    if (std::isfinite(_auxiliarAxisLengths[d]) == false)
    {
      _k->_logger->logWarning("Detailed", "Could not calculate root of Eigenvalue (%+6.3e) after Eigen decomp (no update possible).\n", _auxiliarAxisLengths[d]);
      return;
    }
    if (!_isDiagonal)
      for (size_t e = 0; e < _variableCount; ++e)
        if (std::isfinite(_auxiliarCovarianceEigenvectorMatrix[d * _variableCount + e]) == false)
        {
          _k->_logger->logWarning("Detailed", "Non finite value detected in B (no update possible).\n");
          return;
        }
  }

  _minimumCovarianceEigenvalue = minCovEV;
  _maximumCovarianceEigenvalue = maxCovEV;

  /* write back */
  for (size_t d = 0; d < _variableCount; ++d) _axisLengths[d] = _auxiliarAxisLengths[d];
  _covarianceEigenvectorMatrix.assign(std::begin(_auxiliarCovarianceEigenvectorMatrix), std::end(_auxiliarCovarianceEigenvectorMatrix));
}

/************************************************************************/
/*                    Additional Methods                                */
/************************************************************************/

void @className::eigen(size_t size, const std::vector<double> &M, std::vector<double> &diag, std::vector<double> &Q) const
{
  if (_isDiagonal)
  {
    std::fill(Q.begin(), Q.end(), 0);
    for (size_t i = 0; i < size; ++i) Q[i * size + i] = 1.;
    for (size_t i = 0; i < size; ++i) diag[i] = M[i * size + i];
  }
  else
  {
    std::vector<double> data(size * size);

    for (size_t i = 0; i < size; i++)
      for (size_t j = 0; j <= i; j++)
      {
        data[i * size + j] = M[i * size + j];
        data[j * size + i] = M[i * size + j];
      }

    // GSL Workspace
    
    gsl_vector *gsl_eval = gsl_vector_alloc(size);
    gsl_matrix *gsl_evec = gsl_matrix_alloc(size, size);
    gsl_eigen_symmv_workspace *gsl_work = gsl_eigen_symmv_alloc(size);

    gsl_matrix_view m = gsl_matrix_view_array(data.data(), size, size);

    gsl_eigen_symmv(&m.matrix, gsl_eval, gsl_evec, gsl_work);
    gsl_eigen_symmv_sort(gsl_eval, gsl_evec, GSL_EIGEN_SORT_ABS_ASC);

    for (size_t i = 0; i < size; i++)
    {
      gsl_vector_view gsl_evec_i = gsl_matrix_column(gsl_evec, i);
      for (size_t j = 0; j < size; j++) Q[j * size + i] = gsl_vector_get(&gsl_evec_i.vector, j);
    }

    for (size_t i = 0; i < size; i++) diag[i] = gsl_vector_get(gsl_eval, i);

    gsl_vector_free(gsl_eval);
    gsl_matrix_free(gsl_evec);
    gsl_eigen_symmv_free(gsl_work);
  }
}

void @className::sort_index(const std::vector<double> &vec, std::vector<size_t> &sortingIndex, size_t N) const
{
  // initialize original sortingIndex locations
  std::iota(std::begin(sortingIndex), std::begin(sortingIndex) + N, (size_t)0);

  // sort indexes based on comparing values in vec
  std::sort(std::begin(sortingIndex), std::begin(sortingIndex) + N, [vec](size_t i1, size_t i2) { return vec[i1] > vec[i2]; });
}

void @className::printGenerationBefore() { return; }

void @className::printGenerationAfter()
{
  if (_areConstraintsDefined && _isViabilityRegime)
  {
    _k->_logger->logInfo("Normal", "Searching start (MeanX violates constraints) .. \n");
    _k->_logger->logInfo("Normal", "Viability Bounds:\n");
    for (size_t c = 0; c < _constraintEvaluations.size(); c++) _k->_logger->logData("Normal", "         (%+6.3e)\n", _viabilityBoundaries[c]);
    _k->_logger->logInfo("Normal", "\n");
  }

  _k->_logger->logInfo("Normal", "Sigma:                        %+6.3e\n", _sigma);
  _k->_logger->logInfo("Normal", "Current Function Value: Max = %+6.3e - Best = %+6.3e\n", _currentBestValue, _bestEverValue);
  _k->_logger->logInfo("Normal", "Diagonal Covariance:    Min = %+6.3e -  Max = %+6.3e\n", _minimumDiagonalCovarianceMatrixElement, _maximumDiagonalCovarianceMatrixElement);
  _k->_logger->logInfo("Normal", "Covariance Eigenvalues: Min = %+6.3e -  Max = %+6.3e\n", _minimumCovarianceEigenvalue, _maximumCovarianceEigenvalue);

  _k->_logger->logInfo("Detailed", "Variable = (MeanX, BestX):\n");
  for (size_t d = 0; d < _variableCount; d++) _k->_logger->logData("Detailed", "         %s = (%+6.3e, %+6.3e)\n", _k->_variables[d]->_name.c_str(), _currentMean[d], _bestEverVariables[d]);

  if (_areConstraintsDefined)
  {
    _k->_logger->logInfo("Detailed", "Constraint Evaluation at Current Function Value:\n");
    if (_bestValidSample >= 0)
      for (size_t c = 0; c < _constraintEvaluations.size(); c++) _k->_logger->logData("Detailed", "         ( %+6.3e )\n", _constraintEvaluations[c][_bestValidSample]);
    else
      for (size_t c = 0; c < _constraintEvaluations.size(); c++) _k->_logger->logData("Detailed", "         ( %+6.3e )\n", _constraintEvaluations[c][0]);
  }

  _k->_logger->logInfo("Detailed", "Covariance Matrix:\n");
  for (size_t d = 0; d < _variableCount; d++)
  {
    for (size_t e = 0; e <= d; e++) _k->_logger->logData("Detailed", "   %+6.3e  ", _covarianceMatrix[d * _variableCount + e]);
    _k->_logger->logInfo("Detailed", "\n");
  }

  _k->_logger->logInfo("Detailed", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
  if (_areConstraintsDefined)
  {
    _k->_logger->logInfo("Detailed", "Number of Constraint Evaluations: %zu\n", _constraintEvaluationCount);
    _k->_logger->logInfo("Detailed", "Number of Matrix Corrections: %zu\n", _covarianceMatrixAdaptationCount);
  }
}

void @className::finalize()
{
  // Updating Results
  (*_k)["Results"]["Best Sample"]["F(x)"] = _bestEverValue;
  (*_k)["Results"]["Best Sample"]["Parameters"] = _bestEverVariables;

  _k->_logger->logInfo("Minimal", "Optimum found at:\n");
  for (size_t d = 0; d < _variableCount; ++d) _k->_logger->logData("Minimal", "         %s = %+6.3e\n", _k->_variables[d]->_name.c_str(), _bestEverVariables[d]);
  _k->_logger->logInfo("Minimal", "Optimum found: %e\n", _bestEverValue);
  if (_areConstraintsDefined)
  {
    _k->_logger->logInfo("Minimal", "Constraint Evaluation at Optimum:\n");
    for (size_t c = 0; c < _constraintEvaluations.size(); c++)
      _k->_logger->logData("Minimal", "         ( %+6.3e )\n", _bestConstraintEvaluations[c]);
  }
  _k->_logger->logInfo("Minimal", "Number of Infeasible Samples: %zu\n", _infeasibleSampleCount);
}

@moduleAutoCode

@endNamespace

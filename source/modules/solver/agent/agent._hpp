#ifndef _KORALI_AGENT_HPP_
#define _KORALI_AGENT_HPP_

#include "auxiliar/cbuffer.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/problem/supervisedLearning/supervisedLearning.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <algorithm> // std::shuffle
#include <random>

namespace korali
{
namespace solver
{
/**
* @brief This enumerator details all possible termination statuses for a given episode's experience
*/
enum termination_t
{
  /**
  * @brief The experience is non-terminal
  */
  e_nonTerminal = 0,

  /**
   * @brief This is the terminal experience in a normally executed episode
   */
  e_terminal = 1,

  /**
   * @brief This is the terminal experience in a truncated episode
   *        (i.e., should have continued, but it was artificially truncated to limit running time)
   */
  e_truncated = 2
};

/**
* @brief Structure to store all relevant information about an agent's experience within a given episode
*/
struct experience_t
{
  /**
  * @brief Stores the state of the experience
  */
  std::vector<float> state;

  /**
   * @brief Stores the actiont aken by the agent at the given state
   */
  std::vector<float> action;

  /**
   * @brief Contains the reward of the state/action pair
   */
  float reward;

  /**
   * @brief Specifies whether the experience is terminal (truncated or normal) or not.
   */
  termination_t termination;

  /**
   * @brief If this is a truncated terminal experience, the truncated state is also saved here
   */
  std::vector<float> truncatedState;

  /**
   * @brief Contains any additional necessary information about the experience
   */
  knlohmann::json metadata;
};

class Agent : public Solver
{
  public:
  /**
  * @brief Array of agents collecting new experiences
  */
  std::vector<Sample> _agents;

  /**
   * @brief Keeps track of the age
   */
  std::vector<bool> _isAgentRunning;

  /**
   * @brief Storage for the entire episodes before adding it to the replay memory
   */
  std::vector<std::vector<knlohmann::json>> _episodes;

  /**
   * @brief Session-specific experience count. This is useful in case of restart: counters from the old session won't count
   */
  size_t _sessionExperienceCount;

  /**
   * @brief Session-specific episode count. This is useful in case of restart: counters from the old session won't count
   */
  size_t _sessionEpisodeCount;

  /**
   * @brief Session-specific generation count. This is useful in case of restart: counters from the old session won't count
   */
  size_t _sessionGeneration;

  /**
   * @brief Session-specific policy update count. This is useful in case of restart: counters from the old session won't count
   */
  size_t _sessionPolicyUpdateCount;

  /**
   * @brief Session-specific counter that keeps track of how many experiences need to be obtained this session to reach the start training threshold.
   */
  size_t _sessionExperiencesUntilStartSize;

  /**
  * @brief Stores experience replay states.
  */
  cBuffer<experience_t> _experienceReplay;

  /**
  * @brief Stores the current sequence of states observed by the agent (limited to time sequence length defined by the user)
  */
  cBuffer<std::vector<float>> _stateTimeSequence;

  /**
  * @brief Stores the priority annealing rate.
  */
  float _priorityAnnealingRate;

  /**
  * @brief Stores the importance weight annealing factor.
  */
  float _importanceWeightAnnealingRate;

  /**
  * @brief Stores the current policy configuration.
  */
  knlohmann::json _trainingCurrentPolicy;

  /**
  * @brief Stores the training policy configuration that has produced the best results.
  */
  knlohmann::json _trainingBestPolicy;

  /**
  * @brief Stores the candidate policy configuration that has produced the best results.
  */
  knlohmann::json _testingBestPolicy;

  /**
  * @brief Stores the state of training networks, necessary to continue learning at a later point.
  */
  knlohmann::json _trainingState;

  /**
  * @brief Storage for the pointer to the learning problem
  */
  problem::ReinforcementLearning *_problem;

  /**
   * @brief Random device for the generation of shuffling numbers
   */
  std::random_device rd;

  /**
  * @brief Mersenne twister for the generation of shuffling numbers
  */
  std::mt19937 *mt;

  /****************************************************************************************************
   * Session-wise Profiling Timers
   ***************************************************************************************************/

  /**
  * @brief [Profiling] Measures the amount of time taken by the generation
  */
  double _sessionRunningTime;

  /**
  * @brief [Profiling] Measures the amount of time taken by ER serialization
  */
  double _sessionSerializationTime;

  /**
   * @brief [Profiling] Stores the computation time per episode taken by Agents
   */
  double _sessionAgentComputationTime;

  /**
   * @brief [Profiling] Measures the average communication time per episode taken by Agents
   */
  double _sessionAgentCommunicationTime;

  /**
   * @brief [Profiling] Measures the average policy evaluation time per episode taken by Agents
   */
  double _sessionAgentPolicyEvaluationTime;

  /**
   * @brief [Profiling] Measures the time taken to update the policy in the current generation
   */
  double _sessionPolicyUpdateTime;

  /**
   * @brief [Profiling] Measures the time taken to update the attend the agent's state
   */
  double _sessionAgentAttendingTime;

  /****************************************************************************************************
   * Generation-wise Profiling Timers
   ***************************************************************************************************/

  /**
  * @brief [Profiling] Measures the amount of time taken by the generation
  */
  double _generationRunningTime;

  /**
  * @brief [Profiling] Measures the amount of time taken by ER serialization
  */
  double _generationSerializationTime;

  /**
   * @brief [Profiling] Stores the computation time per episode taken by Agents
   */
  double _generationAgentComputationTime;

  /**
   * @brief [Profiling] Measures the average communication time per episode taken by Agents
   */
  double _generationAgentCommunicationTime;

  /**
   * @brief [Profiling] Measures the average policy evaluation time per episode taken by Agents
   */
  double _generationAgentPolicyEvaluationTime;

  /**
   * @brief [Profiling] Measures the time taken to update the policy in the current generation
   */
  double _generationPolicyUpdateTime;

  /**
   * @brief [Profiling] Measures the time taken to update the attend the agent's state
   */
  double _generationAgentAttendingTime;

  /****************************************************************************************************
   * Common Agent functions
   ***************************************************************************************************/

  /**
   * @brief Mini-batch based normalization routine for Neural Networks with state and action inputs (typically critics)
   * @param neuralNetwork Neural Network to normalize
   * @param miniBatchSize Number of entries in the normalization minibatch
   * @param normalizationSteps How many normalization steps to perform (and grab the average)
   */
  void normalizeStateActionNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps);

  /**
   * @brief Mini-batch based normalization routine for Neural Networks with state inputs only (typically policy)
   * @param neuralNetwork Neural Network to normalize
   * @param miniBatchSize Number of entries in the normalization minibatch
   * @param normalizationSteps How many normalization steps to perform (and grab the average)
   */
  void normalizeStateNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps);

  /**
  * @brief Additional post-processing of episode after episode terminated.
  * @param episode A vector of experiences.
  */
  void processEpisode(size_t episodeId, std::vector<knlohmann::json> &episode);

  /**
  * @brief Updates probabilities of experiences stored based on their priority.
  */
  void updateExperienceReplayProbabilities();

  /**
  * @brief Generates an experience mini batch from the replay memory
  * @param size Size of the mini batch to create
  * @return A vector with the indexes to the experiences in the mini batch
  */
  std::vector<size_t> generateMiniBatch(size_t size);

  /**
  * @brief Resets time sequence within the agent, to forget past actions from other episodes
  */
  void resetTimeSequence();

  /**
  * @brief Calculates the starting experience index of the time sequence for the selected experience
  * @param expId The index of the latest experience in the sequence
  * @return The starting time sequence index
  */
  size_t getTimeSequenceStartExpId(size_t expId);

  /**
   * @brief Gets a vector of states corresponding of time sequence corresponding to the provided last experience index
   * @param expId The index of the latest experience in the sequence
   * @return The time step vector of states
   */
  std::vector<std::vector<float>> getStateTimeSequence(size_t expId);

  /**
   * @brief Gets a vector of states corresponding of time sequence corresponding to the provided second-to-last experience index for which a truncated state exists
   * @param expId The index of the second-to-latest experience in the sequence
   * @return The time step vector of states, including the truncated state
   */
  std::vector<std::vector<float>> getTruncatedStateTimeSequence(size_t expId);

  /**
  * @brief Gets a vector of state/actions corresponding of time sequence corresponding to the provided last experience index
  * @param expId The index of the latest experience in the sequence
  * @return The time step vector of state/actions
  */
  std::vector<std::vector<float>> getStateActionTimeSequence(size_t expId);

  /**
   * @brief Returns the retrace value for the given experience
   * @param expId The id (position within the replay memory) of the experience
   * @return The qRet value of the experience
   */
  float retraceFunction(size_t expId);

  /**
   * @brief Listens to incoming experience from the given agent, sends back policy or terminates the episode depending on what's needed
   * @param agentId The Agent's ID
   */
  void attendAgent(size_t agentId);

  /**
   * @brief Serializes the experience replay into a JSON compatible format
   */
  void serializeExperienceReplay();

  /**
   * @brief Deserializes a JSON object into the experience replay
   */
  void deserializeExperienceReplay();

  /**
   * @brief Runs a generation when running in training mode
   */
  void trainingGeneration();

  /**
   * @brief Runs a generation when running in testing mode
   */
  void testingGeneration();

  /****************************************************************************************************
   * Virtual functions (responsibilities) for learning algorithms to fulfill
   ***************************************************************************************************/

  /**
   * @brief Calculates the state value function V(s,a)
   * @param stateSequence State sequence to evaluate
   * @return The value of V(s)
   */
  virtual float stateValueFunction(const std::vector<std::vector<float>> &stateSequence) = 0;

  /**
  * @brief Trains the Agent's policy, based on the new experiences
  */
  virtual void trainPolicy() = 0;

  /**
   * @brief Obtains the policy hyperaparamters from the learner for the agent to generate new actions
   * @return The current policy hyperparameters
   */
  virtual knlohmann::json getAgentPolicy() = 0;

  /**
  * @brief Function to pass a state time series through the NN and calculates the action probabilities, along with any additional information
  * @param stateSequence The state time series (Format: TxS, where T is the time series lenght, and S is the state size)
  * @return A JSON object containing the information produced by the policy given the current state
  */
  virtual knlohmann::json runPolicy(const std::vector<std::vector<float>> &stateSequence) = 0;

  /**
  * @brief Updates the agent's hyperparameters
  * @param hyperparameters The hyperparameters to update the agent.
  */
  virtual void setAgentPolicy(const knlohmann::json &hyperparameters) = 0;

  /**
   * @brief Sets training state to continue learning from a previous run or state file
   * @param state The state to update the training NNs.
   */
  virtual void setTrainingState(const knlohmann::json &state) = 0;

  /**
   * @brief Obtains the current training state to store to a file and continue learning later
   * @return The state to store
   */
  virtual knlohmann::json getTrainingState() = 0;

  /**
   * @brief Initializes the internal state of the policy
   */
  virtual void initializeAgent() = 0;

  /**
   * @brief Prints information about the training policy
   */
  virtual void printAgentInformation() = 0;

  /**
   * @brief Gathers the next action either from the policy or randomly
   * @param sample Sample on which the action and metadata will be stored
   */
  virtual void getAction(korali::Sample &sample) = 0;

  virtual void updateExperienceMetadata(size_t expId) = 0;

  void runGeneration() override;
  void printGenerationAfter() override;
  void initialize() override;
  void finalize() override;
};

} // namespace solver
} // namespace korali

#endif // _KORALI_AGENT_HPP_

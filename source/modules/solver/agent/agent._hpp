#ifndef _KORALI_AGENT_HPP_
#define _KORALI_AGENT_HPP_

#include "engine.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/problem/supervisedLearning/supervisedLearning.hpp"
#include "modules/solver/learner/deepGD/deepGD.hpp"
#include <algorithm> // std::shuffle
#include <random>

namespace korali
{
namespace solver
{
class Agent : public Solver
{
  public:
  /**
 * @brief Korali engine for running internal experiments
 */
  korali::Engine _engine;

  /**
   * @brief Variable to keep track of the current noise added to the action
   */
  std::vector<double> _currentActionNoises;

  /**
   * @brief Pointer to training experiment's problem
   */
  problem::SupervisedLearning *_criticProblem;

  /**
  * @brief Pointer to training experiment's learner
  */
  learner::DeepGD *_criticLearner;

  /**
   * @brief Korali experiment for the training of the Qvalue-estimator
   */
  korali::Experiment _criticExperiment;

  /**
  * @brief Storage for the pointer to the learning problem
  */
  problem::ReinforcementLearning *_problem;

  /**
   * @brief Random device for the generation of shuffling numbers
   */
  std::random_device rd;

  /**
  * @brief Mersenne twister for the generation of shuffling numbers
  */
  std::mt19937 *mt;

  /**
   * @brief Gets a random action for the given state
   * @param state The vector containing the current agent's state
   * @return A vector containing the random action
   */
  std::vector<double> getRandomAction(const std::vector<double> &state);

  /**
   * @brief Gets the next action for the agent, based on the state and current policy, for training purposes (may contain noise)
   * @param state The vector containing the current agent's state
   * @return A vector containing the action
   */
  virtual std::vector<double> getTrainingAction(const std::vector<double> &state) = 0;

  /**
   * @brief Gets the next action for the actor, based on the state and best policy, for inference purposes (may not contain noise)
   * @param state The vector containing the current agent's state
   * @return A vector containing the action
   */
  std::vector<double> getInferenceAction(const std::vector<double> &state);

  /**
  * @brief Trains the algorithm's critic (Q), based on the new experiences
  */
  void trainCritic();

  /**
   * @brief Calculates the state+action value function Q(s,a) = Expectation_s,a [ R | s,a ] of a given experience using the latest critic
   * @param state state to evaluate
   * @param action action to evaluate
   * @return The value of Q(s,a)
   */
  double stateActionValueFunction(const std::vector<double>& state, const std::vector<double>& action);

  /**
   * @brief Calculates the state value function V(s) = Expectation_a [ Q(s,a) | s ] of a given experience using the latest critic
   * @param state state to evaluate
   * @return The value of the V(s)
   */
  virtual double stateValueFunction(const std::vector<double>& state) = 0;

  /**
   * @brief Calculates the probability density of selecting the input state+action given the latest critic
   * @param state state to evaluate
   * @param action action to evaluate
   * @return p(s,a | Q)
   */
  virtual double getStateActionProbabilityDensity(const std::vector<double>& state, const std::vector<double>& action) = 0;

  /**
   * @brief Calculates (recursively) the Qretrace value of a given (state/action) experience
   * @param expId Position within the experience replay history that locates the given experience
   * @return The value of the retrace function for the given experience
   */
  double retraceFunction(size_t expId);

  /**
  * @brief Performs the training of the algorithm's policy, based on the new experiences
  */
  virtual void trainPolicy() = 0;

  /**
  * @brief Updates the agent's hyperparameters
  * @param hyperparameters The hyperparameters to update the agent.
  */
  virtual void updateHyperparameters(const knlohmann::json &hyperparameters) = 0;

  /**
  * @brief Normalizes neural network batch normalization mean and variances
  * @param nn The neural network to normalize
  */
  void normalizeNeuralNetwork(korali::NeuralNetwork *nn);

  /**
  * @brief Function to query policy and get the best action for the given state
  * @param state The input state
  * @return The action, given by the policy, corresponding to the state and the current policy hyperparameters
  */
  virtual std::vector<double> queryPolicy(const std::vector<double> &state) = 0;

  /**
    * @brief Initializes the internal state of the policy
    */
  virtual void initializePolicy() = 0;

  /**
 * @brief Runs a generation of the environment(s), running an action on each episode, and updating the policy.
 */
  void runGeneration() override;

  /**
   * @brief Prints information about the training policy
   */
  virtual void printPolicyInformation() = 0;

  void printGenerationAfter() override;
  void initialize() override;
};

} // namespace solver
} // namespace korali

#endif // _KORALI_AGENT_HPP_

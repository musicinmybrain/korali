#ifndef _KORALI_AGENT_CONTINUOUS_HPP_
#define _KORALI_AGENT_CONTINUOUS_HPP_

#include "modules/distribution/univariate/beta/beta.hpp"
#include "modules/problem/reinforcementLearning/continuous/continuous.hpp"
#include "modules/solver/agent/agent.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
class Continuous : public Agent
{
  public:
  /**
 * @brief Storage for the pointer to the (continuous) learning problem
 */
  problem::reinforcementLearning::Continuous *_problem;

  /**
   * @brief Calculates the gradient of policy wrt to the parameter of the 2nd (current) distribution evaluated at old action.
   * @param oldAction Action from memory
   * @param curParamsOne cur means for Normal distribution, beta for Beta distribution
   * @param curParamsTwo cur sigmas for Normal distribution, alpha for Beta distribution
   * @param oldParamsOne old means for Normal distribution, beta for Beta distribution
   * @param oldParamsTwo old sigmas for Normal distribution, alpha for Beta distribution
   * @return gradient of policy wrt curParamsOne and curParamsTwo
   */
  std::vector<float> calculateImportanceWeightGradient(const std::vector<float> &action, knlohmann::json &curPolicy, knlohmann::json &oldPolicy);

  /**
   * @brief Calculates the gradient of KL(p_old, p_cur) wrt to the parameter of the 2nd (current) distribution.
   * @param oldParamsOne old means for Normal distribution, alpha for Beta distribution
   * @param oldParamsTwo old sigmas for Normal distribution, beta for Beta distribution
   * @param curParamsOne cur means for Normal distribution, alpha for Beta distribution
   * @param curParamsTwo cur sigmas for Normal distribution, beta for Beta distribution
   * @return gradient of KL wrt curParamsOne and curParamsTwo
   */
  std::vector<float> calculateKLDivergenceGradient(knlohmann::json &oldPolicy, knlohmann::json &curPolicy);

  /**
  * @brief Function to generate randomized actions from neural network output.
  * @param curMeans parameter vector for policy distribution
  * @param curSigmas parameter vector for policy distribution
  * @return An action vector
  */
  std::vector<float> generateTrainingAction(const std::vector<float> &curMeans, const std::vector<float> &curSigmas);

  /**
  * @brief Function to generate deterministic actions from neural network output required for policy evaluation, respectively testing.
  * @param curMeans parameter vector for policy distribution
  * @param curSigmas parameter vector for policy distribution
  * @return An action vector
  */
  std::vector<float> generateTestingAction(const std::vector<float> &curMeans, const std::vector<float> &curSigmas);

  float calculateImportanceWeight(const std::vector<float> &action, knlohmann::json &curPolicy, knlohmann::json &oldPolicy) override;
  virtual void getAction(korali::Sample &sample) override;
  virtual void initializeAgent();
};

} // namespace agent
} // namespace solver
} // namespace korali

#endif // _KORALI_AGENT_CONTINUOUS_HPP_

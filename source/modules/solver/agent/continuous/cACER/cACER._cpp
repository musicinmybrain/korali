#include "engine.hpp"
#include "modules/solver/agent/continuous/cACER/cACER.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
void cACER::initializeAgent()
{
  // Initializing common discrete agent configuration
  Continuous::initializeAgent();

  /*********************************************************************
 * Initializing V(s)
 *********************************************************************/

  _vExperiment["Problem"]["Type"] = "Supervised Learning";
  _vExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _vExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _vExperiment["Solver"]["Learning Rate"] = _criticLearningRate;
  _vExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
  _vExperiment["Solver"]["Steps Per Generation"] = 1;
  _vExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t p = 0; p < _criticMiniBatchSize; p++)
  {
    for (size_t i = 0; i < _problem->_stateVectorSize; i++)
      _vExperiment["Problem"]["Input"]["Data"][0][p][i] = 0.0;
    _vExperiment["Problem"]["Solution"]["Data"][0][p][0] = 0.0; // V(s)
  }

  size_t vOutputLayerId = _vExperiment["Solver"]["Neural Network"]["Layers"].size() - 1;
  _vExperiment["Solver"]["Neural Network"]["Layers"][vOutputLayerId]["Weight Scaling"] = 0.1;

  if (_criticNeuralNetwork["Layers"][vOutputLayerId]["Activation Function"]["Type"] != "Elementwise/Linear")
    KORALI_LOG_ERROR("Critic Network Output Layers' (%zu) Activation Function must be of type 'Elementwise/Linear'", vOutputLayerId);

  // Running initialization to verify that the configuration is correct
  _vExperiment.initialize();

  // Getting learner pointers
  _vProblem = dynamic_cast<problem::SupervisedLearning *>(_vExperiment._problem);
  _vLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_vExperiment._solver);

  /*********************************************************************
 * Initializing A(s,a)
 *********************************************************************/

  _aExperiment["Problem"]["Type"] = "Supervised Learning";
  _aExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _aExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _aExperiment["Solver"]["Learning Rate"] = _criticLearningRate;
  _aExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
  _aExperiment["Solver"]["Steps Per Generation"] = 1;
  _aExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t p = 0; p < _criticMiniBatchSize + _criticMiniBatchSize * _criticAdvantageFunctionPopulation; p++)
  {
    for (size_t i = 0; i < _problem->_stateVectorSize + _problem->_actionVectorSize; i++)
      _aExperiment["Problem"]["Input"]["Data"][0][p][i] = 0.0;
    _aExperiment["Problem"]["Solution"]["Data"][0][p][0] = 0.0; // A(s,a)
  }

  size_t aOutputLayerId = _aExperiment["Solver"]["Neural Network"]["Layers"].size() - 1;
  _aExperiment["Solver"]["Neural Network"]["Layers"][aOutputLayerId]["Weight Scaling"] = 0.1;

  // Running initialization to verify that the configuration is correct
  _aExperiment.initialize();

  // Getting learner pointers
  _aProblem = dynamic_cast<problem::SupervisedLearning *>(_aExperiment._problem);
  _aLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_aExperiment._solver);

  /*********************************************************************
  * Initializing Policy-Related Structures
  *********************************************************************/

  // Creating and running Actor Learning Experiments

  _policyExperiment["Problem"]["Type"] = "Supervised Learning";
  _policyExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _policyExperiment["Solver"]["Loss Function"] = "Direct Gradient";
  _policyExperiment["Solver"]["Optimizer"] = _policyOptimizer;
  _policyExperiment["Solver"]["Learning Rate"] = _policyLearningRate;
  _policyExperiment["Solver"]["Steps Per Generation"] = 1;
  _policyExperiment["Solver"]["Neural Network"] = _policyNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t p = 0; p < _criticMiniBatchSize; p++)
  {
    for (size_t i = 0; i < _problem->_stateVectorSize; i++)
      _policyExperiment["Problem"]["Input"]["Data"][0][p][i] = 0.0;

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      _policyExperiment["Problem"]["Solution"]["Data"][0][p][i] = 0.0; // Gaussian Mean
  }

  size_t policyOutputLayerId = _policyNeuralNetwork["Layers"].size() - 1;
  _policyNeuralNetwork["Solver"]["Neural Network"]["Layers"][policyOutputLayerId]["Weight Scaling"] = 0.1;
  if (_policyNeuralNetwork["Layers"][policyOutputLayerId]["Activation Function"]["Type"] != "Elementwise/Tanh")
    KORALI_LOG_ERROR("Policy Network Output Layers' (%zu) Activation Function must be of type 'Elementwise/Tanh'", policyOutputLayerId);

  // Running initialization to verify that the configuration is correct
  _policyExperiment.initialize();

  // Getting learner pointers
  _policyProblem = dynamic_cast<problem::SupervisedLearning *>(_policyExperiment._problem);
  _policyLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_policyExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Getting current hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getTrainingHyperparameters();

  // Storing initial average policy parameters
  if (_k->_currentGeneration == 0)
    _policyAverageHyperparameters = _policyCurrentHyperparameters;
}

float cACER::currentActionAdvantageFunction(const std::vector<float> &state, const std::vector<float> &action)
{
  // Storage to put together state and action
  std::vector<float> stateActionInput(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Calculating max_a A(s_i+1, a)
  for (size_t j = 0; j < _problem->_stateVectorSize; j++) stateActionInput[j] = state[j];
  for (size_t j = 0; j < _problem->_actionVectorSize; j++) stateActionInput[j + _problem->_stateVectorSize] = action[j];
  float a = _aLearner->getEvaluation({{stateActionInput}})[0][0][0];

  return a;
}

float cACER::averageActionAdvantageFunction(const std::vector<float> &curMeans, const std::vector<float> &curSigmas, const std::vector<float> &state)
{
  // Creating storage for average action minibatch
  std::vector<float> aResults(_criticAdvantageFunctionPopulation);

  // Initializing average A(s,a) counter
  float avgA = 0.0;

  // Filling minibatch to calculate average action Q
  for (size_t i = 0; i < _criticAdvantageFunctionPopulation; i++)
  {
    // Generating random a'
    auto newAction = generateTrainingAction(curMeans, curSigmas);

    // Running evaluation of the entire minibatch
    aResults[i] = currentActionAdvantageFunction(state, newAction);
  }

  // Summing A(s_i+1, a') for many a'
  for (size_t i = 0; i < _criticAdvantageFunctionPopulation; i++)
    avgA += aResults[i];

  // Normalizing Average
  avgA /= (float)_criticAdvantageFunctionPopulation;

  return avgA;
}

std::tuple<float, float> cACER::retraceFunction(size_t expId)
{
  // Finding last experience in the episode that corresponds to expId
  ssize_t startId = expId;
  ssize_t endId = startId;
  while (_experienceReplay[endId].termination == e_nonTerminal) endId++;

  // Initializing qRet and qOpc to terminal reward
  float qOpc = 0.0f;
  float qRet = 0.0f;

  // Initializing values coming from the next experience (for latest experience, just reward)
  float qRetNext = _experienceReplay[endId].reward;
  float qOpcNext = _experienceReplay[endId].reward;

  // Setting starting experience as the second to last
  ssize_t curId = endId - 1;

  // Now checking whether any necessary qTmp is present in cache
  for (ssize_t i = startId; i < endId; i++)
  {
    // If the difference in age is within the range, then it's a cache hit
    if (_experienceReplay[i].cache.contains("QRetNext") && _experienceReplay[i].cache.contains("QOpcNext"))
    {
      qRetNext = _experienceReplay[i].cache.get("QRetNext");
      qOpcNext = _experienceReplay[i].cache.get("QOpcNext");
      curId = i;
      break;
    }
  }

  // Now iterating backwards to calculate the rest of qRet
  for (; curId >= startId; curId--)
  {
    // Getting current reward
    float curReward = _experienceReplay[curId].reward;

    // Re-calculating qRet and qOpc
    qRet = curReward + _criticDiscountFactor * qRetNext;
    qOpc = curReward + _criticDiscountFactor * qOpcNext;

    // If this is the starting experience, return the current value
    if (curId == startId) break;

    // Getting current state
    std::vector<float> curState = _experienceReplay[curId].state;

    // Getting current action index
    auto curAction = _experienceReplay[curId].action;

    // Updating action distributions for the current state
    auto policy = runPolicy(curState);
    auto curMeans = policy["Action Means"].get<std::vector<float>>();
    auto curSigmas = policy["Action Sigmas"].get<std::vector<float>>();

    // Now calculating V, A, and avgA for current state/action
    float curA = currentActionAdvantageFunction(curState, curAction);
    float avgA = averageActionAdvantageFunction(curMeans, curSigmas, curState);
    float curV = stateValueFunction(curState);

    // Current Q = V - A - avgA
    float curQ = curV + curA - avgA;

    // Getting means and sigmas
    auto oldMeans = _experienceReplay[curId].policy["Action Means"].get<std::vector<float>>();
    auto oldSigmas = _experienceReplay[curId].policy["Action Sigmas"].get<std::vector<float>>();

    // Calculating importance weight
    float importanceWeight = calculateImportanceWeight(curAction, curMeans, curSigmas, oldMeans, oldSigmas);

    // Now calculating truncated importance weight with 1.0 as truncation factor
    float truncatedImportanceWeight = std::min(1.0f, importanceWeight);

    // Updating qRet
    qRetNext = truncatedImportanceWeight * (qRet - curQ) + curV;
    qOpcNext = (qOpc - curQ) + curV;

    // Refreshing cache with the calculated value
    _experienceReplay[curId - 1].cache.set("QRetNext", qRetNext);
    _experienceReplay[curId - 1].cache.set("qOpcNext", qOpcNext);
  }

  // Returning qRet
  return std::make_tuple(qRet, qOpc);
}

knlohmann::json cACER::runPolicy(const std::vector<float> &state)
{
  auto eval = _policyLearner->getEvaluation({{state}})[0][0];

  std::vector<float> actionMeans(_problem->_actionVectorSize);
  std::vector<float> actionSigmas(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float sigma = _k->_variables[varIdx]->_explorationSigma;

    actionMeans[i] = eval[i];
    actionSigmas[i] = sigma;
  }

  knlohmann::json policy;
  policy["Action Means"] = actionMeans;
  policy["Action Sigmas"] = actionSigmas;
  return policy;
}

float cACER::stateValueFunction(const std::vector<float> &state)
{
  // Forward propagating state through the critic to get V(s)
  auto v = _vLearner->getEvaluation({{state}})[0][0][0];

  // Returning V(s)
  return v;
}

void cACER::trainPolicy()
{
  // Resetting cumulative Q*, for statistical purposes
  _cumulativeQStar = 0.;
  _cumulativeQStarSquared = 0.;

  // Init input count for average problem
  size_t aInputCount = 0;

  // Creating minibatch for the critic update
  auto miniBatchIndexes = generateMiniBatch(_criticMiniBatchSize);

  for (size_t expPos = 0; expPos < _criticMiniBatchSize; expPos++)
  {
    // Selecting a uniformly random selected, yet not repeated experience
    size_t expId = miniBatchIndexes[expPos];

    // Calculating qRet and qOpc (equation 5 in ACER paper, also see below)
    float qRet, qOpc;
    std::tie(qRet, qOpc) = retraceFunction(expId);

    // Getting experience's state and action
    std::vector<float> expState = _experienceReplay[expId].state;
    std::vector<float> expAction = _experienceReplay[expId].action;

    // Getting policy distribution parameters from the current policy NN
    auto policy = runPolicy(expState);
    auto curMeans = policy["Action Means"].get<std::vector<float>>();
    auto curSigmas = policy["Action Sigmas"].get<std::vector<float>>();

    // Calculating inverse variances
    std::vector<float> inverseVariances(_problem->_actionVectorSize);
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) inverseVariances[i] = 1.0f / (curSigmas[i] * curSigmas[i]);

    // Getting policy distribution parameters from the old policy NN
    std::vector<float> oldMeans = _experienceReplay[expId].policy["Action Means"].get<std::vector<float>>();
    std::vector<float> oldSigmas = _experienceReplay[expId].policy["Action Sigmas"].get<std::vector<float>>();

    // Allocating stateAction placeholder vector for combining a state and an action
    std::vector<float> stateActionVector(_problem->_stateVectorSize + _problem->_actionVectorSize);
    for (size_t i = 0; i < _problem->_stateVectorSize; i++) stateActionVector[i] = expState[i];

    // Calculating V(state) with the current policy
    float vExpState = stateValueFunction(expState);
    _cumulativeQStar += vExpState;
    _cumulativeQStar += vExpState * vExpState;

    // Getting new action given the current policy
    std::vector<float> newAction = generateTrainingAction(curMeans, curSigmas);

    // Getting A of experience action
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) stateActionVector[_problem->_stateVectorSize + i] = expAction[i];
    auto aExpStateExpAction = _aLearner->getEvaluation({{stateActionVector}})[0][0][0];

    // Getting A of new action
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) stateActionVector[_problem->_stateVectorSize + i] = newAction[i];
    auto aExpStateNewAction = _aLearner->getEvaluation({{stateActionVector}})[0][0][0];

    // Getting n action samples from the policy
    std::vector<std::vector<float>> actionSamples(_criticAdvantageFunctionPopulation);
    for (size_t i = 0; i < _criticAdvantageFunctionPopulation; i++) actionSamples[i] = generateTrainingAction(curMeans, curSigmas);

    // Getting sum_n(A(x,u)) with u ~ policy(x)
    float aSamplesMean = 0.0;
    for (size_t sampleIdx = 0; sampleIdx < _criticAdvantageFunctionPopulation; sampleIdx++)
    {
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
        stateActionVector[_problem->_stateVectorSize + i] = actionSamples[sampleIdx][i];
      aSamplesMean += _aLearner->getEvaluation({{stateActionVector}})[0][0][0];
    }
    aSamplesMean = aSamplesMean / (float)_criticAdvantageFunctionPopulation;

    // Getting Q(s, a) for the experience action
    float qExpStateExpAction = vExpState + aExpStateExpAction - aSamplesMean;

    // Getting Q(s, a') for the new (policy) action
    float qExpStateNewAction = vExpState + aExpStateNewAction - aSamplesMean;

    // Now calculating importance weight for s,a (old)
    float oldImportanceWeight = calculateImportanceWeight(expAction, curMeans, curSigmas, oldMeans, oldSigmas);

    // Now calculating importance weight for s, a' (new)
    float newImportanceWeight = calculateImportanceWeight(newAction, curMeans, curSigmas, oldMeans, oldSigmas);

    /*****************************************
     * Policy Section
     *****************************************/

    // Now calculating truncated importance weight
    float truncatedImportanceWeight = std::min(_importanceWeightTruncation, oldImportanceWeight);

    // First part of the gradient vector
    float g1 = truncatedImportanceWeight * (qOpc - vExpState);

    // Now calculating r(a) - c / r(a) ratio
    float importanceRatio = 1.0f - (_importanceWeightTruncation / newImportanceWeight);

    // Now calculating the correction weight
    float correctionWeight = importanceRatio > 0.0 ? importanceRatio : 0.0;

    // Compute the expectation for the second part of the gradient vector
    float g2 = correctionWeight * (qExpStateNewAction - vExpState);

    // Now Calculating Acer gradient for all of the action's variables (mean and sigma)
    std::vector<float> gPolicy(_problem->_actionVectorSize, 0.0);
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      float diffExpAction = expAction[i] - curMeans[i];
      float diffNewAction = newAction[i] - curMeans[i];
      gPolicy[i] = inverseVariances[i] * (g1 * diffExpAction + g2 * diffNewAction);
    }

    ///////////////// KL Calculation /////////////////////

    // Now calculating trust region, if required
    if (_policyTrustRegionEnabled)
    {
      // Getting policy distribution parameters given the average policy
      _policyLearner->setInferenceHyperparameters(_policyAverageHyperparameters);

      // Getting policy distribution parameters from the average policy NN
      auto avgMeans = _policyLearner->getEvaluation({{expState}})[0][0];

      // Obtaining KL Divergence gradients for the current state.
      std::vector<float> k(_problem->_actionVectorSize);
      for (size_t i = 0; i < _problem->_actionVectorSize; i++) k[i] = inverseVariances[i] * (curMeans[i] - avgMeans[i]);

      // Getting dot product between the gradient vector and k
      auto gkDotProduct = dotProduct(k, gPolicy);

      // Getting norm(k)^2, simply by dot product of k and itself
      auto kNormSquared = dotProduct(k, k);

      // Getting magnitude of adjustment
      float adjustmentMagnitude = std::max(0.0f, (gkDotProduct - _policyTrustRegionDivergenceConstraint) / kNormSquared);

      // Adjusting gradients to trust region
      for (size_t i = 0; i < gPolicy.size(); i++) gPolicy[i] = gPolicy[i] - adjustmentMagnitude * k[i];

      // Getting policy distribution parameters given the average policy
      _policyLearner->setInferenceHyperparameters(_policyCurrentHyperparameters);
    }

    ///////////////// Storing Gradients /////////////////////

    _policyProblem->_inputData[0][expPos] = expState;
    _policyProblem->_solutionData[0][expPos] = gPolicy;

    /*****************************************
     * Update Networks for Q (A and V) estimation
     *****************************************/

    // Calculating simple truncated importance
    float tImportance = std::min(1.0f, oldImportanceWeight);

    // Calculating error between Qret and Q(s,a)
    float qErr = qRet - qExpStateExpAction;

    ///////////////// Gradients for the State Function V(s) /////////////////

    // Updating inputs to training learner
    _vProblem->_inputData[0][expPos] = expState;
    _vProblem->_solutionData[0][expPos] = {qErr * tImportance + vExpState};

    ///////////////// Gradients for the Advantage Function A(s) /////////////////

    // Putting together state and action
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) stateActionVector[_problem->_stateVectorSize + i] = expAction[i];

    // Updating inputs to training learner
    _aProblem->_inputData[0][aInputCount] = stateActionVector;
    _aProblem->_solutionData[0][aInputCount++] = {qErr};

    // Now adding all additional action samples
    for (size_t sampleIdx = 0; sampleIdx < _criticAdvantageFunctionPopulation; sampleIdx++)
    {
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
        stateActionVector[_problem->_stateVectorSize + i] = actionSamples[sampleIdx][i];
      _aProblem->_inputData[0].push_back(stateActionVector);
      _aProblem->_solutionData[0].push_back({-qErr / (float)_criticAdvantageFunctionPopulation});
    }
  }

  // Declaring engine to launch experiments
  korali::Engine engine;

  // Running one generation of the optimization method with the given mini-batch
  _aLearner->initialize();
  _aLearner->runGeneration();
  _aLearner->finalize();

  // Running one generation of the optimization method with the given mini-batch
  _vLearner->initialize();
  _vLearner->runGeneration();
  _vLearner->finalize();

  // Getting new policy hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getTrainingHyperparameters();

  // Running one generation of the optimization method with the given mini-batch
  _policyLearner->initialize();
  _policyLearner->runGeneration();
  _policyLearner->finalize();

  /****************************************************************************
  * Updating Policy
  ******************************************************************************/

  // Getting new policy hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getTrainingHyperparameters();

  // If using a trust region, softly adopting the new parameters of the average policy, using an adoption rate
  if (_policyTrustRegionEnabled)
    for (size_t i = 0; i < _policyCurrentHyperparameters.size(); i++)
      _policyAverageHyperparameters[i] = _policyTrustRegionAverageNetworkAdoptionRate * _policyAverageHyperparameters[i] + (1 - _policyTrustRegionAverageNetworkAdoptionRate) * _policyCurrentHyperparameters[i];
}

knlohmann::json cACER::getAgentPolicy()
{
  knlohmann::json hyperparameters;
  hyperparameters["Policy"] = _policyLearner->getInferenceHyperparameters();
  return hyperparameters;
}

void cACER::setAgentPolicy(const knlohmann::json &hyperparameters)
{
  auto policyHyperparameters = hyperparameters["Policy"].get<std::vector<float>>();
  _policyLearner->setInferenceHyperparameters(policyHyperparameters);
}

void cACER::setTrainingState(const knlohmann::json &state)
{
  _aLearner->setTrainingHyperparameters(state["A"]["Training"]);
  _aLearner->setInferenceHyperparameters(state["A"]["Inference"]);

  _vLearner->setTrainingHyperparameters(state["V"]["Training"]);
  _vLearner->setInferenceHyperparameters(state["V"]["Inference"]);

  _policyLearner->setTrainingHyperparameters(state["Policy"]["Training"]);
  _policyLearner->setInferenceHyperparameters(state["Policy"]["Inference"]);
}

knlohmann::json cACER::getTrainingState()
{
  knlohmann::json state;
  state["A"]["Training"] = _aLearner->getTrainingHyperparameters();
  state["A"]["Inference"] = _aLearner->getInferenceHyperparameters();

  state["V"]["Training"] = _vLearner->getTrainingHyperparameters();
  state["V"]["Inference"] = _vLearner->getInferenceHyperparameters();

  state["Policy"]["Training"] = _policyLearner->getTrainingHyperparameters();
  state["Policy"]["Inference"] = _policyLearner->getInferenceHyperparameters();
  return state;
}

void cACER::printAgentInformation()
{
  // Updating average Q*, for statistical purposes
  _averageQStar = _cumulativeQStar / _aProblem->_solutionData.size();
  _stdevQStar = sqrt(_cumulativeQStarSquared / _aProblem->_solutionData.size() - _averageQStar * _averageQStar);

  _k->_logger->logInfo("Normal", "Critic Information:\n");
  _k->_logger->logInfo("Normal", " + Average (Std) Q-Value:  %f (%f)\n", _averageQStar, _stdevQStar);

  _k->_logger->logInfo("Normal", "State Value (V) Information:\n");
  _vExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _vExperiment._solver->printGenerationAfter();
  _vExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Advantage (A) Information:\n");
  _aExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _aExperiment._solver->printGenerationAfter();
  _aExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Policy Information:\n");
  _policyExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _policyExperiment._solver->printGenerationAfter();
  _policyExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

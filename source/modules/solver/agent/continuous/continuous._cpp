#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"
#include <gsl/gsl_multifit.h>

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  /*********************************************************************
  * Initializing Action Space Information
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;

    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;
  }

  // Allocate memory for linear controller
  _observationsApproximatorWeights.resize(_problem->_actionVectorSize, std::vector<float>(_problem->_stateVectorSize + 1));
  _observationsApproximatorSigmas.resize(_problem->_actionVectorSize);

  // Building linear controller for observed state action pairs
  gsl_matrix *X = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);
  gsl_matrix *Y = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_actionVectorSize);
  size_t idx = 0;
  for (size_t i = 0; i < _problem->_numberObservedTrajectories; ++i)
  {
    size_t trajectoryLength = _problem->_observationsStates[i].size();
    for (size_t t = 0; t < trajectoryLength; ++t)
    {
      gsl_matrix_set(X, idx, 0, 1.0); // intercept
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
      {
        gsl_matrix_set(X, idx, j + 1, (double)_problem->_observationsStates[i][t][j]);
      }
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        gsl_matrix_set(Y, idx, k, (double)_problem->_observationsActions[i][t][k]);
      }
      idx++;
    }
  }

  _k->_logger->logInfo("Normal", "Linear Approximator Expert Policy\n");

  // Do regression over actions
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    double chisq;
    gsl_vector *c = gsl_vector_alloc(_problem->_stateVectorSize + 1);
    gsl_matrix *cov = gsl_matrix_alloc(_problem->_stateVectorSize + 1, _problem->_stateVectorSize + 1);
    gsl_multifit_linear_workspace *work = gsl_multifit_linear_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);

    // predict action Y_k
    gsl_vector_view y = gsl_matrix_column(Y, k);
    gsl_multifit_linear(X, &y.vector, c, cov, &chisq, work);

    for (size_t j = 0; j < _problem->_stateVectorSize + 1; ++j)
    {
      _observationsApproximatorWeights[k][j] = gsl_vector_get(c, j);
      _k->_logger->logInfo("Normal", "    + Weights  [%zu, %zu] %f \n", k, j, _observationsApproximatorWeights[k][j]);
    }

    gsl_multifit_linear_free(work);
    gsl_vector_free(c);
    gsl_matrix_free(cov);
  }

  // Calculate squared error over all predictions
  std::vector<float> squaredErrors(_problem->_actionVectorSize);
  for (size_t t = 0; t < _problem->_numberObservedTrajectories; ++t)
    for (size_t i = 0; i < _problem->_observationsStates[t].size(); ++i)
    {
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        float approx = _observationsApproximatorWeights[k][0]; // intercept
        for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        {
          approx += _problem->_observationsStates[t][i][j] * _observationsApproximatorWeights[k][j + 1];
        }
        squaredErrors[k] += std::pow(_problem->_observationsActions[t][i][k] - approx, 2.);
      }
    }

  // Set MLE sigma estimates
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    _observationsApproximatorSigmas[k] = std::sqrt(squaredErrors[k] / (float) _problem->_totalObservedStateActionPairs);
    _k->_logger->logInfo("Normal", "    + Sigma    [%zu] %f \n", k, _observationsApproximatorSigmas[k]);
  }

  gsl_matrix_free(X);
  gsl_matrix_free(Y);

}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"];

  // Adding state to the state time sequence
  _stateTimeSequence.add(state);

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward state sequence to get the Gaussian means and sigmas from policy
  auto policy = runPolicy({_stateTimeSequence.getVector()})[0];

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateTrainingAction(policy);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing") action = generateTestingAction(policy);

  /*****************************************************************************
  * Storing the action and it's policy
  ****************************************************************************/

  sample["Policy"]["Action Means"] = policy.actionMeans;
  sample["Policy"]["Action Sigmas"] = policy.actionSigmas;
  sample["Policy"]["State Value"] = policy.stateValue;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateTrainingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Getting parameters from the new and old policies
  const auto &curMeans = curPolicy.actionMeans;
  const auto &curSigmas = curPolicy.actionSigmas;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    action[i] = curMeans[i] + curSigmas[i] * _normalGenerator->getRandomNumber();

  return action;
}

std::vector<float> Continuous::generateTestingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Getting parameters from the new and old policies
  const auto &curMeans = curPolicy.actionMeans;

  // ParamsOne are the Means, ParamsTwo are the Sigmas
  for (size_t i = 0; i < _problem->_actionVectorSize; i++) action[i] = curMeans[i];

  return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  float logpCurPolicy = 0.0f;
  float logpOldPolicy = 0.0f;

  // Getting parameters from the new and old policies
  const auto &oldMeans = oldPolicy.actionMeans;
  const auto &oldSigmas = oldPolicy.actionSigmas;
  const auto &curMeans = curPolicy.actionMeans;
  const auto &curSigmas = curPolicy.actionSigmas;

  // ParamsOne are the Means, ParamsTwo are the Sigmas
  for (size_t i = 0; i < action.size(); i++)
  {
    logpOldPolicy += normalLogDensity(action[i], oldMeans[i], oldSigmas[i]);
    logpCurPolicy += normalLogDensity(action[i], curMeans[i], curSigmas[i]);
  }

  // Now calculating importance weight for the old s,a experience
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.0f) logImportanceWeight = +7.0f;
  if (logImportanceWeight < -7.0f) logImportanceWeight = -7.0f;

  // Calculating actual importance weight by exp
  float importanceWeight = std::exp(logImportanceWeight);
  return importanceWeight;
}

std::vector<float> Continuous::calculateImportanceWeightGradient(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  std::vector<float> grad(2 * _problem->_actionVectorSize, 0.0);

  // Getting parameters from the new and old policies
  const auto &oldMeans = oldPolicy.actionMeans;
  const auto &oldSigmas = oldPolicy.actionSigmas;
  const auto &curMeans = curPolicy.actionMeans;
  const auto &curSigmas = curPolicy.actionSigmas;

  float logImportanceWeight = 0.0;

  // ParamsOne are the Means, ParamsTwo are the Sigmas
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    // Deviation from expAction and current Mean
    float curActionDiff = (action[i] - curMeans[i]);

    // Deviation from expAction and old Mean
    float oldActionDiff = (action[i] - oldMeans[i]);

    // Inverse Variances
    float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
    float oldInvVar = 1. / (oldSigmas[i] * oldSigmas[i]);

    // Calc Imp Weight
    logImportanceWeight += std::log(oldSigmas[i] / curSigmas[i]);
    logImportanceWeight += 0.5 * (oldActionDiff * oldActionDiff * oldInvVar - curActionDiff * curActionDiff * curInvVar);

    // Gradient with respect to Mean
    grad[i] = curActionDiff * curInvVar;

    // Gradient with respect to Sigma
    grad[_problem->_actionVectorSize + i] = (curActionDiff * curActionDiff) * (curInvVar / curSigmas[i]) - 1.0f / curSigmas[i];
  }

  float importanceWeight = std::exp(logImportanceWeight);

  // Scale by importance weight to get gradient
  for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
    grad[i] *= importanceWeight;

  return grad;
}

std::vector<float> Continuous::calculateKLDivergenceGradient(const policy_t &oldPolicy, const policy_t &curPolicy)
{
  std::vector<float> klGrad(2.0 * _problem->_actionVectorSize, 0.0);

  // Getting parameters from the new and old policies
  const auto &oldMeans = oldPolicy.actionMeans;
  const auto &oldSigmas = oldPolicy.actionSigmas;
  const auto &curMeans = curPolicy.actionMeans;
  const auto &curSigmas = curPolicy.actionSigmas;

  for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
  {
    float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
    float actionDiff = (curMeans[i] - oldMeans[i]);

    // KL-Gradient with respect to Mean
    klGrad[i] = actionDiff * curInvVar;

    // Contribution to Sigma from Trace
    float gradTr = -(curInvVar / curSigmas[i]) * oldSigmas[i] * oldSigmas[i];

    // Contribution to Sigma from Quadratic term
    float gradQuad = -(actionDiff * actionDiff) * (curInvVar / curSigmas[i]);

    // Contribution to Sigma from Determinant
    float gradDet = 1.0f / curSigmas[i];

    // KL-Gradient with respect to Sigma
    klGrad[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
  }

  return klGrad;
}

float Continuous::evaluateTrajectoryLogProbability(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions, const std::vector<float> &policyHyperparameter)
{
  knlohmann::json policy;
  policy["Policy"] = policyHyperparameter;
  setAgentPolicy(policy);

  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    auto evaluation = runPolicy({{states[t]}})[0];
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation.actionMeans[d], evaluation.actionSigmas[d]);
  }

  return trajectoryLogProbability;
}

float Continuous::evaluateTrajectoryLogProbabilityWithObservedPolicy(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions)
{
  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    std::vector<float> evaluation(_problem->_actionVectorSize);
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
    {
      // Predict action with linear policy
      evaluation[d] = _observationsApproximatorWeights[d][0];
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        evaluation[d] += states[t][j] * _observationsApproximatorWeights[d][j + 1];
    }
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation[d], _observationsApproximatorSigmas[d]);
  }

  return trajectoryLogProbability;
}


} // namespace agent

} // namespace solver

} // namespace korali

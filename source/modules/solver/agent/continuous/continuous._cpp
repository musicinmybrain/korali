#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  if ((_policyDistribution != "Normal") && (_policyDistribution != "Beta"))
    KORALI_LOG_ERROR("Policy Distribution must be either 'Normal' or 'Beta' (is '%s').", _policyDistribution.c_str());

  /*********************************************************************
  * Initializing Action Space Information
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;

    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;
  }
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<float>>();

  // Adding state to the state time sequence
  _stateTimeSequence.add(state);

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward state sequence to get the Gaussian means and sigmas from policy
  const auto &policy = runPolicy(_stateTimeSequence.getVector());

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateTrainingAction(policy);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing") action = generateTestingAction(policy);

  /*****************************************************************************
  * Storing the action and it's policy
  ****************************************************************************/

  sample["Metadata"]["Experience Policy"] = policy;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateTrainingAction(const knlohmann::json &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);
  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    auto curMeans = curPolicy["Action Means"].get<std::vector<float>>();
    auto curSigmas = curPolicy["Action Sigmas"].get<std::vector<float>>();

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = curMeans[i] + curSigmas[i] * _normalGenerator->getRandomNumber();
  }

  if (_policyDistribution == "Beta")
  {
    auto curAlpha = curPolicy["Action Alpha"].get<std::vector<float>>();
    auto curBeta = curPolicy["Action Beta"].get<std::vector<float>>();

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = ranBetaAlt(_normalGenerator->_range, curAlpha[i], curBeta[i], _actionLowerBounds[i], _actionUpperBounds[i]);
  }

  return action;
}

std::vector<float> Continuous::generateTestingAction(const knlohmann::json &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);
  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    auto curMeans = curPolicy["Action Means"].get<std::vector<float>>();

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = curMeans[i];
  }

  if (_policyDistribution == "Beta")
  {
    auto curAlpha = curPolicy["Action Alpha"].get<std::vector<float>>();
    auto curBeta = curPolicy["Action Beta"].get<std::vector<float>>();

    // ParamsOne are the Means, ParamsTwo are the Variance Coefficients
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      float alpha;
      float beta;
      std::tie(alpha, beta) = betaParamTransformAlt(curAlpha[i], curBeta[i], _actionLowerBounds[i], _actionUpperBounds[i]);

      if (alpha > 1. && beta > 1.)
        action[i] = _actionLowerBounds[i] + (_actionUpperBounds[i] - _actionLowerBounds[i]) * (alpha - 1) / (alpha + beta - 2);
      else if (alpha <= 1. && beta > 1.)
        action[i] = _actionLowerBounds[i];
      else if (alpha > 1. && beta <= 1.)
        action[i] = _actionUpperBounds[i];
      else
        KORALI_LOG_ERROR("Case (a,b <=1) not yet treated (a,b = %f,%f)", alpha, beta);
    }
  }

  return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const knlohmann::json &curPolicy, const knlohmann::json &oldPolicy)
{
  float logpCurPolicy = 0.0f;
  float logpOldPolicy = 0.0f;

  // Getting probability densities for current and past action given current policy
  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    auto oldMeans = oldPolicy["Action Means"].get<std::vector<float>>();
    auto oldSigmas = oldPolicy["Action Sigmas"].get<std::vector<float>>();
    auto curMeans = curPolicy["Action Means"].get<std::vector<float>>();
    auto curSigmas = curPolicy["Action Sigmas"].get<std::vector<float>>();

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < action.size(); i++)
    {
      logpOldPolicy += normalLogDensity(action[i], oldMeans[i], oldSigmas[i]);
      logpCurPolicy += normalLogDensity(action[i], curMeans[i], curSigmas[i]);
    }
  }

  if (_policyDistribution == "Beta")
  {
    // Getting parameters from the new and old policies
    auto oldAlpha = oldPolicy["Action Alpha"].get<std::vector<float>>();
    auto oldBeta = oldPolicy["Action Beta"].get<std::vector<float>>();
    auto curAlpha = curPolicy["Action Alpha"].get<std::vector<float>>();
    auto curBeta = curPolicy["Action Beta"].get<std::vector<float>>();

    // ParamsOne are the Means, ParamsTwo are the Variance Coefficients
    for (size_t i = 0; i < action.size(); i++)
    {
      logpOldPolicy += betaLogDensityAlt(action[i], oldAlpha[i], oldBeta[i], _actionLowerBounds[i], _actionUpperBounds[i]);
      logpCurPolicy += betaLogDensityAlt(action[i], curAlpha[i], curBeta[i], _actionLowerBounds[i], _actionUpperBounds[i]);
    }
  }

  // Now calculating importance weight for the old s,a experience
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.0f) logImportanceWeight = +7.0f;
  if (logImportanceWeight < -7.0f) logImportanceWeight = -7.0f;

  // Calculating actual importance weight by exp
  float importanceWeight = std::exp(logImportanceWeight);
  return importanceWeight;
}

std::vector<float> Continuous::calculateImportanceWeightGradient(const std::vector<float> &action, const knlohmann::json &curPolicy, const knlohmann::json &oldPolicy)
{
  std::vector<float> grad(2 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    auto oldMeans = oldPolicy["Action Means"].get<std::vector<float>>();
    auto oldSigmas = oldPolicy["Action Sigmas"].get<std::vector<float>>();
    auto curMeans = curPolicy["Action Means"].get<std::vector<float>>();
    auto curSigmas = curPolicy["Action Sigmas"].get<std::vector<float>>();

    float logImportanceWeight = 0.0;

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Deviation from expAction and current Mean
      float curActionDiff = (action[i] - curMeans[i]);

      // Deviation from expAction and old Mean
      float oldActionDiff = (action[i] - oldMeans[i]);

      // Inverse Variances
      float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
      float oldInvVar = 1. / (oldSigmas[i] * oldSigmas[i]);

      // Calc Imp Weight
      logImportanceWeight += std::log(oldSigmas[i] / curSigmas[i]);
      logImportanceWeight += 0.5 * (oldActionDiff * oldActionDiff * oldInvVar - curActionDiff * curActionDiff * curInvVar);

      // Gradient with respect to Mean
      grad[i] = curActionDiff * curInvVar;

      // Gradient with respect to Sigma
      grad[_problem->_actionVectorSize + i] = (curActionDiff * curActionDiff) * (curInvVar / curSigmas[i]) - 1.0f / curSigmas[i];
    }

    float importanceWeight = std::exp(logImportanceWeight);

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
      grad[i] *= importanceWeight;
  }

  if (_policyDistribution == "Beta")
  {
    // Getting parameters from the new and old policies
    auto oldAlpha = oldPolicy["Action Alpha"].get<std::vector<float>>();
    auto oldBeta = oldPolicy["Action Beta"].get<std::vector<float>>();
    auto curAlpha = curPolicy["Action Alpha"].get<std::vector<float>>();
    auto curBeta = curPolicy["Action Beta"].get<std::vector<float>>();

    // ParamsOne are the Means, ParamsTwo are the Variance Coefficients
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Variable preparation
      const float muCur = curAlpha[i];
      const float varcoefCur = curBeta[i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

      const float muOld = oldAlpha[i];
      const float varcoefOld = oldBeta[i];

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Log probability of action with old policy params
      const float logpOldPolicy = betaLogDensityAlt(action[i], muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);
      const float invpOldPolicy = std::exp(-logpOldPolicy);

      // Variable preparation
      const float Bab = gsl_sf_beta(alphaCur, betaCur);
      const float psiAb = gsl_sf_psi(alphaCur + betaCur);

      const float logscale = std::log(_actionScalings[i]);
      const float powscale = std::pow(_actionScalings[i], -betaCur - alphaCur + 1.);
      const float factor = -1. * std::pow(action[i] - _actionLowerBounds[i], alphaCur - 1.) * powscale * std::pow(_actionUpperBounds[i] - action[i], betaCur - 1.) * invpOldPolicy / Bab;

      // Rho Grad wrt alpha and beta
      const float daBab = gsl_sf_psi(alphaCur) - psiAb;
      const float drhoda = ((logscale - std::log(action[i] - _actionLowerBounds[i])) + daBab) * factor;
      const float dbBab = gsl_sf_psi(betaCur) - psiAb;
      const float drhodb = (logscale - std::log(_actionUpperBounds[i] - action[i]) + dbBab) * factor;

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Rho Grad wrt mu and varc
      grad[i] = drhoda * dadmu + drhodb * dbdmu;
      grad[_problem->_actionVectorSize + i] = drhoda * dadvarc + drhodb * dbdvarc;
    }
  }

  return grad;
}

std::vector<float> Continuous::calculateKLDivergenceGradient(const knlohmann::json &oldPolicy, const knlohmann::json &curPolicy)
{
  std::vector<float> klGrad(2.0 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    auto oldMeans = oldPolicy["Action Means"].get<std::vector<float>>();
    auto oldSigmas = oldPolicy["Action Sigmas"].get<std::vector<float>>();
    auto curMeans = curPolicy["Action Means"].get<std::vector<float>>();
    auto curSigmas = curPolicy["Action Sigmas"].get<std::vector<float>>();

    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
      float actionDiff = (curMeans[i] - oldMeans[i]);

      // KL-Gradient with respect to Mean
      klGrad[i] = actionDiff * curInvVar;

      // Contribution to Sigma from Trace
      float gradTr = -(curInvVar / curSigmas[i]) * oldSigmas[i] * oldSigmas[i];

      // Contribution to Sigma from Quadratic term
      float gradQuad = -(actionDiff * actionDiff) * (curInvVar / curSigmas[i]);

      // Contribution to Sigma from Determinant
      float gradDet = 1.0f / curSigmas[i];

      // KL-Gradient with respect to Sigma
      klGrad[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
    }
  }

  if (_policyDistribution == "Beta")
  {
    // ParamsOne are the Means, ParamsTwo are the Variance Coefficients
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      auto oldAlpha = oldPolicy["Action Alpha"].get<std::vector<float>>();
      auto oldBeta = oldPolicy["Action Beta"].get<std::vector<float>>();
      auto curAlpha = curPolicy["Action Alpha"].get<std::vector<float>>();
      auto curBeta = curPolicy["Action Beta"].get<std::vector<float>>();

      // Variable preparation
      const float muCur = curAlpha[i];
      const float varcoefCur = curBeta[i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

      const float muOld = oldAlpha[i];
      const float varcoefOld = oldBeta[i];

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);

      const float psiAbCur = gsl_sf_psi(alphaCur + betaCur);
      const float psiAbOld = gsl_sf_psi(alphaOld + betaOld);

      // KL Grad wrt alpha
      const float dklda = (gsl_sf_psi(alphaCur) - psiAbCur - gsl_sf_psi(alphaOld) - psiAbOld) / _actionScalings[i];

      // KL Grad wrt beta
      const float dkldb = (gsl_sf_psi(betaCur) - psiAbCur - gsl_sf_psi(betaOld) - psiAbOld) / _actionScalings[i];

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

      // KL Grad wrt mu and varc
      klGrad[i] = dklda * dadmu + dkldb * dbdmu;
      klGrad[_problem->_actionVectorSize + i] = dklda * dadvarc + dkldb * dbdvarc;
    }
  }

  return klGrad;
}

} // namespace agent

} // namespace solver

} // namespace korali

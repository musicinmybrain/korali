#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  // Allocating continuous-specific replay memory items
  _experienceReplayActionMeans.resize(_experienceReplayMaximumSize);
  _experienceReplayActionSigmas.resize(_experienceReplayMaximumSize);

  /*********************************************************************
  * Initializing Action Noise Sigmas
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _inverseVariances.resize(_problem->_actionVectorSize);
  _actionMeans.resize(_problem->_actionVectorSize);
  _actionSigmas.resize(_problem->_actionVectorSize);
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float sigma = _k->_variables[varIdx]->_explorationSigma;
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;
    if (sigma <= 0.0) KORALI_LOG_ERROR("Value of Exploration Sigma (%f) for action %lu (variable %lu) is not defined or invalid.\n", sigma, i, varIdx);
    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;

    // compute inverse variance
    _actionSigmas[i] = sigma;
    _inverseVariances[i] = 1.0 / (sigma * sigma);
  }
}

void Continuous::processExperience(knlohmann::json &experience)
{
  // Running common experience processor first
  Agent::processExperience(experience);

  // Adding specific information to the memory
  _experienceReplayActionMeans.add(experience["Metadata"]["Action Means"].get<std::vector<float>>());
  _experienceReplayActionSigmas.add(experience["Metadata"]["Action Sigmas"].get<std::vector<float>>());
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<float>>();

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  /*****************************************************************************
  * During training, we follow the Epsilon-greedy strategy. Choose, given a
  * probability (pEpsilon), one from the following:
  *  - Uniformly random action among all possible actions
  *  - Sample action guided by the policy's probability distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training")
  {
    // Getting current probability of random action for the agent
    float pRandom = sample["Random Action Probability"];

    // Getting pGreedy = U[0,1] for the epsilon-greedy strategy
    float pEpsilon = _uniformGenerator->getRandomNumber();

    // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
    if (pEpsilon < pRandom)
    {
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      {
        // Producing random (uniform) number to select within an lower/upper bound range
        float x = _uniformGenerator->getRandomNumber();

        // Producing the random value for action element i
        action[i] = _actionLowerBounds[i] + x * (_actionUpperBounds[i] - _actionLowerBounds[i]);

        sample["Metadata"]["Action Means"][i] = 0.5 * (_actionLowerBounds[i] + _actionUpperBounds[i]);
        sample["Metadata"]["Action Sigmas"][i] = 0.3 * (_actionUpperBounds[i] - _actionLowerBounds[i]);
      }
    }
    else // else we select guided by the policy's probability distribution
    {
      // Update the Gaussian means and sigmas of the actions given by the agent's policy
      forwardPolicy(state);

      for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
      {
        // Generating action from the normal distribution
        action[i] = _actionMeans[i] + _actionSigmas[i] * _normalGenerator->getRandomNumber();

        // Clipping actions
        if (action[i] > _actionUpperBounds[i])
          action[i] = _actionUpperBounds[i];
        else if (action[i] < _actionLowerBounds[i])
          action[i] = _actionLowerBounds[i];
      }

      // Storing selection metadata
      sample["Metadata"]["Action Means"] = _actionMeans;
      sample["Metadata"]["Action Sigmas"] = _actionSigmas;
    }
  }

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing")
  {
    forwardPolicy(state);
    action = _actionMeans;
  }

  /*****************************************************************************
  * Storing the action itself
  ****************************************************************************/

  sample["Action"] = action;
}

} // namespace agent

} // namespace solver

} // namespace korali

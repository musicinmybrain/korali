#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  if ((_policyDistribution != "Normal") && (_policyDistribution != "Beta"))
    KORALI_LOG_ERROR("Policy Distribution must be either 'Normal' or 'Beta' (is '%s').", _policyDistribution.c_str());

  /*********************************************************************
  * Initializing Action Noise Sigmas
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float sigma = _k->_variables[varIdx]->_explorationSigma;
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;

    if (sigma <= 0.0) KORALI_LOG_ERROR("Value of Exploration Sigma (%f) for action %lu (variable %lu) is not defined or invalid.\n", sigma, i, varIdx);
    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;
  }
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<float>>();

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward Policy to get the Gaussian means and sigmas from policy
  auto policy = runPolicy(state);
  auto curMeans = policy["Action Means"].get<std::vector<float>>();
  auto curSigmas = policy["Action Sigmas"].get<std::vector<float>>();

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateAction(curMeans, curSigmas);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing") action = curMeans;

  /*****************************************************************************
  * Storing the action itself and associated metadata
  ****************************************************************************/

  sample["Policy"] = policy;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateAction(const std::vector<float> &paramsOne, const std::vector<float> &paramsTwo)
{
  std::vector<float> action(_problem->_actionVectorSize);
  if (_policyDistribution == "Normal")
  {
    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = paramsOne[i] + paramsTwo[i] * _normalGenerator->getRandomNumber();
  }
  else /* _policyDistribution == "Beta" */
  {
    // ParamsOne are the Alphas, ParamsTwo are the Betas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = _actionLowerBounds[i] + _actionScalings[i] * gsl_ran_beta(_normalGenerator->_range, paramsOne[i], paramsTwo[i]);
  }

  return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const std::vector<float> &curParamsOne, const std::vector<float> &curParamsTwo, const std::vector<float> &oldParamsOne, const std::vector<float> &oldParamsTwo)
{
  float logpCurPolicy = 0.0f;
  float logpOldPolicy = 0.0f;

  // Getting probability densities for current and past action given current policy
  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
      logpCurPolicy += normalLogDensity(action[i], curParamsOne[i], curParamsTwo[i]);

    for (size_t i = 0; i < action.size(); i++)
      logpOldPolicy += normalLogDensity(action[i], oldParamsOne[i], oldParamsTwo[i]);
  }
  else /* _policyDistribution == "Beta" */
  {
    for (size_t i = 0; i < action.size(); i++)
      logpCurPolicy += betaLogDensity(action[i], curParamsOne[i], curParamsTwo[i]);

    for (size_t i = 0; i < action.size(); i++)
      logpOldPolicy += betaLogDensity(action[i], oldParamsOne[i], oldParamsTwo[i]);
  }

  // Now calculating importance weight for the old s,a experience
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.0f) logImportanceWeight = +7.0f;
  if (logImportanceWeight < -7.0f) logImportanceWeight = -7.0f;

  // Calculating actual importance weight by exp
  float importanceWeight = std::exp(logImportanceWeight);

  return importanceWeight;
}

std::vector<float> Continuous::calculateKLDivergenceGradient(const std::vector<float> &oldParamsOne, const std::vector<float> &oldParamsTwo, const std::vector<float> &curParamsOne, const std::vector<float> &curParamsTwo)
{
  std::vector<float> klGrad(2.0 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      float curInvVar = 1. / (curParamsTwo[i] * curParamsTwo[i]);
      float actionDiff = (curParamsOne[i] - oldParamsOne[i]);

      // KL-Gradient with respect to Mean
      klGrad[i] = actionDiff * curInvVar;

      // Contribution to Sigma from Trace
      float gradTr = -(curInvVar / curParamsTwo[i]) * oldParamsTwo[i] * oldParamsTwo[i];

      // Contribution to Sigma from Quadratic term
      float gradQuad = -(actionDiff * actionDiff) * (curInvVar / curParamsTwo[i]);

      // Contribution to Sigma from Determinant
      float gradDet = 1.0f / curParamsTwo[i];

      // KL-Gradient with respect to Sigma
      klGrad[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
    }
  }
  else /* _policyDistribution == "Beta" */
  {
    // ParamsOne are the Alphas, ParamsTwo are the Betas
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      float psiAbOld = gsl_sf_psi(oldParamsOne[i] + oldParamsTwo[i]);
      float psiAbCur = gsl_sf_psi(curParamsOne[i] + curParamsTwo[i]);

      // KL-Gradient with respect to Alpha
      klGrad[i] = gsl_sf_psi(curParamsOne[i]) - psiAbCur - gsl_sf_psi(oldParamsOne[i]) - psiAbOld;

      // KL-Gradient with respect to Beta
      klGrad[_problem->_actionVectorSize + i] = gsl_sf_psi(curParamsTwo[i]) - psiAbCur - gsl_sf_psi(oldParamsTwo[i]) - psiAbOld;
    }
  }

  return klGrad;
}

} // namespace agent

} // namespace solver

} // namespace korali

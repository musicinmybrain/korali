#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  /*********************************************************************
  * Initializing Action Noise Sigmas
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _inverseVariances.resize(_problem->_actionVectorSize);
  _actionMeans.resize(_problem->_actionVectorSize);
  _actionSigmas.resize(_problem->_actionVectorSize);
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float sigma = _k->_variables[varIdx]->_explorationSigma;
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;
    if (sigma <= 0.0) KORALI_LOG_ERROR("Value of Exploration Sigma (%f) for action %lu (variable %lu) is not defined or invalid.\n", sigma, i, varIdx);
    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;

    // compute inverse variance
    _actionSigmas[i] = sigma;
    _inverseVariances[i] = 1.0 / (sigma * sigma);
  }
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<float>>();

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  /*****************************************************************************
  * During training, we follow the Epsilon-greedy strategy. Choose, given a
  * probability (pEpsilon), one from the following:
  *  - Uniformly random action among all possible actions
  *  - Sample action guided by the policy's probability distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training")
  {
    // Getting current probability of random action for the agent
    float pRandom = sample["Random Action Probability"];

    // Getting pGreedy = U[0,1] for the epsilon-greedy strategy
    float pEpsilon = _uniformGenerator->getRandomNumber();

    // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
    if (pEpsilon < pRandom)
    {
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      {
        // Producing random (uniform) number to select within an lower/upper bound range
        float x = _normalGenerator->getRandomNumber();

        float mean = 0.5 * (_actionLowerBounds[i] + _actionUpperBounds[i]);
        float sigma = 0.3 * (_actionUpperBounds[i] - _actionLowerBounds[i]);

        // Producing the random value for action element i
        action[i] = mean + x * sigma;

        // Storing selection metadata
        sample["Metadata"]["Action Means"][i] = mean;
        sample["Metadata"]["Action Sigmas"][i] = sigma;
      }
    }
    else // else we select guided by the policy's probability distribution
    {
      // Update the Gaussian means and sigmas of the actions given by the agent's policy
      forwardPolicy(state);

      // Generating action from the normal distribution
      for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
        action[i] = _actionMeans[i] + _actionSigmas[i] * _normalGenerator->getRandomNumber();

      sample["Metadata"]["Action Means"] = _actionMeans;
      sample["Metadata"]["Action Sigmas"] = _actionSigmas;
    }
  }

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing")
  {
    forwardPolicy(state);
    action = _actionMeans;
  }

  // Clipping actions
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
   if (action[i] > _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
   if (action[i] < _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
  }

  /*****************************************************************************
  * Storing the action itself
  ****************************************************************************/

  sample["Action"] = action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const std::vector<float> &curMeans, const std::vector<float> &curSigmas, const std::vector<float> &oldMeans, const std::vector<float> &oldSigmas)
{
  // Getting probability densities for current action given current policy
  float logpCurPolicy = 0.0f;
  for (size_t i = 0; i < action.size(); i++)
  {
    _normalGenerator->_mean = curMeans[i];
    _normalGenerator->_standardDeviation = curSigmas[i];
    _normalGenerator->updateDistribution();
    logpCurPolicy += _normalGenerator->getLogDensity(action[i]);
  }

  // Getting probability densities for current action given current policy
  float logpOldPolicy = 0.0f;
  for (size_t i = 0; i < action.size(); i++)
  {
    _normalGenerator->_mean = oldMeans[i];
    _normalGenerator->_standardDeviation = oldSigmas[i];
    _normalGenerator->updateDistribution();
    logpOldPolicy += _normalGenerator->getLogDensity(action[i]);
  }

  // Now calculating importance weight for the old s,a experience
  float importanceWeight = std::exp(logpCurPolicy - logpOldPolicy);

  return importanceWeight;
}

} // namespace agent

} // namespace solver

} // namespace korali

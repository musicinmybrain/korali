#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"
#include <gsl/gsl_multifit.h>

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  // Obtaining action shift and scales for bounded distributions
  _actionShifts.resize(_problem->_actionVectorSize);
  _actionScales.resize(_problem->_actionVectorSize);
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    // For bounded distributions, infinite bounds should result in an error message
    if (_policyDistribution == "Squashed Normal" || _policyDistribution == "Beta")
    {
      if (isfinite(_actionLowerBounds[i]) == false)
        KORALI_LOG_ERROR("Provided lower bound (%f) for action variable %lu is non-finite, but the distribution (%s) is bounded.\n", _actionLowerBounds[i], i, _policyDistribution.c_str());

      if (isfinite(_actionUpperBounds[i]) == false)
        KORALI_LOG_ERROR("Provided upper bound (%f) for action variable %lu is non-finite, but the distribution (%s) is bounded.\n", _actionUpperBounds[i], i, _policyDistribution.c_str());

      _actionShifts[i] = (_actionUpperBounds[i] + _actionLowerBounds[i]) * 0.5f;
      _actionScales[i] = (_actionUpperBounds[i] - _actionLowerBounds[i]) * 0.5f;
    }
  }

  // Obtaining policy parameter transformations (depends on which policy distribution chosen)
  if (_policyDistribution == "Normal" || _policyDistribution == "Squashed Normal")
  {
    _policyParameterCount = 2 * _problem->_actionVectorSize; // Means and Sigmas

    // Allocating space for the required transformations
    _policyParameterTransformationMasks.resize(_policyParameterCount);
    _policyParameterScaling.resize(_policyParameterCount);
    _policyParameterShifting.resize(_policyParameterCount);

    // Establishing transformations for the Normal policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      auto varIdx = _problem->_actionVectorIndexes[i];
      float sigma = _k->_variables[varIdx]->_initialExplorationNoise;

      // Checking correct noise configuration
      if (sigma <= 0.0f) KORALI_LOG_ERROR("Provided initial noise (%f) for action variable %lu is not defined or negative.\n", sigma, varIdx);

      // Identity mask for Means
      _policyParameterScaling[i] = 1.0f;
      _policyParameterShifting[i] = 0.0f;
      _policyParameterTransformationMasks[i] = "Identity";

      // Softplus mask for Sigmas
      _policyParameterTransformationMasks[_problem->_actionVectorSize + i] = "Softplus";
      _policyParameterScaling[_problem->_actionVectorSize + i] = sigma * 2.0f;
      _policyParameterShifting[_problem->_actionVectorSize + i] = 0.0f;
    }
  }

  if (_policyDistribution == "Beta")
  {
    _policyParameterCount = 2 * _problem->_actionVectorSize; // Mu and Variance

    // Allocating space for the required transformations
    _policyParameterTransformationMasks.resize(_policyParameterCount);
    _policyParameterScaling.resize(_policyParameterCount);
    _policyParameterShifting.resize(_policyParameterCount);

    // Establishing transformations for the Normal policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      auto varIdx = _problem->_actionVectorIndexes[i];
      float variance = _k->_variables[varIdx]->_initialExplorationNoise;

      // Checking correct noise configuration
      if (variance <= 0.0f) KORALI_LOG_ERROR("Provided initial noise (%f) for action variable %lu is not defined or negative.\n", variance, varIdx);

      // Identity mask for Means
      _policyParameterScaling[i] = 1.0f;
      _policyParameterShifting[i] = 0.0f;
      _policyParameterTransformationMasks[i] = "Identity";

      // Sigmoid Mask for Variance
      _policyParameterTransformationMasks[_problem->_actionVectorSize + i] = "Sigmoid";
      _policyParameterScaling[_problem->_actionVectorSize + i] = variance * 2.0f;
      _policyParameterShifting[_problem->_actionVectorSize + i] = 0.0f;
    }
  }

  // Allocate memory for linear controller
  _observationsApproximatorWeights.resize(_problem->_actionVectorSize, std::vector<float>(_problem->_stateVectorSize + 1));
  _observationsApproximatorSigmas.resize(_problem->_actionVectorSize);

  // Building linear controller for observed state action pairs
  gsl_matrix *X = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);
  gsl_matrix *Y = gsl_matrix_alloc(_problem->_totalObservedStateActionPairs, _problem->_actionVectorSize);
  size_t idx = 0;
  for (size_t i = 0; i < _problem->_numberObservedTrajectories; ++i)
  {
    size_t trajectoryLength = _problem->_observationsStates[i].size();
    for (size_t t = 0; t < trajectoryLength; ++t)
    {
      gsl_matrix_set(X, idx, 0, 1.0); // intercept
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
      {
        gsl_matrix_set(X, idx, j + 1, (double)_problem->_observationsStates[i][t][j]);
      }
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        gsl_matrix_set(Y, idx, k, (double)_problem->_observationsActions[i][t][k]);
      }
      idx++;
    }
  }

  _k->_logger->logInfo("Normal", "Linear Approximator Expert Policy\n");

  // Do regression over actions
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    double chisq;
    gsl_vector *c = gsl_vector_alloc(_problem->_stateVectorSize + 1);
    gsl_matrix *cov = gsl_matrix_alloc(_problem->_stateVectorSize + 1, _problem->_stateVectorSize + 1);
    gsl_multifit_linear_workspace *work = gsl_multifit_linear_alloc(_problem->_totalObservedStateActionPairs, _problem->_stateVectorSize + 1);

    // predict action Y_k
    gsl_vector_view y = gsl_matrix_column(Y, k);
    gsl_multifit_linear(X, &y.vector, c, cov, &chisq, work);

    for (size_t j = 0; j < _problem->_stateVectorSize + 1; ++j)
    {
      _observationsApproximatorWeights[k][j] = gsl_vector_get(c, j);
      _k->_logger->logInfo("Normal", "    + Weights  [%zu, %zu] %f \n", k, j, _observationsApproximatorWeights[k][j]);
    }

    gsl_multifit_linear_free(work);
    gsl_vector_free(c);
    gsl_matrix_free(cov);
  }

  // Calculate squared error over all predictions
  std::vector<float> squaredErrors(_problem->_actionVectorSize);
  for (size_t t = 0; t < _problem->_numberObservedTrajectories; ++t)
    for (size_t i = 0; i < _problem->_observationsStates[t].size(); ++i)
    {
      for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
      {
        float approx = _observationsApproximatorWeights[k][0]; // intercept
        for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        {
          approx += _problem->_observationsStates[t][i][j] * _observationsApproximatorWeights[k][j + 1];
        }
        squaredErrors[k] += std::pow(_problem->_observationsActions[t][i][k] - approx, 2.);
      }
    }

  // Set MLE sigma estimates
  for (size_t k = 0; k < _problem->_actionVectorSize; ++k)
  {
    _observationsApproximatorSigmas[k] = std::sqrt(squaredErrors[k] / (float)_problem->_totalObservedStateActionPairs);
    _k->_logger->logInfo("Normal", "    + Sigma    [%zu] %f \n", k, _observationsApproximatorSigmas[k]);
  }

  gsl_matrix_free(X);
  gsl_matrix_free(Y);
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"];

  // Adding state to the state time sequence
  _stateTimeSequence.add(state);

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward state sequence to get the Gaussian means and sigmas from policy
  auto policy = runPolicy({_stateTimeSequence.getVector()})[0];

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateTrainingAction(policy);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing") action = generateTestingAction(policy);

  /*****************************************************************************
  * Storing the action and its policy
  ****************************************************************************/

  sample["Policy"]["Distribution Parameters"] = policy.distributionParameters;
  sample["Policy"]["State Value"] = policy.stateValue;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateTrainingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Creating the action based on the selected policy
  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mean = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      action[i] = mean + sigma * _normalGenerator->getRandomNumber();
    }
  }

  if (_policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mean = curPolicy.distributionParameters[i];
      const float sigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];
      const float unboundedAction = mean + sigma * _normalGenerator->getRandomNumber();

      action[i] = (std::tanh(unboundedAction) * scale) + shift;
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];
      action[i] = ranBetaAlt(_normalGenerator->_range, curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
    }
  }

  return action;
}

std::vector<float> Continuous::generateTestingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  if (_policyDistribution == "Normal")
  {
    // Take only the means without noise
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      action[i] = curPolicy.distributionParameters[i];
  }

  if (_policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float mean = curPolicy.distributionParameters[i];
      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];
      action[i] = (std::tanh(mean) * scale) + shift;
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float alpha, beta;
      std::tie(alpha, beta) = betaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      if (alpha > 1.0f && beta > 1.0f)
        action[i] = _actionLowerBounds[i] + (_actionUpperBounds[i] - _actionLowerBounds[i]) * (alpha - 1.0f) / (alpha + beta - 2.0f);
      else if (alpha <= 1.0f && beta > 1.0f)
        action[i] = _actionLowerBounds[i];
      else if (alpha > 1.0f && beta <= 1.0f)
        action[i] = _actionUpperBounds[i];
      else
        KORALI_LOG_ERROR("Case (a,b <=1) not yet treated (a,b = %f,%f)", alpha, beta);
    }
  }

  return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  float logpOldPolicy = 0.0f;
  float logpCurPolicy = 0.0f;

  // Storage for importance weight value
  float importanceWeight = 0.0f;

  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      logpOldPolicy += normalLogDensity(action[i], oldMean, oldSigma);
      logpCurPolicy += normalLogDensity(action[i], curMean, curSigma);
    }
  }

  if (_policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Get unbounded action
      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];
      const float unboundedAction = std::atanh((action[i] - shift) / scale);

      // Importance weight of squashed normal is the importance weight of normal evaluated at unbounded action
      logpOldPolicy += normalLogDensity(unboundedAction, oldMean, oldSigma);
      logpCurPolicy += normalLogDensity(unboundedAction, curMean, curSigma);
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < action.size(); i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      logpOldPolicy += betaLogDensityAlt(action[i], oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
      logpCurPolicy += betaLogDensityAlt(action[i], curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
    }
  }

  // Calculating log importance weight
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;
 
  if (std::isfinite(logImportanceWeight) == false)
  {
      for (size_t i = 0; i < action.size(); i++) printf("a %f\n", action[i]);
 
      for (size_t i = 0; i < action.size(); i++) printf("os %f\n", oldPolicy.distributionParameters[_problem->_actionVectorSize+i]);
      for (size_t i = 0; i < action.size(); i++) printf("cs %f\n", curPolicy.distributionParameters[_problem->_actionVectorSize+i]);

      KORALI_LOG_ERROR("Calculated value for logImportanceWeight returned an invalid value: %f %f %f\n", logImportanceWeight, logpOldPolicy, logpCurPolicy);

  }

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.0f) logImportanceWeight = +7.0f;
  if (logImportanceWeight < -7.0f) logImportanceWeight = -7.0f;

  // Calculating importance weight by exp
  importanceWeight = std::exp(logImportanceWeight);
 
  if (std::isfinite(importanceWeight) == false)
    KORALI_LOG_ERROR("Calculated value for importanceWeight returned an invalid value: %f\n", importanceWeight);

  return importanceWeight;
}

std::vector<float> Continuous::calculateImportanceWeightGradient(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  // Storage for importance weight gradients
  std::vector<float> importanceWeightGradients(2 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
    float logpOldPolicy = 0.0f;
    float logpCurPolicy = 0.0f;

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      // Calculate importance weight
      logpOldPolicy += normalLogDensity(action[i], oldMean, oldSigma);
      logpCurPolicy += normalLogDensity(action[i], curMean, curSigma);

      // Deviation from expAction and current Mean
      float curActionDiff = (action[i] - curMean);

      // Inverse Variances
      float curInvVar = 1. / (curSigma * curSigma);

      // Gradient with respect to Mean
      importanceWeightGradients[i] = curActionDiff * curInvVar;

      // Gradient with respect to Sigma
      importanceWeightGradients[_problem->_actionVectorSize + i] = (curActionDiff * curActionDiff) * (curInvVar / curSigma) - 1.0f / curSigma;
    }

    float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    float importanceWeight = std::exp(logImportanceWeight);

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
      importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Squashed Normal")
  {
    float logpOldPolicy = 0.0f;
    float logpCurPolicy = 0.0f;

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      const float scale = _actionScales[i];
      const float shift = _actionShifts[i];
      const float unboundedAction = std::atanh((action[i] - shift) / scale);

      // Importance weight of squashed normal is the importance weight of normal evaluated at unbounded action
      logpOldPolicy += normalLogDensity(unboundedAction, oldMean, oldSigma);
      logpCurPolicy += normalLogDensity(unboundedAction, curMean, curSigma);

      // Deviation from expAction and current Mean
      float curActionDiff = (unboundedAction - curMean);

      // Inverse Variances
      float curInvVar = 1. / (curSigma * curSigma);

      // Gradient with respect to Mean
      importanceWeightGradients[i] = curActionDiff * curInvVar;

      // Gradient with respect to Sigma
      importanceWeightGradients[_problem->_actionVectorSize + i] = (curActionDiff * curActionDiff) * (curInvVar / curSigma) - 1.0f / curSigma;
    }

    float logImportanceWeight = logpCurPolicy - logpOldPolicy;
    float importanceWeight = std::exp(logImportanceWeight);

    // Scale by importance weight to get gradient
    for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
      importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Log probability of action with old policy params
      const float logpOldPolicy = betaLogDensityAlt(action[i], oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);
      const float invpOldPolicy = std::exp(-logpOldPolicy);

      // Variable preparation
      const float Bab = gsl_sf_beta(alphaCur, betaCur);

      const float psiAb = gsl_sf_psi(alphaCur + betaCur);

      const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];
      const float logscale = std::log(actionRange);
      const float powscale = std::pow(actionRange, -betaCur - alphaCur + 1.);
      const float factor = -1. * std::pow(action[i] - _actionLowerBounds[i], alphaCur - 1.) * powscale * std::pow(_actionUpperBounds[i] - action[i], betaCur - 1.) * invpOldPolicy / Bab;

      // Rho Grad wrt alpha and beta
      const float daBab = gsl_sf_psi(alphaCur) - psiAb;
      const float drhoda = ((logscale - std::log(action[i] - _actionLowerBounds[i])) + daBab) * factor;
      const float dbBab = gsl_sf_psi(betaCur) - psiAb;
      const float drhodb = (logscale - std::log(_actionUpperBounds[i] - action[i]) + dbBab) * factor;

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // Rho Grad wrt mu and varc
      importanceWeightGradients[i] = drhoda * dadmu + drhodb * dbdmu;
      importanceWeightGradients[_problem->_actionVectorSize + i] = drhoda * dadvarc + drhodb * dbdvarc;
    }
  }

  return importanceWeightGradients;
}

std::vector<float> Continuous::calculateKLDivergenceGradient(const policy_t &oldPolicy, const policy_t &curPolicy)
{
  // Storage for KL Divergence Gradients
  std::vector<float> KLDivergenceGradients(2.0 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal" || _policyDistribution == "Squashed Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      const float oldMean = oldPolicy.distributionParameters[i];
      const float oldSigma = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMean = curPolicy.distributionParameters[i];
      const float curSigma = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float curInvVar = 1. / (curSigma * curSigma);
      float actionDiff = (curMean - oldMean);

      // KL-Gradient with respect to Mean
      KLDivergenceGradients[i] = actionDiff * curInvVar;

      // Contribution to Sigma from Trace
      float gradTr = -(curInvVar / curSigma) * oldSigma * oldSigma;

      // Contribution to Sigma from Quadratic term
      float gradQuad = -(actionDiff * actionDiff) * (curInvVar / curSigma);

      // Contribution to Sigma from Determinant
      float gradDet = 1.0f / curSigma;

      // KL-Gradient with respect to Sigma
      KLDivergenceGradients[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
    }
  }

  if (_policyDistribution == "Beta")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      // Getting parameters from the new and old policies
      const float oldMu = oldPolicy.distributionParameters[i];
      const float oldVariance = oldPolicy.distributionParameters[_problem->_actionVectorSize + i];
      const float curMu = curPolicy.distributionParameters[i];
      const float curVariance = curPolicy.distributionParameters[_problem->_actionVectorSize + i];

      float alphaCur;
      float betaCur;
      std::tie(alphaCur, betaCur) = betaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      float alphaOld;
      float betaOld;
      std::tie(alphaOld, betaOld) = betaParamTransformAlt(oldMu, oldVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      const float psiAbCur = gsl_sf_psi(alphaCur + betaCur);
      const float psiAbOld = gsl_sf_psi(alphaOld + betaOld);

      const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];

      // KL Grad wrt alpha
      const float dklda = (gsl_sf_psi(alphaCur) - psiAbCur - gsl_sf_psi(alphaOld) - psiAbOld) / actionRange;

      // KL Grad wrt beta
      const float dkldb = (gsl_sf_psi(betaCur) - psiAbCur - gsl_sf_psi(betaOld) - psiAbOld) / actionRange;

      // Derivatives of alpha and beta wrt mu and varc
      float dadmu, dadvarc, dbdmu, dbdvarc;
      std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(curMu, curVariance, _actionLowerBounds[i], _actionUpperBounds[i]);

      // KL Grad wrt mu and varc
      KLDivergenceGradients[i] = dklda * dadmu + dkldb * dbdmu;
      KLDivergenceGradients[_problem->_actionVectorSize + i] = dklda * dadvarc + dkldb * dbdvarc;
    }
  }

  return KLDivergenceGradients;
}

float Continuous::evaluateTrajectoryLogProbability(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions, const std::vector<float> &policyHyperparameter)
{
  knlohmann::json policy;
  policy["Policy"] = policyHyperparameter;
  setAgentPolicy(policy);

  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    auto evaluation = runPolicy({{states[t]}})[0];
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation.distributionParameters[d], evaluation.distributionParameters[_problem->_actionVectorSize + d]);
  }

  return trajectoryLogProbability;
}

float Continuous::evaluateTrajectoryLogProbabilityWithObservedPolicy(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions)
{
  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    std::vector<float> evaluation(_problem->_actionVectorSize);
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
    {
      // Predict action with linear policy
      evaluation[d] = _observationsApproximatorWeights[d][0];
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        evaluation[d] += states[t][j] * _observationsApproximatorWeights[d][j + 1];
    }
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation[d], _observationsApproximatorSigmas[d]);
  }

  return trajectoryLogProbability;
}

} // namespace agent

} // namespace solver

} // namespace korali

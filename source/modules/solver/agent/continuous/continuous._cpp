#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  /*********************************************************************
  * Initializing Action Noise Sigmas
  *********************************************************************/

  // Allocating space for the inverse variance calculation
  _actionScalings.resize(_problem->_actionVectorSize);
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    float sigma = _k->_variables[varIdx]->_explorationSigma;
    float lowerBound = _k->_variables[varIdx]->_lowerBound;
    float upperBound = _k->_variables[varIdx]->_upperBound;
    if (sigma <= 0.0) KORALI_LOG_ERROR("Value of Exploration Sigma (%f) for action %lu (variable %lu) is not defined or invalid.\n", sigma, i, varIdx);
    if (upperBound - lowerBound <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", upperBound, lowerBound, i);

    // Obtaining lower and upper bounds for the action
    _actionLowerBounds[i] = lowerBound;
    _actionUpperBounds[i] = upperBound;
    _actionScalings[i] = upperBound - lowerBound;
  }
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<float>>();

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward Policy to get the Gaussian means and sigmas from policy
  std::vector<float> curMeans;
  std::vector<float> curSigmas;
  std::tie(curMeans, curSigmas) = getPolicyDistribution(state);

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateAction(curMeans, curSigmas);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing")  action = curMeans;

  /*****************************************************************************
  * Storing the action itself and associated metadata
  ****************************************************************************/

  // Clipping actions TODO: Use truncated Normal or Beta distribution!!! (PW)
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
   if (action[i] > _actionUpperBounds[i]) action[i] = _actionUpperBounds[i];
   if (action[i] < _actionLowerBounds[i]) action[i] = _actionLowerBounds[i];
  }

  sample["Metadata"]["Action Means"] = curMeans;
  sample["Metadata"]["Action Sigmas"] = curSigmas;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateAction(const std::vector<float> &curMeans, const std::vector<float> &curSigmas)
{
 std::vector<float> action(_problem->_actionVectorSize);
 for (size_t i = 0; i < _problem->_actionVectorSize; i++)
 {
   // Updating normal distribution for the current action element
   _normalGenerator->_mean = curMeans[i];
   _normalGenerator->_standardDeviation = curSigmas[i];

   // Generating action from the updated normal distribution
   action[i] = _normalGenerator->getRandomNumber();
 }

 return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const std::vector<float> &curMeans, const std::vector<float> &curSigmas, const std::vector<float> &oldMeans, const std::vector<float> &oldSigmas)
{
  // Getting probability densities for current action given current policy
  float logpCurPolicy = 0.0f;
  for (size_t i = 0; i < action.size(); i++)
  {
    _normalGenerator->_mean = curMeans[i];
    _normalGenerator->_standardDeviation = curSigmas[i];
    _normalGenerator->updateDistribution();
    logpCurPolicy += _normalGenerator->getLogDensity(action[i]);
  }

  // Getting probability densities for current action given current policy
  float logpOldPolicy = 0.0f;
  for (size_t i = 0; i < action.size(); i++)
  {
    _normalGenerator->_mean = oldMeans[i];
    _normalGenerator->_standardDeviation = oldSigmas[i];
    _normalGenerator->updateDistribution();
    logpOldPolicy += _normalGenerator->getLogDensity(action[i]);
  }

  // Now calculating importance weight for the old s,a experience
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;
  float importanceWeight = std::exp( logImportanceWeight > 7 ? 7 : ( logImportanceWeight < -7 ? -7 : logImportanceWeight ) );

  return importanceWeight;
}

} // namespace agent

} // namespace solver

} // namespace korali

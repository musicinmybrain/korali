#include "engine.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
void Continuous::initializeAgent()
{
  // Getting continuous problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  // Allocating and obtaining action bounds information
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    _actionLowerBounds[i] = _k->_variables[varIdx]->_lowerBound;
    _actionUpperBounds[i] = _k->_variables[varIdx]->_upperBound;

    if (_actionUpperBounds[i] - _actionLowerBounds[i] <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", _actionUpperBounds[i], _actionLowerBounds[i], i);
  }

  // Obtaining policy parameter transformations (depends on which policy distribution chosen)
  if (_policyDistribution == "Normal")
  {
    _policyParameterCount = 2 * _problem->_actionVectorSize; // Means and Sigmas

    // Allocating space for the required transformations
    _policyParameterTransformationMasks.resize(_policyParameterCount);
    _policyParameterScaling.resize(_policyParameterCount);
    _policyParameterShifting.resize(_policyParameterCount);

    // Establishing transformations for the unbounded normal policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      auto varIdx = _problem->_actionVectorIndexes[i];
      float sigma = _k->_variables[varIdx]->_initialExplorationNoise;

      // Checking correct noise configuration
      if (sigma <= 0.0f) KORALI_LOG_ERROR("Provided initial noise (%f) for action variable %lu is not defined or negative.\n", sigma, varIdx);

      // Identity mask for Means
      _policyParameterScaling[i] = 1.0f;
      _policyParameterShifting[i] = 0.0f;
      _policyParameterTransformationMasks[i] = "Identity";

      // Softplus mask for Sigmas
      _policyParameterTransformationMasks[_problem->_actionVectorSize + i] = "Softplus";
      _policyParameterScaling[_problem->_actionVectorSize + i] = sigma * 2.0f;
      _policyParameterShifting[_problem->_actionVectorSize + i] = 0.0f;
    }
  }
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"];

  // Adding state to the state time sequence
  _stateTimeSequence.add(state);

  // Storage for the action to select
  std::vector<float> action(_problem->_actionVectorSize);

  // Forward state sequence to get the Gaussian means and sigmas from policy
  auto policy = runPolicy({_stateTimeSequence.getVector()})[0];

  /*****************************************************************************
  * During Training we select action according to policy's probability 
  * distribution
  ****************************************************************************/

  if (sample["Mode"] == "Training") action = generateTrainingAction(policy);

  /*****************************************************************************
  * During testing, we select the means (point of highest density) for all
  * elements of the action vector
  ****************************************************************************/

  if (sample["Mode"] == "Testing") action = generateTestingAction(policy);

  /*****************************************************************************
  * Storing the action and its policy
  ****************************************************************************/

  sample["Policy"]["Action Means"] = policy.actionMeans;
  sample["Policy"]["Action Sigmas"] = policy.actionSigmas;
  sample["Policy"]["State Value"] = policy.stateValue;
  sample["Action"] = action;
}

std::vector<float> Continuous::generateTrainingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Getting parameters from the new and old policies
  const auto &curMeans = curPolicy.actionMeans;
  const auto &curSigmas = curPolicy.actionSigmas;

  // Creating the action based on the selected policy
  if (_policyDistribution == "Normal")
  {
   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
     action[i] = curMeans[i] + curSigmas[i] * _normalGenerator->getRandomNumber();
  }

  if (_policyDistribution == "Squashed Normal")
  {
    KORALI_LOG_ERROR("Not yet implemented.\n");
  }

  if (_policyDistribution == "Beta")
  {
   const auto &actionAlphas = curPolicy.actionMeans;
   const auto &actionBetas = curPolicy.actionSigmas;

   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    action[i] = ranBetaAlt(_normalGenerator->_range, actionAlphas[i], actionBetas[i], _actionLowerBounds[i], _actionUpperBounds[i]);
  }

  return action;
}

std::vector<float> Continuous::generateTestingAction(const policy_t &curPolicy)
{
  std::vector<float> action(_problem->_actionVectorSize);

  // Getting parameters from the new and old policies
  const auto &curMeans = curPolicy.actionMeans;

  if (_policyDistribution == "Normal")
  {
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
     action[i] = curMeans[i];
  }

  if (_policyDistribution == "Squashed Normal")
  {
    KORALI_LOG_ERROR("Not yet implemented.\n");
  }

  if (_policyDistribution == "Beta")
  {
   const auto &actionAlphas = curPolicy.actionMeans;
   const auto &actionBetas = curPolicy.actionSigmas;

   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   {
    float alpha;
    float beta;

    std::tie(alpha, beta) = betaParamTransformAlt(actionAlphas[i], actionBetas[i], _actionLowerBounds[i], _actionUpperBounds[i]);

    if (actionAlphas[i] > 1.0f && actionBetas[i] > 1.0f)
      action[i] = _actionLowerBounds[i] + (_actionUpperBounds[i] - _actionLowerBounds[i]) * (actionAlphas[i] - 1.0f) / (actionAlphas[i] + actionBetas[i] - 2.0f);
    else if (actionAlphas[i] <= 1.0f && actionBetas[i] > 1.0f)
      action[i] = _actionLowerBounds[i];
    else if (actionAlphas[i] > 1.0f && actionBetas[i] <= 1.0f)
      action[i] = _actionUpperBounds[i];
    else
      KORALI_LOG_ERROR("Case (a,b <=1) not yet treated (a,b = %f,%f)", actionAlphas[i], actionBetas[i]);
   }
  }

  return action;
}

float Continuous::calculateImportanceWeight(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  float logpCurPolicy = 0.0f;
  float logpOldPolicy = 0.0f;

  // Storage for importance weight value
  float importanceWeight = 0.0f;

  if (_policyDistribution == "Normal")
  {
    // Getting parameters from the new and old policies
    const auto &oldMeans = oldPolicy.actionMeans;
    const auto &oldSigmas = oldPolicy.actionSigmas;
    const auto &curMeans = curPolicy.actionMeans;
    const auto &curSigmas = curPolicy.actionSigmas;

    // ParamsOne are the Means, ParamsTwo are the Sigmas
    for (size_t i = 0; i < action.size(); i++)
    {
      logpOldPolicy += normalLogDensity(action[i], oldMeans[i], oldSigmas[i]);
      logpCurPolicy += normalLogDensity(action[i], curMeans[i], curSigmas[i]);
    }
  }

  if (_policyDistribution == "Squashed Normal")
  {
    KORALI_LOG_ERROR("Not yet implemented.\n");
  }

  if (_policyDistribution == "Beta")
  {
   const auto &curActionAlphas = curPolicy.actionMeans;
   const auto &curActionBetas = curPolicy.actionSigmas;
   const auto &oldActionAlphas = oldPolicy.actionMeans;
   const auto &oldActionBetas = oldPolicy.actionSigmas;

   for (size_t i = 0; i < action.size(); i++)
   {
    logpOldPolicy += betaLogDensityAlt(action[i], oldActionAlphas[i], oldActionBetas[i], _actionLowerBounds[i], _actionUpperBounds[i]);
    logpCurPolicy += betaLogDensityAlt(action[i], curActionAlphas[i], curActionBetas[i], _actionLowerBounds[i], _actionUpperBounds[i]);
   }
  }

  // Now calculating importance weight for the old s,a experience
  float logImportanceWeight = logpCurPolicy - logpOldPolicy;

  // Normalizing extreme values to prevent loss of precision
  if (logImportanceWeight > +7.0f) logImportanceWeight = +7.0f;
  if (logImportanceWeight < -7.0f) logImportanceWeight = -7.0f;

  // Calculating actual importance weight by exp
  importanceWeight = std::exp(logImportanceWeight);

  return importanceWeight;
}

std::vector<float> Continuous::calculateImportanceWeightGradient(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  // Storage for importance weight gradients
  std::vector<float> importanceWeightGradients(2 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
   // Getting parameters from the new and old policies
   const auto &oldMeans = oldPolicy.actionMeans;
   const auto &oldSigmas = oldPolicy.actionSigmas;
   const auto &curMeans = curPolicy.actionMeans;
   const auto &curSigmas = curPolicy.actionSigmas;

   float logImportanceWeight = 0.0;

   // ParamsOne are the Means, ParamsTwo are the Sigmas
   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   {
     // Deviation from expAction and current Mean
     float curActionDiff = (action[i] - curMeans[i]);

     // Deviation from expAction and old Mean
     float oldActionDiff = (action[i] - oldMeans[i]);

     // Inverse Variances
     float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
     float oldInvVar = 1. / (oldSigmas[i] * oldSigmas[i]);

     // Calc Imp Weight
     logImportanceWeight += std::log(oldSigmas[i] / curSigmas[i]);
     logImportanceWeight += 0.5 * (oldActionDiff * oldActionDiff * oldInvVar - curActionDiff * curActionDiff * curInvVar);

     // Gradient with respect to Mean
     importanceWeightGradients[i] = curActionDiff * curInvVar;

     // Gradient with respect to Sigma
     importanceWeightGradients[_problem->_actionVectorSize + i] = (curActionDiff * curActionDiff) * (curInvVar / curSigmas[i]) - 1.0f / curSigmas[i];
   }

   float importanceWeight = std::exp(logImportanceWeight);

   // Scale by importance weight to get gradient
   for (size_t i = 0; i < 2 * _problem->_actionVectorSize; i++)
     importanceWeightGradients[i] *= importanceWeight;
  }

  if (_policyDistribution == "Beta")
  {
   const auto &curActionAlphas = curPolicy.actionMeans;
   const auto &curActionBetas = curPolicy.actionSigmas;
   const auto &oldActionAlphas = oldPolicy.actionMeans;
   const auto &oldActionBetas = oldPolicy.actionSigmas;

   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   {
     // Variable preparation
     const float muCur = curActionAlphas[i];
     const float varcoefCur = curActionBetas[i];

     float alphaCur;
     float betaCur;
     std::tie(alphaCur, betaCur) = betaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

     const float muOld = oldActionAlphas[i];
     const float varcoefOld  = oldActionBetas[i];

     float alphaOld;
     float betaOld;
     std::tie(alphaOld, betaOld) = betaParamTransformAlt(muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);

     // Log probability of action with old policy params
     const float logpOldPolicy = betaLogDensityAlt(action[i], muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);
     const float invpOldPolicy = std::exp(-logpOldPolicy);

     // Variable preparation
     const float Bab = gsl_sf_beta(alphaCur, betaCur);

     const float psiAb = gsl_sf_psi(alphaCur + betaCur);

     const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];
     const float logscale = std::log(actionRange);
     const float powscale = std::pow(actionRange, -betaCur - alphaCur + 1.);
     const float factor = -1. * std::pow(action[i] - _actionLowerBounds[i], alphaCur - 1.) * powscale * std::pow(_actionUpperBounds[i] - action[i], betaCur - 1.) * invpOldPolicy / Bab;

     // Rho Grad wrt alpha and beta
     const float daBab = gsl_sf_psi(alphaCur) - psiAb;
     const float drhoda = ((logscale - std::log(action[i] - _actionLowerBounds[i])) + daBab) * factor;
     const float dbBab = gsl_sf_psi(betaCur) - psiAb;
     const float drhodb = (logscale - std::log(_actionUpperBounds[i] - action[i]) + dbBab) * factor;

     // Derivatives of alpha and beta wrt mu and varc
     float dadmu, dadvarc, dbdmu, dbdvarc;
     std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

     // Rho Grad wrt mu and varc
     importanceWeightGradients[i] = drhoda * dadmu + drhodb * dbdmu;
     importanceWeightGradients[_problem->_actionVectorSize + i] = drhoda * dadvarc + drhodb * dbdvarc;
   }
  }

  return importanceWeightGradients;
}

std::vector<float> Continuous::calculateKLDivergenceGradient(const policy_t &oldPolicy, const policy_t &curPolicy)
{
  // Storage for KL Divergence Gradients
  std::vector<float> KLDivergenceGradients(2.0 * _problem->_actionVectorSize, 0.0);

  if (_policyDistribution == "Normal")
  {
   // Getting parameters from the new and old policies
   const auto &oldMeans = oldPolicy.actionMeans;
   const auto &oldSigmas = oldPolicy.actionSigmas;
   const auto &curMeans = curPolicy.actionMeans;
   const auto &curSigmas = curPolicy.actionSigmas;

   for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
   {
     float curInvVar = 1. / (curSigmas[i] * curSigmas[i]);
     float actionDiff = (curMeans[i] - oldMeans[i]);

     // KL-Gradient with respect to Mean
     KLDivergenceGradients[i] = actionDiff * curInvVar;

     // Contribution to Sigma from Trace
     float gradTr = -(curInvVar / curSigmas[i]) * oldSigmas[i] * oldSigmas[i];

     // Contribution to Sigma from Quadratic term
     float gradQuad = -(actionDiff * actionDiff) * (curInvVar / curSigmas[i]);

     // Contribution to Sigma from Determinant
     float gradDet = 1.0f / curSigmas[i];

     // KL-Gradient with respect to Sigma
     KLDivergenceGradients[_problem->_actionVectorSize + i] = gradTr + gradQuad + gradDet;
   }
  }

  if (_policyDistribution == "Beta")
  {
   const auto &curActionAlphas = curPolicy.actionMeans;
   const auto &curActionBetas = curPolicy.actionSigmas;
   const auto &oldActionAlphas = oldPolicy.actionMeans;
   const auto &oldActionBetas = oldPolicy.actionSigmas;

   for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
   {
    // Variable preparation
    const float muCur = curActionAlphas[i];
    const float varcoefCur = curActionBetas[i];

    float alphaCur;
    float betaCur;
    std::tie(alphaCur, betaCur) = betaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

    const float muOld = oldActionAlphas[i];
    const float varcoefOld = oldActionBetas[i];

    float alphaOld;
    float betaOld;
    std::tie(alphaOld, betaOld) = betaParamTransformAlt(muOld, varcoefOld, _actionLowerBounds[i], _actionUpperBounds[i]);

    const float psiAbCur = gsl_sf_psi(alphaCur + betaCur);
    const float psiAbOld = gsl_sf_psi(alphaOld + betaOld);

    const float actionRange = _actionUpperBounds[i] - _actionLowerBounds[i];

    // KL Grad wrt alpha
    const float dklda = (gsl_sf_psi(alphaCur) - psiAbCur - gsl_sf_psi(alphaOld) - psiAbOld) / actionRange;

    // KL Grad wrt beta
    const float dkldb = (gsl_sf_psi(betaCur) - psiAbCur - gsl_sf_psi(betaOld) - psiAbOld) / actionRange;

    // Derivatives of alpha and beta wrt mu and varc
    float dadmu, dadvarc, dbdmu, dbdvarc;
    std::tie(dadmu, dadvarc, dbdmu, dbdvarc) = derivativesBetaParamTransformAlt(muCur, varcoefCur, _actionLowerBounds[i], _actionUpperBounds[i]);

    // KL Grad wrt mu and varc
    KLDivergenceGradients[i] = dklda * dadmu + dkldb * dbdmu;
    KLDivergenceGradients[_problem->_actionVectorSize + i] = dklda * dadvarc + dkldb * dbdvarc;
   }
  }

  return KLDivergenceGradients;
}

} // namespace agent

} // namespace solver

} // namespace korali

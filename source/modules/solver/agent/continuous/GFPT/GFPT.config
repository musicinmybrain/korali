{
 "Configuration Settings":
 [
  {
    "Name": [ "Policy", "Learning Rate Scale" ],
    "Type": "float",
    "Description": "Indicates the relation of the policy learning rate to the critic learning rate. This can be used to delay the learning of the policy."
  },
  {
    "Name": [ "Policy", "Optimization Candidates" ],
    "Type": "float",
    "Description": "The number of candidates evaluated per generation CMAES."
  },
  {
    "Name": [ "Policy", "Target Accuracy" ],
    "Type": "float",
    "Description": "The minimum difference between two global optimization steps to reach to update policy."
  },
  {
   "Name": [ "Critic", "Advantage Function Population" ],
   "Type": "size_t",
   "Description": "The number of samples (n) to sample the policy to compute avg_{a}A(s,a)"
  }
 ],
 
 "Results":
 [
 
 ],
 
  "Termination Criteria":
 [

 ],

 "Variables Configuration":
 [
  {
    "Name": [ "Exploration Sigma", "Initial" ],
    "Type": "float",
    "Description": "Initial standard deviation of the fixed exploration noise for the given action."
  },
  {
    "Name": [ "Exploration Sigma", "Final" ],
    "Type": "float",
    "Description": "Final standard deviation of the fixed exploration noise for the given action."
  },
  {
    "Name": [ "Exploration Sigma", "Annealing Rate" ],
    "Type": "float",
    "Description": "The factor (k) by which the exploration sigma (e) is multiplied after every policy update. The formula is e = e*(1-k)"
  }
 ],

 "Internal Settings":
 [
   {
    "Name": [ "Current Exploration Sigmas" ],
    "Type": "std::vector<float>",
    "Description": "The current exploration sigma (e) for action variables."
  }
 ],
 
 "Module Defaults":
 {
  "Experience Replay": { "REFER": { "Enabled": true } },
  "Policy": { "Learning Rate Scale": 1.0 }
 },
 
 "Variable Defaults":
 {
  "Exploration Sigma": 
  {
   "Initial": -1.0,
   "Final": -1.0,
   "Annealing Rate": 1e-5
  } 
 }
}

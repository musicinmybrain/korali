{
 "Configuration Settings":
 [
  {
    "Name": [ "Policy", "Learning Rate Scale" ],
    "Type": "float",
    "Description": "Indicates the relation of the policy learning rate to the critic learning rate. This can be used to delay the learning of the policy."
  },
  {
    "Name": [ "Policy", "Optimization Candidates" ],
    "Type": "float",
    "Description": "The number of candidates evaluated per generation CMAES."
  },
  {
    "Name": [ "Policy", "Target Accuracy" ],
    "Type": "float",
    "Description": "The minimum difference between two global optimization steps to reach to update policy."
  },
  {
   "Name": [ "Critic", "Advantage Function Population" ],
   "Type": "size_t",
   "Description": "The number of samples (n) to sample the policy to compute avg_{a}A(s,a)"
  }
 ],
 
 "Results":
 [
 
 ],
 
  "Termination Criteria":
 [

 ],

 "Variables Configuration":
 [
  {
    "Name": [ "Exploration Sigma" ],
    "Type": "float",
    "Description": "Standard deviation of the fixed exploration noise for the given action."
  }
 ],

 "Internal Settings":
 [
 
 ],
 
 "Module Defaults":
 {
  "Experience Replay": { "REFER": { "Enabled": true } },
  "Policy": { "Learning Rate Scale": 1.0 }
 },
 
 "Variable Defaults":
 {
  "Exploration Sigma": -1.0 
 }
}

{
 "Configuration Settings":
 [
   {
    "Name": [ "Critic", "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Critic", "Learning Rate" ],
    "Type": "float",
    "Description": "The learning rate for the critic NN."
  },
  {
    "Name": [ "Policy", "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Policy", "Learning Rate" ],
    "Type": "float",
    "Description": "The learning rate for the policy NN."
  },
  {
    "Name": [ "Refer", "Cutoff Scale" ],
    "Type": "float",
    "Description": "The parameters C from eq. (13)."
  },
  {
    "Name": [ "Refer", "Annealing Rate" ],
    "Type": "float",
    "Description": "The parameters A from eq. (13)."
  },
  {
    "Name": [ "Refer", "Target Off Policy Fraction" ],
    "Type": "float",
    "Description": "The targetet number of Off Policy samples in the replay memory. The Parameter D in eq. (12)"
  },
  {
    "Name": [ "Policy", "Target Accuracy" ],
    "Type": "float",
    "Description": "The minimum difference between two global optimization steps to reach to update policy."
  },
  {
   "Name": [ "Critic", "Advantage Function Population" ],
   "Type": "size_t",
   "Description": "The number of samples (n) to sample the policy to compute avg_{a}A(s,a)"
  }
 ],
 
 "Results":
 [
 
 ],
 
  "Termination Criteria":
 [

 ],

 "Variables Configuration":
 [
  {
    "Name": [ "Exploration Sigma" ],
    "Type": "float",
    "Description": "Standard deviation of the fixed exploration noise for the given action."
  }
 ],

 "Internal Settings":
 [
  {
   "Name": [ "Refer", "Beta" ],
   "Type": "float",
   "Description": "Penalisation coefficient for Off-poplicyness."
  },
  {
   "Name": [ "Refer", "Cutoff" ],
   "Type": "float",
   "Description": "Cutoff to dermine Off-Policyness of experinence."
  },
  {
   "Name": [ "Refer", "Off Policy Fraction" ],
   "Type": "float",
   "Description": "Estimate of the number of Off Policy Samples in the Replay Memory."
  },
  {
   "Name": [ "Refer", "Initial Learning Rate" ],
   "Type": "float",
   "Description": "Initial Learning Rate given by the user."
  },
  {
   "Name": [ "Refer", "Start Size" ],
   "Type": "float",
   "Description": "The number of experiences in memory before starting the adaptation of the beta parameter."
  }
 ],
 
 "Module Defaults":
 {
  "Refer":
  {
   "Start Size": 1e+10,
   "Cutoff Scale": 4.0,
   "Annealing Rate": 5e-7,
   "Target Off Policy Fraction": 0.1
  },
  
  "Policy": { "Target Accuracy": 0.0001 },
  "Critic": { "Advantage Function Population": 10 }
 },
 
 "Variable Defaults":
 {
  "Exploration Sigma": -1.0 
 }
}

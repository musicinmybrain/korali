#ifndef _KORALI_AGENT_CONTINUOUS_GFPT_HPP_
#define _KORALI_AGENT_CONTINUOUS_GFPT_HPP_

#include "modules/problem/reinforcementLearning/continuous/continuous.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fCMAES.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
class GFPT : public Continuous
{
  public:
  /**
   * @brief A neural network for the simultaneous evaluation of the advantage CMAES candidates (one per thread)
   */
  std::vector<NeuralNetwork *> _batchOptimizationNetworks;

  /**
   * @brief Pointer to training experiment's V(s) problem
   */
  problem::SupervisedLearning *_vpProblem;

  /**
  * @brief Pointer to training experiment's V(s) critic learner
  */
  learner::DeepSupervisor *_vpLearner;

  /**
   * @brief Korali experiment for the training of V(s)
   */
  korali::Experiment _vpExperiment;

  /**
  * @brief Pointer to training experiment's Q(s,x) problem
  */
  problem::SupervisedLearning *_qProblem;

  /**
  * @brief Pointer to training experiment's Q(s,x) learner
  */
  learner::DeepSupervisor *_qLearner;

  /**
   * @brief Korali experiment for the training of Q(s,x)
   */
  korali::Experiment _qExperiment;

  /**
   * @brief Korali optimizer for finding the optimal action as per the critic. One instance per parallel thread.
   */
  std::vector<korali::fCMAES *> _actionOptimizers;

  /**
   * @brief Returns the advantage value of the given state/action sequence
   * @param stateActionSequence State/action sequence to evaluate
   * @return The advantage value
   */
  float getActionAdvantage(const std::vector<std::vector<float>> &stateActionSequence);

  /**
   * @brief Returns the best action: that with highest advantage for the given state
   * @param state State to evaluate
   * @return The best action
   */
  std::vector<float> argmaxAction(const std::vector<float> &state);

  /**
   * @brief Returns the optimal action for a given experience, based on the evaluation of the advantage function
   * @param stateActionSequence Time sequence of state/actions
   * @param curMeans The current means of the policy to use as initial guess
   * @return The optimal action
   */
  knlohmann::json optimizeAction(const std::vector<std::vector<float>> stateActionSequence, const std::vector<float> &curMeans);

  knlohmann::json runPolicy(const std::vector<std::vector<float>> &stateSequence) override;
  float stateValueFunction(const std::vector<std::vector<float>> &stateSequence) override;
  knlohmann::json getAgentPolicy() override;
  void setAgentPolicy(const knlohmann::json &hyperparameters) override;
  void setTrainingState(const knlohmann::json &state) override;
  knlohmann::json getTrainingState() override;
  void trainPolicy() override;
  void printAgentInformation() override;
  void initializeAgent() override;
};

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

#endif // _KORALI_AGENT_CONTINUOUS_GFPT_HPP_

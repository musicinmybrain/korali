#ifndef _KORALI_AGENT_CONTINUOUS_GFPT_HPP_
#define _KORALI_AGENT_CONTINUOUS_GFPT_HPP_

#include "modules/problem/reinforcementLearning/continuous/continuous.hpp"
#include "modules/solver/agent/continuous/continuous.hpp"
#include "modules/solver/learner/deepSupervisor/optimizers/fCMAES.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
class GFPT : public Continuous
{
  public:
  /**
   * @brief A neural network for the simultaneous evaluation of CMAES candidates (one per thread)
   */
  std::vector<NeuralNetwork *> _batchOptimizationNetworks;

  /**
   * @brief Pointer to training experiment's V(s) problem
   */
  problem::SupervisedLearning *_vProblem;

  /**
  * @brief Pointer to training experiment's V(s) critic learner
  */
  learner::DeepSupervisor *_vLearner;

  /**
   * @brief Korali experiment for the training of V(s)
   */
  korali::Experiment _vExperiment;

  /**
  * @brief Pointer to training experiment's A(s,x) problem
  */
  problem::SupervisedLearning *_aProblem;

  /**
  * @brief Pointer to training experiment's A(s,x) learner
  */
  learner::DeepSupervisor *_aLearner;

  /**
   * @brief Korali experiment for the training of A(s,x)
   */
  korali::Experiment _aExperiment;

  /**
   * @brief Korali optimizer for finding the optimal action as per the critic. One instance per parallel thread.
   */
  std::vector<korali::fCMAES *> _actionOptimizers;

  /**
  * @brief Korali experiment for argmax_action(Q) based on a trained Q-Network
  */
  korali::Experiment _policyExperiment;

  /**
   * @brief Pointer to training the actor network
   */
  learner::DeepSupervisor *_policyLearner;

  /**
   * @brief Pointer to actor's experiment problem
   */
  problem::SupervisedLearning *_policyProblem;

  /**
  * @brief [Profiling] Measures the amount of time taken by critic updates
  */
  double _criticUpdateTime;

  /**
   * @brief [Profiling] Measures the amount of time taken by policy updates
   */
  double _policyUpdateTime;

  /**
   * @brief Calculates the state value function V(s,a)
   * @param state state to evaluate
   * @return The value of V(s)
   */
  float stateValueFunction(const std::vector<float> &state);

  /**
   * @brief Returns the advantage value of the given state/action
   * @param state State to evaluate
   * @param action Action to evaluate
   * @return The advantage value
   */
  float currentActionAdvantageFunction(const std::vector<float> &state, const std::vector<float> &action);

  /**
   * @brief Returns the average advantage value of all actions given the state
   * @param curMeans Means of the current policy
   * @param curSigmas Standard deviations of the current policy
   * @param state State to evaluate
   * @return The average advantage value
   */
  float averageActionAdvantageFunction(const std::vector<float> &curMeans, const std::vector<float> &curSigmas, const std::vector<float> &state);

  /**
   * @brief Returns the best action: that with highest advantage for the given state
   * @param state State to evaluate
   * @return The best action
   */
  std::vector<float> argmaxAction(const std::vector<float> &state);

  /**
   * @brief Returns the retrace value for the given experience
   * @param expId The id (position within the replay memory) of the experience
   * @return The qRet value of the experience
   */
  float retraceFunction(size_t expId);

  knlohmann::json runPolicy(const std::vector<float> &state) override;

  knlohmann::json getAgentPolicy() override;
  void setAgentPolicy(const knlohmann::json &hyperparameters) override;
  void setTrainingState(const knlohmann::json &state) override;
  knlohmann::json getTrainingState() override;
  void trainPolicy() override;
  void printAgentInformation() override;
  void initializeAgent() override;
};

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

#endif // _KORALI_AGENT_CONTINUOUS_GFPT_HPP_

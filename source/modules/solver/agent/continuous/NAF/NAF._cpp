#include "engine.hpp"
#include "modules/solver/agent/continuous/NAF/NAF.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
void NAF::initializeAgent()
{
  // Initializing common continuous agent configuration
  Continuous::initializeAgent();

  if (_covarianceScaling <= 0.0) KORALI_LOG_ERROR("Covariance scaling must be larger 0.0 (is %f).\n", _covarianceScaling);

  /*********************************************************************
 * Initializing Q(a,s) = A(a,s) + V(s)
 *********************************************************************/

  _qExperiment["Problem"]["Type"] = "Supervised Learning";
  _qExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _qExperiment["Solver"]["Optimizer"] = _optimizer;
  _qExperiment["Solver"]["Learning Rate"] = _learningRate;
  _qExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
  _qExperiment["Solver"]["Steps Per Generation"] = 1;
  _qExperiment["Solver"]["Batch Size"] = _miniBatchSize;
  _qExperiment["Solver"]["Time Sequence Length"] = _timeSequenceLength;

  // Configuring neural network
  _qExperiment["Solver"]["Neural Network"]["Engine"] = _neuralNetworkEngine;
  _qExperiment["Solver"]["Neural Network"]["Layers"][0]["Type"] = "Layer/Input";
  _qExperiment["Solver"]["Neural Network"]["Layers"][0]["Node Count"] = _problem->_stateVectorSize;

  // Adding user-defined hidden layers
  for (size_t i = 0; i < _neuralNetworkHiddenLayers.size(); i++) _qExperiment["Solver"]["Neural Network"]["Layers"][i+1] = _neuralNetworkHiddenLayers[i];

  // Adding a linear transformation to bring the node count to the desired count
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+1]["Type"] = "Layer/Linear";
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+1]["Node Count"] = 1 + 2 * _problem->_actionVectorSize;
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+1]["Weight Scaling"] = 0.1;

  // Finally adding the output layer
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Type"] = "Layer/Output";

  // No mask for the state value
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Scale"][0] = 1.0f;
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Shift"][0] = 0.0f;
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Tanh Mask"][0] = false;
  _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Softplus Mask"][0] = false;

  // Masks to normalize and rescaling output to fit the action's [lower, upper] bounds (and sigmas/P)
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    auto lowerBound = _k->_variables[varIdx]->_lowerBound;
    auto upperBound = _k->_variables[varIdx]->_upperBound;
    float scale = (upperBound - lowerBound) * 0.5;
    float shift = (upperBound + lowerBound) * 0.5;

    // Tanh mask for Mean
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Scale"][i + 1] = scale;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Shift"][i + 1] = shift;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Tanh Mask"][i + 1] = true;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Softplus Mask"][i + 1] = false;

    // Softplus mask for P
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Scale"][_problem->_actionVectorSize + i + 1] = 1.0f;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Shift"][_problem->_actionVectorSize + i + 1] = 0.0f;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Tanh Mask"][_problem->_actionVectorSize + i + 1] = false;
    _qExperiment["Solver"]["Neural Network"]["Layers"][_neuralNetworkHiddenLayers.size()+2]["Softplus Mask"][_problem->_actionVectorSize + i + 1] = true;
  }

  // Running initialization to verify that the configuration is correct
  _qExperiment.initialize();
  _qProblem = dynamic_cast<problem::SupervisedLearning *>(_qExperiment._problem);
  _qLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_qExperiment._solver);

  /*********************************************************************
 * Initializing Critic
 *********************************************************************/

  // Copy Settings
  _criticExperiment._js = _qExperiment._js;

  // Running initialization to verify that the configuration is correct
  _criticExperiment.initialize();

  // Getting learner pointers
  _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
  _criticLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Setting Initial Hyperparameters
  auto qHyperparameters = _qLearner->getTrainingHyperparameters();
  _qHatHyperparameter = qHyperparameters;
  _criticLearner->setInferenceHyperparameters(qHyperparameters);

  _averageActionSigmas.resize(_problem->_actionVectorSize);
}

void NAF::trainPolicy()
{
  /***********************************************************************************
   * Training Phase
   **********************************************************************************/

  // Calculating cumulative Q*, for statistical purposes
  _cumulativeQStar = 0.0;
  _cumulativeQStarSquared = 0.0;
  float cumulativeTdError = 0.0;
  float cumulativeTdErrorSquarred = 0.0;
  std::fill(_averageActionSigmas.begin(), _averageActionSigmas.end(), 0.0);

  // Creating minibatch for the critic update
  auto miniBatchIndexes = generateMiniBatch(_miniBatchSize);
  auto importanceWeights = calculateImportanceWeights(miniBatchIndexes);

  // Reevaluating minibatch with Qhat
#pragma omp parallel for schedule(dynamic, 1)
  for (size_t i = 0; i < _miniBatchSize; i++)
  {
    // Selecting a uniformly random selected, yet not repeated experience
    size_t expId = miniBatchIndexes[i];

    // Access experiences
    auto curState = _experienceReplay[expId].state;
    auto curAction = _experienceReplay[expId].action;
    float yHat = _experienceReplay[expId].reward;

    // Evaluate Q
    auto qEvaluation = _qLearner->getEvaluation({{curState}})[0][0];

    // yHat* = r -- If terminal state
    // yHat* = r + gamma*V(s) -- If not terminal state

    // If state is not terminal (next state is filled) then add Qnew to the Q value.
    if (_experienceReplay[expId].termination == e_nonTerminal)
    {
      // Getting experience's next state
      auto nextState = _experienceReplay[expId + 1].state;

      // Calculate target y
      float vHat = _criticLearner->getEvaluation({{nextState}})[0][0][0];
      yHat += _discountFactor * vHat;
    }

    float qval = quadraticAdvantageFunction(curAction, qEvaluation) + qEvaluation[0]; // A(s,a) + V(s)
    float diff = yHat - qval;

    cumulativeTdError += diff;
    cumulativeTdErrorSquarred += (diff * diff);

    // Update experience priority
    updateExperienceReplayPriority(expId, abs(diff));

    std::vector<float> policyGradient(1 + 2 * _problem->_actionVectorSize);

    // Fetch importance weight
    float weight = importanceWeights[i];

    // Calculating Gradients of Loss Li wrt. V(s)
    policyGradient[0] = weight * diff;

    // Calculating Gradients of Li wrt. mu(s) and P(s)
    for (size_t j = 0; j < curAction.size(); j++)
    {
      float diagP = qEvaluation[1 + curAction.size() + j] / (_actionScalings[j] * _actionScalings[j]);
      float dmu = curAction[j] - qEvaluation[1 + j];
      policyGradient[1 + j] = weight * diff * dmu * diagP;
      policyGradient[1 + curAction.size() + j] = -weight * 0.5 * diff * dmu * dmu / (_actionScalings[j] * _actionScalings[j]);
    }

    _qProblem->_inputData[0][i] = curState;
    _qProblem->_solutionData[0][i] = policyGradient;

    // Keeping statistics
    _cumulativeQStar += yHat;
    _cumulativeQStarSquared += yHat * yHat;
    for (size_t j = 0; j < curAction.size(); j++) _averageActionSigmas[j] += std::sqrt(qEvaluation[1 + curAction.size() + j]);
  }

  // Running one generation of the optimization method with the given mini-batch
  _qLearner->initialize();
  _qLearner->runGeneration();
  _qLearner->finalize();

  // Keeping statistics
  _averageQStar = _cumulativeQStar / (float)_miniBatchSize;
  _stdevQStar = std::sqrt(_cumulativeQStarSquared / (float)_miniBatchSize - _averageQStar * _averageQStar);
  _averageTDError = cumulativeTdError / (float)_miniBatchSize;
  _stdevTDError = std::sqrt(cumulativeTdErrorSquarred / (float)_miniBatchSize - _averageTDError * _averageTDError);
  for (size_t j = 0; j < _averageActionSigmas.size(); j++) _averageActionSigmas[j] /= (float)_miniBatchSize;

  /*********************************************************************
   * Updating hyperparameters and broadcasting them to the workers
   *********************************************************************/

  auto qHyperparam = _qLearner->getTrainingHyperparameters();

  // Update the target network
  for (size_t i = 0; i < qHyperparam.size(); ++i)
  {
    _qHatHyperparameter[i] = _targetLearningRate * qHyperparam[i] + (1. - _targetLearningRate) * _qHatHyperparameter[i];
  }
  _criticLearner->setInferenceHyperparameters(_qHatHyperparameter);
}

knlohmann::json NAF::getAgentPolicy()
{
  knlohmann::json hyperparameters;
  hyperparameters["Critic"] = _qLearner->getInferenceHyperparameters();
  return hyperparameters;
}

void NAF::setAgentPolicy(const knlohmann::json &hyperparameters)
{
  _qLearner->setInferenceHyperparameters(hyperparameters["Critic"].get<std::vector<float>>());
}

knlohmann::json NAF::runPolicy(const std::vector<float> &state)
{
  auto qNNoutput = _qLearner->getEvaluation({{state}})[0][0];

  // Transforming NN output to receive mu and diagonal of P
  std::vector<float> actionMeans(_problem->_actionVectorSize);
  std::vector<float> actionSigmas(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
  {
    actionMeans[i] = qNNoutput[1 + i];
    actionSigmas[i] = std::sqrt(_covarianceScaling) * _actionScalings[i] / qNNoutput[1 + _problem->_actionVectorSize + i];
  }

  if (_policyDistribution == "Beta")
  {
    // Transform to variance coefficient
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      float mux = (actionMeans[i] - _actionLowerBounds[i]) / _actionScalings[i];
      float sigx = actionSigmas[i] / _actionScalings[i];
      actionSigmas[i] = mux * (1.0 - mux) / sigx;
      if (actionSigmas[i] > 1.0) actionSigmas[i] = 1.0;
    }
  }

  knlohmann::json policy;
  policy["Action Means"] = actionMeans;
  policy["Action Sigmas"] = actionSigmas;
  return policy;
}

float NAF::quadraticAdvantageFunction(const std::vector<float> &action, const std::vector<float> &qEvaluation)
{
  float qval = 0;
  for (size_t i = 0; i < action.size(); ++i)
  {
    float dmu = (action[i] - qEvaluation[i + 1]);
    float diagP = qEvaluation[i + 1 + action.size()] / (_actionScalings[i] * _actionScalings[i]);
    qval += dmu * diagP * dmu;
  }
  return -0.5 * qval;
}

void NAF::setTrainingState(const knlohmann::json &state)
{
  _qLearner->setTrainingHyperparameters(state["Q"]["Training"]);
  _qLearner->setInferenceHyperparameters(state["Q"]["Inference"]);

  _criticLearner->setTrainingHyperparameters(state["Critic"]["Training"]);
  _criticLearner->setInferenceHyperparameters(state["Critic"]["Inference"]);
}

knlohmann::json NAF::getTrainingState()
{
  knlohmann::json state;
  state["Q"]["Training"] = _qLearner->getTrainingHyperparameters();
  state["Q"]["Inference"] = _qLearner->getInferenceHyperparameters();

  state["Critic"]["Training"] = _criticLearner->getTrainingHyperparameters();
  state["Critic"]["Inference"] = _criticLearner->getInferenceHyperparameters();
  return state;
}

void NAF::printAgentInformation()
{
  _k->_logger->logInfo("Normal", "Critic Information:\n");
  _k->_logger->logInfo("Normal", " + Average (Std) Q-Value in Mini-Batch:  %f (%f)\n", _averageQStar, _stdevQStar);
  _k->_logger->logInfo("Normal", " + Average (Std) TD-Error in Mini-Batch:  %f (%f)\n", _averageTDError, _stdevTDError);
  _k->_logger->logInfo("Normal", " + Average P values in Mini-Batch: \n");
  for (size_t i = 0; i < _averageActionSigmas.size(); ++i) _k->_logger->logInfo("Normal", " +  [ %f ]\n", _averageActionSigmas[i]);

  _qExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _qExperiment._solver->printGenerationAfter();
  _qExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

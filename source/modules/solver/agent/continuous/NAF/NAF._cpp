#include "engine.hpp"
#include "modules/solver/agent/continuous/NAF/NAF.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
void NAF::initializeAgent()
{
  // Initializing common continuous agent configuration
  Continuous::initializeAgent();

  if (_covarianceScaling <= 0.0) KORALI_LOG_ERROR("Covariance scaling must be larger 0.0 (is %f).\n", _covarianceScaling);

  /*********************************************************************
 * Initializing Q(a,s) = A(a,s) + V(s)
 *********************************************************************/

  _qExperiment["Problem"]["Type"] = "Supervised Learning";
  _qExperiment["Problem"]["Max Timesteps"] = _timeSequenceLength;
  _qExperiment["Problem"]["Training Batch Size"] = _miniBatchSize;
  _qExperiment["Problem"]["Inference Batch Size"] = 1;
  _qExperiment["Problem"]["Input"]["Size"] = _problem->_stateVectorSize;
  _qExperiment["Problem"]["Solution"]["Size"] = 1 + 2 * _problem->_actionVectorSize;

  _qExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _qExperiment["Solver"]["Optimizer"] = _optimizer;
  _qExperiment["Solver"]["Learning Rate"] = _learningRate;
  _qExperiment["Solver"]["Loss Function"] = "Direct Gradient";
  _qExperiment["Solver"]["Steps Per Generation"] = 1;
  _qExperiment["Solver"]["Neural Network"]["Engine"] = _neuralNetworkEngine;
  _qExperiment["Solver"]["Neural Network"]["Hidden Layers"] = _neuralNetworkHiddenLayers;

  // Masks to normalize and rescaling output to fit the action's [lower, upper] bounds (and sigmas/P)

  _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Scale"][0] = 1.0f;
  _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Shift"][0] = 0.0f;
  _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Tanh Mask"][0] = false;
  _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Softplus Mask"][0] = false;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    auto lowerBound = _k->_variables[varIdx]->_lowerBound;
    auto upperBound = _k->_variables[varIdx]->_upperBound;
    float scale = (upperBound - lowerBound) * 0.5;
    float shift = (upperBound + lowerBound) * 0.5;

    // Tanh mask for Mean
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Scale"][i + 1] = scale;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Shift"][i + 1] = shift;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Tanh Mask"][i + 1] = true;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Softplus Mask"][i + 1] = false;

    // Softplus mask for P
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Scale"][_problem->_actionVectorSize + i + 1] = 1.0f;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Shift"][_problem->_actionVectorSize + i + 1] = 0.0f;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Tanh Mask"][_problem->_actionVectorSize + i + 1] = false;
    _qExperiment["Solver"]["Neural Network"]["Output Layer"]["Softplus Mask"][_problem->_actionVectorSize + i + 1] = true;
  }

  // Running initialization to verify that the configuration is correct
  _qExperiment.initialize();
  _qProblem = dynamic_cast<problem::SupervisedLearning *>(_qExperiment._problem);
  _qLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_qExperiment._solver);

  /*********************************************************************
 * Initializing Critic
 *********************************************************************/

  // Copy Settings
  _criticExperiment._js = _qExperiment._js;

  // Running initialization to verify that the configuration is correct
  _criticExperiment.initialize();

  // Getting learner pointers
  _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
  _criticLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Setting Initial Hyperparameters
  auto qHyperparameters = _qLearner->getTrainingHyperparameters();
  _qHatHyperparameter = qHyperparameters;
  _criticLearner->setInferenceHyperparameters(qHyperparameters);

  _statisticsAverageActionSigmas.resize(_problem->_actionVectorSize);
}

float NAF::stateValueFunction(const std::vector<std::vector<float>> &state)
{
  // Calculating V(s_i)
  float v = _criticLearner->getEvaluation({state})[0][0];
  return v;
}

void NAF::trainPolicy()
{
  /***********************************************************************************
   * Training Phase
   **********************************************************************************/

  // [Statistics] Zeroing initial cumulative Q*
  float cumulativeQStar = 0.0;
  float cumulativeQStarSquared = 0.0;
  float cumulativeTdError = 0.0;
  float cumulativeTdErrorSquarred = 0.0;
  std::fill(_statisticsAverageActionSigmas.begin(), _statisticsAverageActionSigmas.end(), 0.0);

  // Creating minibatch for the critic update
  auto miniBatchIndexes = generateMiniBatch(_miniBatchSize);

  // Reevaluating minibatch with Qhat
#pragma omp parallel for schedule(dynamic, 1)
  for (size_t b = 0; b < _miniBatchSize; b++)
  {
    // Selecting a uniformly random selected, yet not repeated experience
    size_t curId = miniBatchIndexes[b];

    // Access experiences
    auto expStateSequence = getStateTimeSequence(curId);
    auto expAction = _experienceReplay[curId].action;
    auto expMeans = _experienceReplay[curId].policy["Action Means"].get<std::vector<float>>();
    auto expSigmas = _experienceReplay[curId].policy["Action Sigmas"].get<std::vector<float>>();

    // Forward the neural network for this state to get current means and sigmas and Value
    auto policy = runPolicy(expStateSequence);
    float V = policy["State Value"].get<float>();
    auto curMeans = policy["Action Means"].get<std::vector<float>>();
    auto curSigmas = policy["Action Sigmas"].get<std::vector<float>>();
    auto curDiagP = policy["Diag P"].get<std::vector<float>>();

    // Compute importance weight
    float importanceWeight = calculateImportanceWeight(expAction, curMeans, curSigmas, expMeans, expSigmas);

    // Update value and weight in cache
    _experienceReplay[curId].cache.set("State Value", V);
    _experienceReplay[curId].cache.set("Importance Weight", importanceWeight);

    // Getting current experience probability and calculating importance weight
    float probability = _experienceReplay[curId].metadata["Mini Batch"]["Probability"];
    float weight = std::pow((float)_experienceReplay.size() * probability, -_importanceWeightAnnealingRate);

    float yHat = _experienceReplay[curId].reward;

    // yHat* = r -- If terminal state
    // yHat* = r + gamma*V(s) -- If not terminal state

    // If state is not terminal (next state is filled) then add Qnew to the Q value.
    if (_experienceReplay[curId].termination == e_nonTerminal)
    {
      // Getting experience's next state
      auto nextState = _experienceReplay[curId + 1].state;

      // Calculate target y
      float vHat = _criticLearner->getEvaluation({{nextState}})[0][0];
      yHat += _discountFactor * vHat;
    }

    float qval = quadraticAdvantageFunction(expAction, curMeans, curDiagP) + V; // A(s,a) + V(s)
    float diff = yHat - qval;

    cumulativeTdError += diff;
    cumulativeTdErrorSquarred += (diff * diff);

    // Updating experience priority
    _experienceReplay[curId].metadata["Mini Batch"]["Priority"] = abs(diff);

    // Reserving storage for critic/policy gradient
    std::vector<float> policyGradient(1 + 2 * _problem->_actionVectorSize);

    // Calculating Gradients of Loss Li wrt. V(s)
    policyGradient[0] = weight * diff;

    // Checking whether the experience is on policy (i.e., it is within the [1/cutoff, cutoff] region)
    bool isOnPolicy = (importanceWeight > (1.0f / _experienceReplayREFERCutoffScale)) && (importanceWeight < _experienceReplayREFERCutoffScale);

    // Compute mu(s) & and P(s) gradient only if inside trust region (or REFER disabled)
    if (isOnPolicy || _experienceReplayREFEREnabled == false)
    {
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      {
        float dmu = expAction[i] - curMeans[i];
        policyGradient[1 + i] = weight * _experienceReplayREFERCurrentBeta * diff * dmu * curDiagP[i];
        policyGradient[1 + i + _problem->_actionVectorSize] = -weight * _experienceReplayREFERCurrentBeta * 0.5 * diff * dmu * dmu / (_actionScalings[i] * _actionScalings[i]);
      }
    }

    if (_experienceReplayREFEREnabled)
    {
      // Compute derivative of kullback-leibler divergence wrt current distribution params
      auto klGrad = calculateKLDivergenceGradient(expMeans, expSigmas, curMeans, curSigmas);

      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      {
        // Step towards old policy (gradient pointing to larger difference between old and current policy)
        policyGradient[1 + i] -= weight * (1.0f - _experienceReplayREFERCurrentBeta) * klGrad[i];
        // inlcude gradient of sigma wrt. diagP
        policyGradient[1 + i + _problem->_actionVectorSize] -= weight * (1.0f - _experienceReplayREFERCurrentBeta) * klGrad[i + _problem->_actionVectorSize] * (-0.5 * std::sqrt(_covarianceScaling) * std::pow(curDiagP[i], -1.5));
      }
    }

    _qProblem->_inputData[b] = expStateSequence;
    _qProblem->_solutionData[b] = policyGradient;

    // Keeping statistics
    cumulativeQStar += yHat;
    cumulativeQStarSquared += yHat * yHat;
    for (size_t j = 0; j < _problem->_actionVectorSize; j++) _statisticsAverageActionSigmas[j] += curSigmas[j];
  }

  // Running one generation of the optimization method with the given mini-batch
  _qLearner->runGeneration();

  // Keeping statistics
  _statisticsAverageQStar = cumulativeQStar / (float)_miniBatchSize;
  _statisticsStdevQStar = std::sqrt(cumulativeQStarSquared / (float)_miniBatchSize - _statisticsAverageQStar * _statisticsAverageQStar);
  _statisticsAverageTDError = cumulativeTdError / (float)_miniBatchSize;
  _statisticsStdevTDError = std::sqrt(cumulativeTdErrorSquarred / (float)_miniBatchSize - _statisticsAverageTDError * _statisticsAverageTDError);
  for (size_t j = 0; j < _statisticsAverageActionSigmas.size(); j++) _statisticsAverageActionSigmas[j] /= (float)_miniBatchSize;

  /*********************************************************************
   * Updating hyperparameters and broadcasting them to the workers
   *********************************************************************/

  auto qHyperparam = _qLearner->getTrainingHyperparameters();

  // Update the target network
  for (size_t i = 0; i < qHyperparam.size(); ++i)
  {
    _qHatHyperparameter[i] = _targetLearningRate * qHyperparam[i] + (1. - _targetLearningRate) * _qHatHyperparameter[i];
  }
  _criticLearner->setInferenceHyperparameters(_qHatHyperparameter);
}

knlohmann::json NAF::runPolicy(const std::vector<std::vector<float>> &state)
{
  auto qNNoutput = _qLearner->getEvaluation({state})[0];
  float stateValue = qNNoutput[0];

  // Transforming NN output to receive mu and diagonal of P
  std::vector<float> actionMeans(_problem->_actionVectorSize);
  std::vector<float> actionSigmas(_problem->_actionVectorSize);
  std::vector<float> diagP(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
  {
    actionMeans[i] = qNNoutput[1 + i];
    diagP[i] = qNNoutput[1 + _problem->_actionVectorSize + i] / (_actionScalings[i] * _actionScalings[i]);
    actionSigmas[i] = std::sqrt(_covarianceScaling) / std::sqrt(diagP[i]);
  }

  if (_policyDistribution == "Beta")
  {
    // Transform to variance coefficient
    for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
    {
      float mux = (actionMeans[i] - _actionLowerBounds[i]) / _actionScalings[i];
      float sigx = actionSigmas[i] / _actionScalings[i];
      actionSigmas[i] = mux * (1.0 - mux) / sigx;
      if (actionSigmas[i] > 1.0) actionSigmas[i] = 1.0;
    }
  }

  knlohmann::json policy;
  policy["State Value"] = stateValue;
  policy["Action Means"] = actionMeans;
  policy["Action Sigmas"] = actionSigmas;
  policy["Diag P"] = diagP;
  return policy;
}

float NAF::quadraticAdvantageFunction(const std::vector<float> &action, const std::vector<float> &mean, const std::vector<float> &diagP)
{
  float qval = 0;
  for (size_t i = 0; i < _problem->_actionVectorSize; ++i)
  {
    float dmu = (action[i] - mean[i]);
    qval += dmu * diagP[i] * dmu;
  }
  return -0.5 * qval;
}

knlohmann::json NAF::getAgentPolicy()
{
  knlohmann::json hyperparameters;
  hyperparameters["Critic"] = _qLearner->getInferenceHyperparameters();
  return hyperparameters;
}

void NAF::setAgentPolicy(const knlohmann::json &hyperparameters)
{
  _qLearner->setInferenceHyperparameters(hyperparameters["Critic"].get<std::vector<float>>());
}

void NAF::setTrainingState(const knlohmann::json &state)
{
  _qLearner->setTrainingHyperparameters(state["Q"]["Training"]);
  _qLearner->setInferenceHyperparameters(state["Q"]["Inference"]);

  _criticLearner->setTrainingHyperparameters(state["Critic"]["Training"]);
  _criticLearner->setInferenceHyperparameters(state["Critic"]["Inference"]);
}

knlohmann::json NAF::getTrainingState()
{
  knlohmann::json state;
  state["Q"]["Training"] = _qLearner->getTrainingHyperparameters();
  state["Q"]["Inference"] = _qLearner->getInferenceHyperparameters();

  state["Critic"]["Training"] = _criticLearner->getTrainingHyperparameters();
  state["Critic"]["Inference"] = _criticLearner->getInferenceHyperparameters();
  return state;
}

void NAF::printAgentInformation()
{
  _k->_logger->logInfo("Normal", "Critic Information:\n");
  _k->_logger->logInfo("Normal", " + Average (Std) Q-Value in Mini-Batch:  %f (%f)\n", _statisticsAverageQStar, _statisticsStdevQStar);
  _k->_logger->logInfo("Normal", " + Average (Std) TD-Error in Mini-Batch:  %f (%f)\n", _statisticsAverageTDError, _statisticsStdevTDError);
  _k->_logger->logInfo("Normal", " + Average Action Sigma in Mini-Batch: \n");
  for (size_t i = 0; i < _statisticsAverageActionSigmas.size(); ++i) _k->_logger->logInfo("Normal", " +  [ %f ]\n", _statisticsAverageActionSigmas[i]);

  _qExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _qExperiment._solver->printGenerationAfter();
  _qExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

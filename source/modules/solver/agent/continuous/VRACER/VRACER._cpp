#include "engine.hpp"
#include "modules/solver/agent/continuous/VRACER/VRACER.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
void VRACER::initializeAgent()
{
  // Initializing common discrete agent configuration
  Continuous::initializeAgent();

  // Engine to initialize experiments with
  korali::Engine engine;

  // Allocating vracer-specific replay memory items
  _experienceReplayImportanceWeight.resize(_experienceReplayMaximumSize);
  _experienceReplayActionProbability.resize(_experienceReplayMaximumSize);

  /*********************************************************************
  * Initializing Normal Distributions for the Policy
  *********************************************************************/

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    knlohmann::json js;
    js["Type"] = "Univariate/Normal";
    js["Mean"] = 0.0;
    js["Standard Deviation"] = 1.0;

    auto d = dynamic_cast<distribution::univariate::Normal *>(getModule(js, _k));

    _policyDistributions.push_back(d);
  }

  /*********************************************************************
 * Initializing Neural Network
 *********************************************************************/

  _criticPolicyExperiment["Problem"]["Type"] = "Supervised Learning";

  _criticPolicyExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _criticPolicyExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticPolicyExperiment["Solver"]["Learning Rate"] = _criticLearningRate;
  _criticPolicyExperiment["Solver"]["Loss Function"] = "Direct Gradient";
  _criticPolicyExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticPolicyExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _criticMiniBatchSize; i++)
  {
    for (size_t j = 0; j < _problem->_stateVectorSize; j++)
      _criticPolicyExperiment["Problem"]["Inputs"][i][j] = 0.0;
    
    // V(s), mean(s) and Sigma(s)
    for( size_t j = 0; j < 2*_problem->_actionVectorSize+1; j++ )
      _criticPolicyExperiment["Problem"]["Solution"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  engine.initialize(_criticPolicyExperiment);

  // Getting learner and problem pointers
  _criticPolicyProblem = dynamic_cast<problem::SupervisedLearning *>(_criticPolicyExperiment._problem);
  _criticPolicyLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticPolicyExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Get the initial set of policy NN hyperparameters
  _hyperparameters["Policy"] = _criticPolicyLearner->getHyperparameters();
}

void VRACER::updateAgentPolicy(const knlohmann::json &hyperparameters)
{
  _criticPolicyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<float>>());
}

void VRACER::trainAgent()
{
  // Resetting critic statistics
  _cumulativeQStar = 0;

  // Creating a minibatch of starting experiences for the trajectories update
  auto miniBatchIndexes = generateMiniBatch(_criticMiniBatchSize);

  // Initializing inputs and solution for the critic and policy problems
  _criticPolicyProblem->_inputs.clear();
  _criticPolicyProblem->_solution.clear();

  // Fill Minibatch
  for( size_t i = 0; i<_criticMiniBatchSize; i++ )
  {
    size_t curId = miniBatchIndexes[i];

    // Getting Vtbc from retrace function
    float expVtbc = retraceFunction(curId);

    // Get state, action,  mean and Sigma for this experience
    std::vector<float> expState = _experienceReplayStates[curId];
    std::vector<float> expAction = _experienceReplayActions[curId];
    std::vector<float> expMean = _experienceReplayActionMeans[curId];
    std::vector<float> expSigma = _experienceReplayActionSigmas[curId];
    float pExpActionOldPolicy = _experienceReplayActionProbability[curId];

    // Make sure sigma!=0 and take absolute value; compute inverse variance
    std::vector<float> expAbsSigma(_problem->_actionVectorSize);
    std::vector<float> expInvVariance(_problem->_actionVectorSize);
    for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
    {
      expAbsSigma[i] = std::abs(expSigma[i]) < 1e-7 ? 1e-7 : std::abs(expSigma[i]);
      expInvVariance[i] = 1.0f/(expAbsSigma[i]*expAbsSigma[i]);
    }

    // Forward the neural network for this state
    auto expEvaluation = _criticPolicyLearner->getEvaluation(expState);

    // std::cout << "sigma="<< expEvaluation[2] << "\n";
    // getchar();

    // Separate V(s), mean(s), Sigma(s) and compute variance for current policy
    float expStateValue = expEvaluation[0];
    std::vector<float> curMean(_problem->_actionVectorSize);
    std::vector<float> curSigma(_problem->_actionVectorSize);
    std::vector<float> curAbsSigma(_problem->_actionVectorSize);
    std::vector<float> curInvVariance(_problem->_actionVectorSize);
    for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
    {
      curMean[i] = expEvaluation[1+i];
      curSigma[i] = expEvaluation[_problem->_actionVectorSize+1+i];
      curAbsSigma[i] = std::abs(curSigma[i]) < 1e-7 ? 1e-7 : std::abs(curSigma[i]);
      curInvVariance[i] = 1.0f/(curAbsSigma[i]*curAbsSigma[i]);
    }

    // Set policy distribution to follow mean/sigma for current weights
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      _policyDistributions[i]->_mean = curMean[i];
      _policyDistributions[i]->_standardDeviation = curAbsSigma[i];
      _policyDistributions[i]->updateDistribution();
    }

    // Compute probability of action under current policy 
    float pExpActionCurPolicy = 1.0;
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      pExpActionCurPolicy *= _policyDistributions[i]->getDensity(expAction[i]);
    }

    // Compute importance weight
    float importanceWeight = pExpActionCurPolicy / pExpActionOldPolicy;
    _experienceReplayImportanceWeight[curId] = importanceWeight;

    //////// Fill Minibatch /////////
    ////////////////////////////////

    //// Fill input
    _criticPolicyProblem->_inputs[i] = expState;

    //// Fill Gradient of Loss
    std::vector<float> gradientLoss(2*_problem->_actionVectorSize+1,0);
    
    // For Value
    gradientLoss[0] = ( expVtbc - expStateValue );
    // std::cout << "dL_V=" << gradientLoss[0] << "\n";

    // For Policy 
    /* Compute policy gradient if inside trust region */
    _cutoff = 1.0f+_cutoffScale/(1.0f+_annealingRate*(float)_optimizationStepCount);
    if( importanceWeight < _cutoff && importanceWeight > 1.0f/_cutoff )
    {
      // Compute -rho(Qret-V) = rho(V-Qret)
      float V = expStateValue;
      float Qret = 0.0f;

      if( _experienceReplayTerminal[curId] == false )
      {
        float nextExpVtbc = retraceFunction(curId+1);
        float nextExpReward = _experienceReplayRewards[curId+1];
        Qret = nextExpReward + _criticDiscountFactor*nextExpVtbc;
      }

      float valueDiff = (V - Qret) / pExpActionOldPolicy;

      // std::cout << "dL_p=" << valueDiff << "\n";
      //// Compute Contribution from \nabla log(p) (recall Gaussian policy)
      for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
      {
        float actionDiff = ( expMean[i] - expAction[i] );
        gradientLoss[1+i] =  -valueDiff * actionDiff * curInvVariance[i]; // contribution to mean
        // std::cout << "without KL: dL_p/dmu=" << gradientLoss[1+i] << "\n";

        float gradLoss = valueDiff * (actionDiff*actionDiff * curInvVariance[i] / curAbsSigma[i]-1.0f/curAbsSigma[i]);
        gradientLoss[_problem->_actionVectorSize+1+i] = expSigma[i] > 0 ? gradLoss: -gradLoss; // contribution to Sigma accounting for the fact that we use an abs on the NN output
        // std::cout << "dL_p/dsigma=" << gradientLoss[_problem->_actionVectorSize+1+i] << "\n";
      }
    }

    /* Penalize off-policyness */
    // update learning rate and beta
    _criticPolicyLearner->_learningRate *= 1.0f/(1.0f+_annealingRate*_optimizationStepCount);
    // compute fraction of off-policy samples
    float offPolicyFraction = 0.0f;
    for( size_t i = 0; i<_experienceReplayImportanceWeight.size(); i++ )
      if( _experienceReplayImportanceWeight[i] < _cutoff || _experienceReplayImportanceWeight[i] > 1.0f/_cutoff )
        offPolicyFraction++;

    if( offPolicyFraction > _targetOffPolicyFraction )
      _beta=(1.0f-_criticPolicyLearner->_learningRate)*_beta;
    else
      _beta=(1.0f-_criticPolicyLearner->_learningRate)*_beta + _criticPolicyLearner->_learningRate;

    // compute derivative of Kullback-Leibler \nablaCurr(1/2(tr(SigmaCur^{-1}SigmaExp)+(muCur-muExp)SigmaCur^{-1}(muCur-muExp)+ln(det(SigmaCur)/det(SigmaExp))))
    for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
    {
      // contribution to mean
      gradientLoss[1+i] -= (1.0f-_beta)*(curMean[i]-expMean[i]) * curInvVariance[i];
      // std::cout << "with KL: dL_p/dmu=" << gradientLoss[1+i] << "\n";
      // contribution to Sigma from 1/2tr(SigmaCur^{-1}SigmaExp)
      float gradTr = -1.0f/expInvVariance[i]*curInvVariance[i]/curAbsSigma[i];
      // contribution to Sigma from 1/2ln(det(SigmaCur))
      float gradDet = curAbsSigma[i];
      float detSigmaCur = 0.0f;
      for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
        detSigmaCur += 1.0f/curInvVariance[i];
      gradDet /= detSigmaCur;
      // contribution to Sigma
      float gradLoss = (1.0f-_beta)*(gradTr+gradDet);
      gradientLoss[_problem->_actionVectorSize+1+i] -= expSigma[i] > 0 ? gradLoss : -gradLoss; // contribution to Sigma taking into account that we use abs on the NN output
      // std::cout << "with KL part 1: dL_p/dsigma=" << gradientLoss[_problem->_actionVectorSize+1+i] << "\n";
    }

    // Set Gradient of Loss as Solution
    _criticPolicyProblem->_solution[i] = gradientLoss;
  }

  // Running one generation of the optimization method with the given mini-batch
  _criticPolicyLearner->initialize();
  _criticPolicyLearner->runGeneration();
  _criticPolicyLearner->finalize();

  /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

  normalizeStateNeuralNetwork(_criticPolicyLearner->_trainingNeuralNetwork, _criticMiniBatchSize, _criticNormalizationSteps);

  // Storing average policy hyperparameters
  _hyperparameters["Policy"] = _criticPolicyLearner->getHyperparameters();
}

float VRACER::stateValueFunction(const std::vector<float> &state)
{
  // Calculating V(s_i+1)
  float v = _criticPolicyLearner->getEvaluation(state)[0];

  return v;
}

float VRACER::retraceFunction(size_t expId)
{
  // Finding last experience in the episode that corresponds to expId
  ssize_t startId = expId;
  ssize_t endId = startId;
  while (_experienceReplayTerminal[endId] == false) endId++;

  // Initializing vTbc to zero
  float vTbc = 0.0;

  // Initializing values coming from the next experience (for latest experience, just reward)
  float vTbcNext = _experienceReplayRewards[endId];

  // Setting starting experience as the second to last
  ssize_t curId = endId - 1;

  // Now checking whether any necessary vTbcNext is present in cache
  // Here we need to use cutoff criteria instead of cache age
//  for (ssize_t i = startId; i < endId; i++)
//  {
//    // If the difference in age is within the range, then it's a cache hit
//    if (_optimizationStepCount - _qCacheAge[i] < _criticRetraceCachePersistence)
//    {
//      vTbcNext = _vTbcNextCache[i];
//      curId = i;
//      break;
//    }
//  }

  // Now iterating backwards to calculate the rest of vTbc
  for (; curId >= startId; curId--)
  {
    // Getting current reward
    float curReward = _experienceReplayRewards[curId];

    // Re-calculating vTbc
    vTbc = curReward + _criticDiscountFactor * vTbcNext;

    // If this is the starting experience, return the current value
    if (curId == startId) break;

    // Getting current state
    std::vector<float> curState = _experienceReplayStates[curId];

    // Getting current action index
    auto curAction = _experienceReplayActions[curId];

    // Updating action distributions for the current state
    forwardPolicy(curState);

    // Now calculating V, A, and avgA for current state/action
    float curV = stateValueFunction(curState);

    // Getting probability densities for current action given current policy
    float pCurPolicy = 1.0f;
    for (size_t i = 0; i < curAction.size(); i++)
    {
      _normalGenerator->_mean = _actionMeans[i];
      _normalGenerator->_standardDeviation = _actionSigmas[i];
      _normalGenerator->updateDistribution();
      pCurPolicy *= _normalGenerator->getDensity(curAction[i]);
    }

    // Getting probability densities for current action given current policy
    float pOldPolicy = 1.0f;
    for (size_t i = 0; i < curAction.size(); i++)
    {
      _normalGenerator->_mean = _experienceReplayActionMeans[curId][i];
      _normalGenerator->_standardDeviation = _experienceReplayActionSigmas[curId][i];
      _normalGenerator->updateDistribution();
      pOldPolicy *= _normalGenerator->getDensity(curAction[i]);
    }

    // Now calculating importance weight for the old s,a experience
    float importanceWeight = pCurPolicy / pOldPolicy;

    // Now calculating truncated importance weight with 1.0 as truncation factor
    float truncatedImportanceWeight = std::min(1.0f, importanceWeight);

    // Updating vTbc
    vTbcNext = truncatedImportanceWeight * vTbcNext - curV;

    // Refreshing cache with the calculated value
    _vTbcNextCache[curId - 1] = vTbcNext;
    _qCacheAge[curId - 1] = _optimizationStepCount;
  }

  // Returning vTbc
  return vTbc;
}

void VRACER::forwardPolicy(const std::vector<float> &state)
{
  // Forward the neural network for this state
  std::vector<float> evaluation = _criticPolicyLearner->getEvaluation(state);
  _stateValue = evaluation[0];
  for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
  {
    _actionMeans[i] = evaluation[1+i];
    _actionSigmas[i] = evaluation[_problem->_actionVectorSize+1+i];
  }
}

void VRACER::getAction(korali::Sample &sample)
{
  Continuous::getAction(sample);

  // Set Value
  sample["Metadata"]["Value"] = _stateValue;

  // Set policy distribution to follow mean/sigma for current weights
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    _policyDistributions[i]->_mean = _actionMeans[i];
    _policyDistributions[i]->_standardDeviation = std::abs(_actionSigmas[i]) < 1e-7 ? 1e-7 : std::abs(_actionSigmas[i]);
    _policyDistributions[i]->updateDistribution();
  }

  // Compute probability of action under current policy 
  auto action = sample["Action"].get<std::vector<float>>();
  float pAction = 1.0f;
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    pAction *= _policyDistributions[i]->getDensity(action[i]);
  }
  // safety to avoid pAction==0
  sample["Metadata"]["Action Probability"] = pAction <= 1e-7 ? 1e-7 : pAction;
}

void VRACER::printAgentInformation()
{
  _k->_logger->logInfo("Normal", "Neural Network Information:\n");
  _criticPolicyExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _criticPolicyExperiment._solver->printGenerationAfter();
  _criticPolicyExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali


// float VRACER::computeQret(size_t startId)
// {
//   // Going straight to the latest experience in the corresponding trajectory
//   size_t curId = startId;

//   // Increase trajectory end position pointer until
//   while ( (_experienceReplayTerminal[endId] == false) ) // experience is not terminal
//    curId++;

//   // First, get the retrace value for the last experience (just the reward)
//   float qRet = _experienceReplayRewards[curId];

//   while (curId > startId)
//   {
//     // Decreasing current experience index
//     curId--;

//     // Getting current reward
//     float curReward = _experienceReplayRewards[curId];

//     // Getting experience's value, importance weight and Qret
//     float expValue = _experienceReplayValue[curId];
//     float expImportanceWeight = _experienceReplayImportanceWeight[curId];
//     float expQret = _experienceReplayQret[curId];

//     qRet += _criticDiscountFactor*expValue + _criticDiscountFactor * std::min(1.0f, expImportanceWeight)*( expQret - expValue );
//   }

//   return qRet
// }

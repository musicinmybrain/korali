#include "engine.hpp"
#include "modules/solver/agent/continuous/VRACER/VRACER.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace continuous
{
void VRACER::initializeAgent()
{
  // Initializing common discrete agent configuration
  Continuous::initializeAgent();

  // Engine to initialize experiments with
  korali::Engine engine;

  /*********************************************************************
  * Initializing Normal Distributions for the Policy
  *********************************************************************/

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    knlohmann::json js;
    js["Type"] = "Univariate/Normal";
    js["Mean"] = 0.0;
    js["Standard Deviation"] = 0.0;

    auto d = dynamic_cast<distribution::univariate::Normal *>(getModule(js, _k));

    _policyDistributions.push_back(d);
  }

  /*********************************************************************
 * Initializing Neural Network
 *********************************************************************/

  _criticPolicyExperiment["Problem"]["Type"] = "Supervised Learning";

  _criticPolicyExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _criticPolicyExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticPolicyExperiment["Solver"]["Learning Rate"] = _criticLearningRate;
  _criticPolicyExperiment["Solver"]["Loss Function"] = "Direct";
  _criticPolicyExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticPolicyExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _criticMiniBatchSize; i++)
  {
    for (size_t j = 0; j < _problem->_stateVectorSize; j++)
      _criticPolicyExperiment["Problem"]["Inputs"][i][j] = 0.0;
    
    // V(s), mean(s) and Sigma(s)
    for( size_t j = 0; j < 2*_problem->_actionVectorSize+1; j++ )
      _criticPolicyExperiment["Problem"]["Solution"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  engine.initialize(_criticPolicyExperiment);

  // Getting learner and problem pointers
  _criticPolicyProblem = dynamic_cast<problem::SupervisedLearning *>(_criticPolicyExperiment._problem);
  _criticPolicyLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticPolicyExperiment._solver);


  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Get the initial set of policy NN hyperparameters
  _hyperparameters["Policy"] = _criticPolicyLearner->getHyperparameters();
}

void VRACER::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  _criticPolicyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<float>>());
}

void VRACER::trainAgent()
{
  // Resetting critic statistics
  _cumulativeQStar = 0;

  // Creating storage for state history indexes to choose from
  std::vector<size_t> experienceReplayIndexes(_experienceReplayStates.size());
  for (size_t i = 0; i < _experienceReplayStates.size() - 1; i++) experienceReplayIndexes[i] = i;

  // Shuffling indexes to choose the mini batch from
  std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

  // Container for start and end of trajectories
  size_t trajectoryEndIndex;

  // Initializing inputs and solution for the critic and policy problems
  _criticPolicyProblem->_inputs.clear();
  _criticPolicyProblem->_solution.clear();

  // Fill Minibatch
  for( size_t i = 0; i<_criticMiniBatchSize; i++ )
  {
    trajectoryEndIndex = experienceReplayIndexes[i];

    for( size_t j = 0; j<_trajectorySize; j++ ) 
    {
      if ( (_experienceReplayTerminal[trajectoryEndIndex] == false) ||    // The experience is terminal
        (trajectoryEndIndex == _experienceReplayStates.size() - 1))   // Reached the end of the replay experience
        break;
      trajectoryEndIndex++;
    }
    processTrajectory(experienceReplayIndexes[i],trajectoryEndIndex);
  }

  // Declaring engine to launch experiments
  korali::Engine engine;

  // Running one generation of the optimization method with the given mini-batch
  _criticPolicyLearner->initialize();
  _criticPolicyLearner->runGeneration();
  _criticPolicyLearner->finalize();

  /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

  normalizeStateActionNeuralNetwork(_criticPolicyLearner->_trainingNeuralNetwork, _criticMiniBatchSize, _criticNormalizationSteps);

  // Storing new inference parameters
  _criticPolicyLearner->setHyperparameters( _criticPolicyLearner->getHyperparameters());
}

void VRACER::processTrajectory(size_t startId, size_t endId)
{
  // Going straight to the latest experience in the corresponding trajectory
  size_t curId = endId;

  // Get state, action, Vtbc, mean and Sigma for this experience
  std::vector<float> expState = _experienceReplayStates[curId];
  std::vector<float> expAction = _experienceReplayActions[curId];
  float expVtbc = _experienceReplayVtbc[curId];
  std::vector<float> expMean = _experienceReplayActionMeans[curId];
  std::vector<float> expSigma = _experienceReplayActionSigmas[curId];
  float pExpActionOldPolicy = _experienceReplayActionProbability[curId];

  // Forward the neural network for this state
  std::vector<float> expEvaluation = _criticPolicyLearner->getEvaluation(expState);

  // Separate V(s), mean(s), Sigma(s)
  float expStateValue = expEvaluation[0];
  std::vector<float> currMean(_problem->_actionVectorSize);
  std::vector<float> currSigma(_problem->_actionVectorSize);
  for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
  {
    currMean[i] = expEvaluation[1+i];
    currSigma[i] = expEvaluation[_problem->_actionVectorSize+1+i];
  }

  // Set policy distribution to follow mean/sigma for current weights
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    _policyDistributions[i]->_mean = currMean[i];
    _policyDistributions[i]->_standardDeviation = currSigma[i];
  }

  // Compute probability of action under current policy 
  float pExpActionCurPolicy = 1.0;
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    pExpActionCurPolicy *= _policyDistributions[i]->getDensity(expAction[i]);
  }

  // Compute importance weight
  float importanceWeight = pExpActionCurPolicy / pExpActionOldPolicy;

  //////// Fill Minibatch /////////
  ////////////////////////////////

  //// Fill input
  _criticPolicyProblem->_inputs.push_back(expState);

  //// Fill Gradient of Loss
  std::vector<float> gradientLoss(2*_problem->_actionVectorSize+1,0);
  
  // For Value
  gradientLoss[0] = ( expStateValue - expVtbc );

  // For Policy 
  /* Compute policy gradient if inside trust region */
  float cutOffParam = computeCutoff();
  if( importanceWeight < cutOffParam || importanceWeight > 1/cutOffParam )
  {
    // Compute rho(Q-V)
    float valueDiff = importanceWeight*expStateValue;
    if( _experienceReplayTerminal[curId] == false )
    {
      float nextExpVtbc = _experienceReplayVtbc[curId+1];
      float nextExpReward = _experienceReplayReward[curId+1];
      float Qret = nextExpReward + _criticDiscountFactor*nextExpVtbc
      float valueDiff -= importanceWeight*Qret
    }
    // Compute Contribution from \nabla log(p) (recall Gaussian policy)
    for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
    {
      gradientLoss[1+i] = ( expAction - expMean[i] ) / expSigma[i]; // contribution to mean
      gradientLoss[_problem->_actionVectorSize+1+i] = -2 * ( expAction - expMean[i] )*( expAction - expMean[i] ) / ( std::pow(expSigma[i],3) ); // contribution to Sigma
    }
  }

  /* Penalize off-policyness */
  float beta = getBeta();
  float detOldSigma = 1.0;
  float detCurSigma = 1.0;
  for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
  {
    float expVar = 1/(expSigma[i]*expSigma[i]);
    gradientLoss[1+i] = -(1-beta)*(expMean[i]-currMean[i])/expVar; // contribution to mean
    gradientLoss[_problem->_actionVectorSize+1+i] -= (1-beta)*(currSigma[i]/(expSigma[i]*expSigma[i])); // first contribution to Sigma tr(..)
    detOldSigma *= expVar;
    detCurSigma *= currSigma[i]*currSigma[i];
  }
  // second contribution to sigma det(..)
  for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
    gradientLoss[_problem->_actionVectorSize+1+i] -= (1-beta)*detCurSigma/(detOldSigma*currSigma[i]);

  // Set Gradient of Loss as Solution
  _criticPolicyProblem->_solution.push_back(gradientLoss);

  //// UPDATE V-TRACE VALUE ////

  // truncate importance weight at 1
  float truncatedImportanceWeight = std::min(1.0, importanceWeight );

  // First, compute the Vret for the last experience
  _experienceReplayVtbc[curId] = (1-truncatedImportanceWeight) * expStateValue;

  while (curId > startId)
  {
    // Decreasing current experience index
    curId--;

    // Getting experience's state, action and old probability of taking the action
    std::vector<float> expState = _experienceReplayStates[curId];
    std::vector<float> expAction = _experienceReplayActions[curId];
    float pExpActionOldPolicy = _experienceReplayProbability[curId];

    // Compute value with current weights
    float expValue = _criticPolicyLearner->getEvaluation(prevState)[0];

    // Compute probability of action under current policy
    float pExpActionCurPolicy = 1.0;
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      pExpActionCurPolicy *= _policyDistributions[i]->getDensity(expAction[i]);
    }

    // Compute Truncated Importance Weight
    float importanceWeight = _experienceReplayProbability[curId]/pExpActionCurPolicy;
    float truncatedImportanceWeight = std::min(1.0, importanceWeight)

    // Update Vtbc
    _experienceReplayVtbc[curId] = expValue + truncatedImportanceWeight*(_experienceReplayRewards[curId+1]+_criticDiscountFactor*_experienceReplayVtbc[curId+1]-expValue);
  }
}

void VRACER::forwardPolicy(const std::vector<float> &state)
{
  // Forward the neural network for this state
  std::vector<float> evaluation = _criticPolicyLearner->getEvaluation(state);
  _stateValue = evaluation[0];
  for( size_t i = 0; i<_problem->_actionVectorSize; i++ )
  {
    _actionMeans[i] = evaluation[1+i];
    _actionSigmas[i] = evaluation[_problem->_actionVectorSize+1+i];
  }
}

void VRACER::processExperience(knlohmann::json &experience)
{
  // Running common experience processor first
  Continuous::processExperience(experience);

  // Adding specific information to the memory
  _experienceReplayActionVtbc.add(experience["Metadata"]["Vtbc"].get<std::vector<float>>());
  _experienceReplayActionActionProbability.add(experience["Metadata"]["Action Probability"].get<std::vector<float>>());
}

void VRACER::printAgentInformation()
{
  // Updating average Q*, for statistical purposes
  _averageQStar = _cumulativeQStar / _aProblem->_solution.size();

  _k->_logger->logInfo("Normal", "Critic Information:\n");

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _k->_logger->logInfo("Normal", "State Value (V) Information:\n");
  _vExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _vExperiment._solver->printGenerationAfter();
  _vExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Advantage (A) Information:\n");
  _aExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _aExperiment._solver->printGenerationAfter();
  _aExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Policy Information:\n");
  _policyExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _policyExperiment._solver->printGenerationAfter();
  _policyExperiment._logger->setVerbosityLevel("Silent");

  // Resetting cumulative Q*, for statistical purposes
  _cumulativeQStar = 0.0;
}

} // namespace continuous
} // namespace agent
} // namespace solver
} // namespace korali

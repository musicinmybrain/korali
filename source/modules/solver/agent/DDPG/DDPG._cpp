#include "modules/solver/agent/DDPG/DDPG.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
void DDPG::initializePolicy()
{

  // Checking if a correct action space was defined
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   if (_k->_variables[varIdx]->_values.size() > 0) KORALI_LOG_ERROR("Discrete action values (%lu, in total) have been defined for variable %lu (%s), but these are not required for DDPG\n", _k->_variables[varIdx]->_values.size(), varIdx, _k->_variables[varIdx]->_name.c_str());
   if (_k->_variables[varIdx]->_upperBound < _k->_variables[varIdx]->_lowerBound) KORALI_LOG_ERROR("Upper bound for variable %lu (%s) is lower than the lower bound (%f < %f).\n", varIdx, _k->_variables[varIdx]->_name.c_str(), _k->_variables[varIdx]->_upperBound, _k->_variables[varIdx]->_lowerBound);
   if (std::isfinite(_k->_variables[varIdx]->_lowerBound) == false) KORALI_LOG_ERROR("Lower bound for variable %lu (%s) is not finite or has not been specified.\n", varIdx, _k->_variables[varIdx]->_name.c_str());
   if (std::isfinite(_k->_variables[varIdx]->_upperBound) == false) KORALI_LOG_ERROR("Upper bound for variable %lu (%s) is not finite or has not been specified.\n", varIdx, _k->_variables[varIdx]->_name.c_str());
  }

  /*********************************************************************
   * Creating and running Actor Learning Experiments
   *********************************************************************/

  _policyExperiment["Problem"]["Type"] = "Supervised Learning";

  _policyExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _policyExperiment["Solver"]["Batch Normalization"]["Enabled"] = false;
  _policyExperiment["Solver"]["Loss Function"] = "Direct";
  _policyExperiment["Solver"]["Optimizer"] = _policyOptimizer;
  _policyExperiment["Solver"]["Steps Per Generation"] = 1;
  _policyExperiment["Solver"]["Neural Network"] = _policyNeuralNetwork;

  _policyExperiment["Console Output"]["Frequency"] = 0;
  _policyExperiment["Console Output"]["Verbosity"] = "Silent";
  _policyExperiment["File Output"]["Enabled"] = false;
  _policyExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _policyMiniBatchSize; i++)
  {
    for (size_t j = 0; j < _problem->_stateVectorSize; j++)
      _policyExperiment["Problem"]["Inputs"][i][j] = 0.0;

    for (size_t j = 0; j < _problem->_actionVectorSize; j++)
      _policyExperiment["Problem"]["Solution"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_policyExperiment);

  // Getting learner pointers
  _policyProblem = dynamic_cast<problem::SupervisedLearning *>(_policyExperiment._problem);
  _policyLearner = dynamic_cast<solver::learner::DeepGD *>(_policyExperiment._solver);

  if (_k->_currentGeneration == 0)
  {
    // Get the initial set of policy NN hyperparameters
    _hyperparameters["Policy"] = _policyLearner->getHyperparameters();
    _bestHyperparameters["Policy"] = _hyperparameters["Policy"];
  }

  // Storage for statistics
   avgGradients.resize(_problem->_actionVectorSize);

  // Assigning training hyperparameters to inference learner
  _policyLearner->setHyperparameters(_hyperparameters["Policy"]);
}

void DDPG::trainPolicy()
{
  /***********************************************************************************
   * Updating Actor/Policy/Pi Network by performing a Stochastic Gradient Descent step
   ***********************************************************************************/

  // Initializing gradient for statistics
  for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] = 0.0;

  // Creating state history indexes to choose from
  std::vector<size_t> stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) stateHistoryIndexes[i] = i;

  for (size_t i = 0; i < _policyMiniBatchSize; i++)
  {
    // Selecting a uniformly random selected, yet not repeated experience
    size_t expId = stateHistoryIndexes[_stateHistory.size() - _policyMiniBatchSize + i];

    // Storage to put together state and action
    std::vector<double> stateActionInput(_problem->_stateVectorSize + _problem->_actionVectorSize);

    // Now calculating Gradient of q with respect to state+action for the critic NN
    for (size_t j = 0; j < _problem->_stateVectorSize; j++) stateActionInput[j] = _stateHistory[expId][j];
    for (size_t j = 0; j < _problem->_actionVectorSize; j++) stateActionInput[_problem->_stateVectorSize + j] = _actionHistory[expId][j];

    _criticLearner->getEvaluation(stateActionInput);
    auto qGradients = _criticLearner->getGradients({{-1.0}}); // Identity, to estimate the exact gradient dQ/dAS
    std::vector<double> aGradients = std::vector<double>(qGradients.begin() + _problem->_stateVectorSize, qGradients.end());

    // Keeping track of gradient averages for statistics
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] += aGradients[i];

    for (size_t j = 0; j < _problem->_stateVectorSize; j++) _policyProblem->_inputs[i][j] = _stateHistory[expId][j];
    _policyProblem->_solution[i] = aGradients;
  }

  // Calculating gradient statistics
  for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] /= _policyMiniBatchSize;

  // Running one generation of the optimization method on the actor NN with the given mini-batch
  _policyExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _policyExperiment._currentGeneration + _policyOptimizationSteps;
  _policyLearner->initialize();
  _engine.resume(_policyExperiment);

  auto policyHypeparameters = _policyLearner->getHyperparameters();

  /*********************************************************************
  * Updating hyperparameters and broadcasting them to the workers
  *********************************************************************/

  // Getting q training hyperparameters to broadcast to workers
  _hyperparameters["Policy"] = policyHypeparameters;
  _policyLearner->setHyperparameters(policyHypeparameters);
}

std::vector<double> DDPG::queryPolicy(const std::vector<double> &state)
{
  return _policyLearner->getEvaluation({state});
}

void DDPG::updatePolicyHyperparameters(const knlohmann::json &hyperparameters)
{
  _policyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<double>>());
  _policyEpsilonCurrentValue = hyperparameters["Epsilon"].get<double>();

  // Re-initializing action noises to zero
  size_t actionCount = _problem->_actionVectorIndexes.size();
  _currentActionNoises.resize(actionCount);
  for (size_t i = 0; i < actionCount; i++) _currentActionNoises[i] = 0.0;
}

void DDPG::printPolicyInformation()
{
 _k->_logger->logInfo("Normal", " + Average Action Gradients: [");

  // Printing gradient averages
 _k->_logger->logData("Normal", " %e", avgGradients[0]);
  for (size_t i = 1; i < _problem->_actionVectorSize; i++) _k->_logger->logData("Normal", ", %e", avgGradients[i]);
 _k->_logger->logData("Normal", " ]\n");
}

} // namespace agent

} // namespace solver

} // namespace korali

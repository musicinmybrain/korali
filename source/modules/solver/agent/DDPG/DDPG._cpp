#include "modules/solver/agent/DDPG/DDPG.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
void DDPG::initialize()
{
  // Calling common initialization routine for all agents
  Agent::initialize();

  /*********************************************************************
  * Creating and running Critic Learning Experiments
  *********************************************************************/

  _criticExperiment["Problem"]["Type"] = "Supervised Learning";

  _criticExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _criticExperiment["Solver"]["Batch Normalization"]["Enabled"] = _batchNormalizationEnabled;
  _criticExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  _criticExperiment["Console Output"]["Frequency"] = 0;
  _criticExperiment["Console Output"]["Verbosity"] = "Silent";
  _criticExperiment["File Output"]["Enabled"] = false;
  _criticExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _miniBatchSize; i++)
  {
    _criticExperiment["Problem"]["Outputs"][i][0] = 0.0;

    for (size_t j = 0; j < _k->_variables.size(); j++)
      _criticExperiment["Problem"]["Inputs"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_criticExperiment);

  // Getting learner pointers
  _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
  _criticLearner = dynamic_cast<solver::learner::DeepGD *>(_criticExperiment._solver);

  /*********************************************************************
   * Creating and running Actor Learning Experiments
   *********************************************************************/

   _actorExperiment["Problem"]["Type"] = "Supervised Learning";

   _actorExperiment["Solver"]["Type"] = "Learner/DeepGD";
   _actorExperiment["Solver"]["Batch Normalization"]["Enabled"] = false;
   _actorExperiment["Solver"]["Optimizer"] = _actorOptimizer;
   _actorExperiment["Solver"]["Steps Per Generation"] = 1;
   _actorExperiment["Solver"]["Neural Network"] = _actorNeuralNetwork;

   _actorExperiment["Console Output"]["Frequency"] = 0;
   _actorExperiment["Console Output"]["Verbosity"] = "Silent";
   _actorExperiment["File Output"]["Enabled"] = false;
   _actorExperiment["Random Seed"] = _k->_randomSeed++;

   // Initializing experiment with an initial zero set
   for (size_t i = 0; i < _miniBatchSize; i++)
   {
     for (size_t j = 0; j < _problem->_stateVectorSize; j++)
     _actorExperiment["Problem"]["Inputs"][i][j] = 0.0;

     for (size_t j = 0; j < _problem->_actionVectorSize; j++)
     _actorExperiment["Problem"]["Outputs"][i][j] = 0.0;
   }

   // Running initialization to verify that the configuration is correct
   _engine.initialize(_actorExperiment);

   // Getting learner pointers
   _actorProblem = dynamic_cast<problem::SupervisedLearning *>(_actorExperiment._problem);
   _actorLearner = dynamic_cast<solver::learner::DeepGD *>(_actorExperiment._solver);

 /*********************************************************************
  * Setting initial configuration
  *********************************************************************/

  // If initial generation, set initial DDPG configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _suboptimalStepCounter = 0;
    _optimizationStepCount = 0;

    // Initialize best reward
    _bestAverageReward = -korali::Inf;

    // Get the initial set of hyperparameters
    _hyperparameters = _actorLearner->getHyperparameters();
    _bestHyperparameters = _hyperparameters;
  }

  // Assigning training hyperparameters to inference learner
  _actorLearner->setHyperparameters(_hyperparameters);
}

void DDPG::updatePolicy()
{

 /***********************************************************************************
  * Grabbing current set of hyperparameters
  **********************************************************************************/

  auto curActorHypeparameters  = _actorLearner->getHyperparameters();
  auto curCriticHyperparameters = _criticLearner->getHyperparameters();

  /***********************************************************************************
   * Randomly selecting experiences for the mini-batch and calculating their target Q
   ***********************************************************************************/

   // Creating state history indexes to choose from
   std::vector<size_t> _stateHistoryIndexes(_stateHistory.size());
   for (size_t i = 0; i < _stateHistory.size(); i++) _stateHistoryIndexes[i] = i;

   // Calculating cumulative Q*, for statistical purposes
   _cumulativeQStar = 0.0;

   for (size_t step = 0; step < _optimizationStepsPerGeneration; step++)
   {

   // Shuffling indexes to choose the mini batch from
   std::shuffle(_stateHistoryIndexes.begin(), _stateHistoryIndexes.end(), *mt);

   for (size_t i = 0; i < _miniBatchSize; i++)
   {
     // Selecting a uniformly random selected, yet not repeated experience
     size_t expId = _stateHistoryIndexes[i];

     // Qnew = max_a(q) with s' fixed
     // Q* = r + y*Qnew -- If not terminal state
     // Q* = r -- If terminal state

     // Calculating target Q value (solution) for Qnew on selected batch
     double qValue = _rewardHistory[expId];

     // If state is not terminal (next state is filled) then add Qnew to the Q value.
     if (_nextStateHistory[expId].size() > 0)
     {
       // Updating current state
       auto _currentState = _nextStateHistory[expId];

       // Running actor experiment to get its candidate action
       auto action = _actorLearner->getEvaluation({ _currentState });

       // Putting together state and action
       std::vector<double> stateActionInput(_currentState.size() + action.size());
       for (size_t j = 0; j < _currentState.size(); j++) stateActionInput[j] = _currentState[j];
       for (size_t j = 0; j < action.size(); j++) stateActionInput[j + _currentState.size()] = action[j];

       // Forward propagating state/action through the critic inference NN
       auto evaluation = _criticLearner->getEvaluation(stateActionInput);

       // Getting estimate Qnew, based on the NN evaluation
       auto qNew = evaluation[0];

       // We add the expected remaining reward based on the optimization
       qValue += _discountFactor * qNew;

       // Debug only, print new experience values
 //             printf("State: %f %f %f %f \n", _stateHistory[expId][0], _stateHistory[expId][1], _stateHistory[expId][2], _stateHistory[expId][3]);
 //             printf("Action: %f\n", _actionHistory[expId][0]);
 //             printf("Reward: %f\n", _rewardHistory[expId]);
 //             printf("New State: %f %f %f %f\n", _currentState[0], _currentState[1], _currentState[2], _currentState[3]);
 //             printf("New Action: %f\n", action[0]);
 //             printf("QNew: %f\n", qNew);
 //             printf("Q* %f\n", qValue);
      }

      // Now calculating Gradient of q with respect to state+action for the critic NN
      auto qGradients = _criticLearner->getGradients({{ qValue }});

      // Extracting the action gradients from the full critic gradients
      std::vector<double> aGradients = std::vector<double>(qGradients.begin() + _stateHistoryIndexes.size(), qGradients.end());

      // Updating inputs to critic training learner
      for (size_t j = 0; j < _problem->_stateVectorSize; j++) _criticProblem->_inputs[i][j] = _stateHistory[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) _criticProblem->_inputs[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
      _criticProblem->_outputs[i][0] = qValue;

      // Updating inputs to critic learner
      for (size_t j = 0; j < _problem->_stateVectorSize; j++) _actorProblem->_inputs[i][j] = _stateHistory[expId][j];
      _actorProblem->_outputs[i] = aGradients;

      // Keeping statistics
      _cumulativeQStar += qValue;
     }

     // Running one generation of the optimization method with the given mini-batch
     _criticExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _criticExperiment._currentGeneration + 1;
     _criticLearner->initialize();
    _engine.resume(_criticExperiment);

    // Running one generation of the optimization method on the actor NN with the given mini-batch
    _actorExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _criticExperiment._currentGeneration + 1;
    _actorLearner->initialize();
    _engine.resume(_actorExperiment);

     // Increasing optimization step counter
     _optimizationStepCount++;
   }

   // Keeping statistics
   _averageQStar = (double)_cumulativeQStar / (double)(_optimizationStepsPerGeneration * _miniBatchSize);

  /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

  if (_batchNormalizationEnabled) normalizeNeuralNetwork(_criticLearner->_trainingNeuralNetwork);

  /*********************************************************************
   * Adopting the new parameters
   *********************************************************************/

  auto newActorHypeparameters  = _actorLearner->getHyperparameters();
  auto newCriticHyperparameters = _criticLearner->getHyperparameters();

  for (size_t i = 0; i < newActorHypeparameters.size(); i++ ) newActorHypeparameters[i] = _adoptionRate*newActorHypeparameters[i] + (1-_adoptionRate) * curActorHypeparameters[i];
  for (size_t i = 0; i < newCriticHyperparameters.size(); i++ ) newCriticHyperparameters[i] = _adoptionRate*newCriticHyperparameters[i] + (1-_adoptionRate) * curCriticHyperparameters[i];

  /*********************************************************************
  * Updating hyperparameters and broadcasting them to the workers
  *********************************************************************/

  // Updating global configuration for workers to use
  knlohmann::json globals;

  // Getting q training hyperparameters to broadcast to workers
  _hyperparameters = newActorHypeparameters;
  _actorLearner->setHyperparameters(newActorHypeparameters);
  _criticLearner->setHyperparameters(newCriticHyperparameters);
}

void DDPG::getAction(Sample &sample)
{
  // Getting current state
  auto state = KORALI_GET(std::vector<double>, sample, "State");

  // Forward-propagating state through actor NN
  auto action = _actorLearner->getEvaluation( { state } );

  // Checking for correctness of the action
  for (size_t i = 0; i < action.size(); i++)
   if (std::isfinite(action[i]) == false)
    KORALI_LOG_ERROR(" + Action [%lu] is not finite (%f)\n", i, action[i]);

  // Introducing random noise to the action
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];
   double w = _k->_variables[varIdx]->_explorationNoiseRandomVariable->getRandomNumber();
   double noise = _k->_variables[varIdx]->_explorationNoiseTheta * _currentActionNoises[i] + w;
   action[i] += noise;
   //printf("Theta (%f) * Previous Noise: (%f) + W: (%f) = %f\n", _k->_variables[varIdx]->_explorationNoiseTheta, _currentActionNoises[i], w, noise);
   _currentActionNoises[i] = noise;
  }

  // Getting optimal action, based on the NN evaluation
  sample["Action"] = action;
}

void DDPG::updateHyperparmeters(Sample &sample)
{
 _actorLearner->setHyperparameters(sample.globals().get<std::vector<double>>());

 // Re-initializing action noises to zero
 size_t actionCount = _problem->_actionVectorIndexes.size();
 _currentActionNoises.resize(actionCount);
 for (size_t i = 0; i < actionCount; i++) _currentActionNoises[i] = 0.0;
}

void DDPG::printGenerationAfter()
{
  // Printing common experience information
  Agent::printGenerationAfter();

  _k->_logger->logInfo("Normal", "DDPG Statistics:\n");
  _k->_logger->logInfo("Normal", " + Experience Memory Size:          %lu/%lu\n", _stateHistory.size(), _replayMemoryMaximumSize);

  _k->_logger->logInfo("Normal", "Optimization Statistics:\n");
  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _k->_logger->logInfo("Normal", "Critic Network Information:\n");
  _criticExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _criticExperiment._solver->printGenerationAfter();
  _criticExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Actor Network Information:\n");
  _actorExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _actorExperiment._solver->printGenerationAfter();
  _actorExperiment._logger->setVerbosityLevel("Silent");
}

std::vector<double> DDPG::getAction(const std::vector<double> &state)
{
  // Updating learner with the latest hyperparameters
  _actorLearner->setHyperparameters(_bestHyperparameters);

  // Forward-propagating state through actor NN
  auto action = _actorLearner->getEvaluation( { state } );

  // Returning action, based on the NN evaluation
  return action;
}

} // namespace agent

} // namespace solver

} // namespace korali

#include "modules/conduit/conduit.hpp"
#include "modules/solver/agent/DDPG/DDPG.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
void DDPG::initialize()
{
  // Calling common initialization routine for all agents
  Agent::initialize();

  /*********************************************************************
  * Creating and running Critic Learning Experiments
  *********************************************************************/

  _criticTrainingExperiment["Problem"]["Type"] = "Supervised Learning";
  _criticTrainingExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _criticTrainingExperiment["Solver"]["Batch Normalization"]["Enabled"] = _batchNormalizationEnabled;
  _criticNeuralNetwork->getConfiguration(_criticTrainingExperiment["Solver"]["Neural Network"]);

  _criticTrainingExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticTrainingExperiment["Solver"]["Steps Per Generation"] = 1;

  _criticTrainingExperiment["Console Output"]["Frequency"] = 0;
  _criticTrainingExperiment["Console Output"]["Verbosity"] = "Silent";
  _criticTrainingExperiment["File Output"]["Enabled"] = false;
  _criticTrainingExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _miniBatchSize; i++)
  {
    _criticTrainingExperiment["Problem"]["Outputs"][i][0] = 0.0;

    for (size_t j = 0; j < _k->_variables.size(); j++)
      _criticTrainingExperiment["Problem"]["Inputs"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_criticTrainingExperiment);

  // Replicating experiment for training experiment
  knlohmann::json qCriticConfig;
  _criticTrainingExperiment.getConfiguration(qCriticConfig);
  _criticInferenceExperiment._js.getJson() = qCriticConfig;
  _engine.initialize(_criticInferenceExperiment);

  // Getting learner pointers
  _criticTrainingProblem = dynamic_cast<problem::SupervisedLearning *>(_criticTrainingExperiment._problem);
  _criticTrainingLearner = dynamic_cast<solver::learner::DeepGD *>(_criticTrainingExperiment._solver);
  _criticInferenceLearner = dynamic_cast<solver::learner::DeepGD *>(_criticInferenceExperiment._solver);

  /*********************************************************************
   * Creating and running Actor Learning Experiments
   *********************************************************************/

   _actorTrainingExperiment["Problem"]["Type"] = "Supervised Learning";
   _actorTrainingExperiment["Solver"]["Type"] = "Learner/DeepGD";
   _actorTrainingExperiment["Solver"]["Batch Normalization"]["Enabled"] = _batchNormalizationEnabled;
   _actorNeuralNetwork->getConfiguration(_actorTrainingExperiment["Solver"]["Neural Network"]);

   _actorTrainingExperiment["Solver"]["Optimizer"] = _actorOptimizer;
   _actorTrainingExperiment["Solver"]["Steps Per Generation"] = 1;

   _actorTrainingExperiment["Console Output"]["Frequency"] = 0;
   _actorTrainingExperiment["Console Output"]["Verbosity"] = "Silent";
   _actorTrainingExperiment["File Output"]["Enabled"] = false;
   _actorTrainingExperiment["Random Seed"] = _k->_randomSeed++;

   // Initializing experiment with an initial zero set
   for (size_t i = 0; i < _miniBatchSize; i++)
   {
     for (size_t j = 0; j < _problem->_stateVectorSize; j++)
     _actorTrainingExperiment["Problem"]["Inputs"][i][j] = 0.0;

     for (size_t j = 0; j < _problem->_actionVectorSize; j++)
     _actorTrainingExperiment["Problem"]["Outputs"][i][j] = 0.0;
   }

   // Running initialization to verify that the configuration is correct
   _engine.initialize(_actorTrainingExperiment);

   // Replicating experiment for training experiment
   knlohmann::json qActorConfig;
   _actorTrainingExperiment.getConfiguration(qActorConfig);
   _actorInferenceExperiment._js.getJson() = qActorConfig;
   _engine.initialize(_actorInferenceExperiment);

   // Getting learner pointers
   _actorTrainingProblem = dynamic_cast<problem::SupervisedLearning *>(_actorTrainingExperiment._problem);
   _actorTrainingLearner = dynamic_cast<solver::learner::DeepGD *>(_actorTrainingExperiment._solver);
   _actorInferenceLearner = dynamic_cast<solver::learner::DeepGD *>(_actorInferenceExperiment._solver);

 /*********************************************************************
  * Setting initial configuration
  *********************************************************************/

  // If initial generation, set initial DDPG configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _suboptimalStepCounter = 0;
    _optimizationStepCount = 0;

    // Initialize best reward
    _bestAverageReward = -korali::Inf;

    // Get the initial set of hyperparameters
    _hyperparameters = _actorTrainingLearner->getHyperparameters();
    _bestHyperparameters = _hyperparameters;
  }

  // Assigning training hyperparameters to inference learner
  _actorInferenceLearner->setHyperparameters(_hyperparameters);

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);
}

void DDPG::updatePolicy()
{
  /***********************************************************************************
   * Randomly selecting experiences for the mini-batch and calculating their target Q
   ***********************************************************************************/

  // Creating state history indexes to choose from
  std::vector<size_t> _stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) _stateHistoryIndexes[i] = i;

  // Calculating cumulative Q*, for statistical purposes
  _cumulativeQStar = 0.0;

  for (size_t step = 0; step < _optimizationStepsPerGeneration; step++)
  {
    // Shuffling indexes to choose the mini batch from
    std::shuffle(_stateHistoryIndexes.begin(), _stateHistoryIndexes.end(), *mt);

    for (size_t i = 0; i < _miniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = _stateHistoryIndexes[i];

      // Qnew = max_a(q) with s' fixed
      // Q* = r + y*Qnew -- If not terminal state
      // Q* = r -- If terminal state

      // Q Value is the expectation of the reward, with regards to the Bellman Equation
      double qValue = _rewardHistory[expId];

      // If state is not terminal (next state is filled) then add Qnew to the Q value.
      if (_nextStateHistory[expId].size() > 0)
      {
        // Updating current state
        _currentState = _nextStateHistory[expId];

        // Running actor experiment to get its candidate action
        auto action = _actorInferenceLearner->getEvaluation({ _currentState });

        // Putting together state and action
        std::vector<double> stateActionInput(_currentState.size() + action.size());
        for (size_t j = 0; j < _currentState.size(); j++) stateActionInput[j] = _currentState[j];
        for (size_t j = 0; j < action.size(); j++) stateActionInput[j + _currentState.size()] = action[j];

        // Forward propagating state/action through the critic inference NN
        auto evaluation = _criticInferenceLearner->getEvaluation(stateActionInput);

        // Getting estimate Qnew, based on the NN evaluation
        auto qNew = evaluation[0];

        // We add the expected remaining reward based on the optimization
        qValue += _discountFactor * qNew;

        // Debug only, print new experience values
        // printf("State: %f %f\n", _stateHistory[expId][0], _stateHistory[expId][1]);
        // printf("Action: %f\n", _actionHistory[expId][0]);
        // printf("Reward: %f\n", _rewardHistory[expId]);
        // printf("New State: %f %f\n", _currentState[0], _currentState[1]);
        // printf("New Action: %f\n", action[0]);
        // printf("QNew: %f\n", qNew);
        // printf("Q* %f\n", qValue);
      }

      // Now calculating Gradient of q with respect to state+action for the critic NN
      auto qGradients = _criticInferenceLearner->getGradients({{ qValue }});

      // Extracting the action gradients from the full critic gradients
      std::vector<double> aGradients = std::vector<double>(qGradients.begin() + _currentState.size(), qGradients.end());

      // Updating inputs to critic training learner
      for (size_t j = 0; j < _problem->_stateVectorSize; j++) _criticTrainingProblem->_inputs[i][j] = _stateHistory[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) _criticTrainingProblem->_inputs[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
      _criticTrainingProblem->_outputs[i][0] = qValue;

      // Updating inputs to actor training learner
      for (size_t j = 0; j < _problem->_stateVectorSize; j++) _actorTrainingProblem->_inputs[i][j] = _stateHistory[expId][j];
      _actorTrainingProblem->_outputs[i] = aGradients;

      // Keeping statistics
      _cumulativeQStar += qValue;
    }

    // Running one generation of the optimization method with the given mini-batch
    _criticTrainingExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _criticTrainingExperiment._currentGeneration + 1;
    _criticTrainingLearner->initialize();
    _engine.resume(_criticTrainingExperiment);

    // Increasing optimization step counter
    _optimizationStepCount++;
  }

  // Keeping statistics
  _averageQStar = (double)_cumulativeQStar / (double)(_optimizationStepsPerGeneration * _miniBatchSize);

  /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

  if (_batchNormalizationEnabled)
  {
    auto criticNN = _criticTrainingLearner->_trainingNeuralNetwork;
    auto actorNN = _actorTrainingLearner->_trainingNeuralNetwork;

    std::vector<std::vector<double>> criticMeanSums(criticNN->_layers.size());
    std::vector<std::vector<double>> criticVarianceSums(criticNN->_layers.size());

    std::vector<std::vector<double>> actorMeanSums(actorNN->_layers.size());
    std::vector<std::vector<double>> actorVarianceSums(actorNN->_layers.size());

    for (size_t i = 1; i < criticNN->_layers.size(); i++)
    {
      criticMeanSums[i].resize(criticNN->_layers[i]->_nodeCount);
      criticVarianceSums[i].resize(criticNN->_layers[i]->_nodeCount);

      for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticMeanSums[i][j] = 0.0;
      for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticVarianceSums[i][j] = 0.0;
    }

    for (size_t i = 1; i < actorNN->_layers.size(); i++)
    {
      actorMeanSums[i].resize(actorNN->_layers[i]->_nodeCount);
      actorVarianceSums[i].resize(actorNN->_layers[i]->_nodeCount);

      for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorMeanSums[i][j] = 0.0;
      for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorVarianceSums[i][j] = 0.0;
    }

    std::vector<std::vector<double>> criticMiniBatch(_miniBatchSize);
    for (size_t i = 0; i < _miniBatchSize; i++) criticMiniBatch[i].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

    std::vector<std::vector<double>> actorMiniBatch(_miniBatchSize);
    for (size_t i = 0; i < _miniBatchSize; i++) actorMiniBatch[i].resize(_problem->_actionVectorSize);


    // Shuffling indexes to choose the mini batch from
    std::shuffle(_stateHistoryIndexes.begin(), _stateHistoryIndexes.end(), *mt);

    for (size_t step = 0; step < _batchNormalizationCorrectionSteps; step++)
    {
      for (size_t i = 0; i < _miniBatchSize; i++)
      {
        // Selecting a uniformly random selected, yet not repeated experience
        size_t expId = _stateHistoryIndexes[i];

        for (size_t j = 0; j < _problem->_stateVectorSize; j++) criticMiniBatch[i][j] = _stateHistory[expId][j];
        for (size_t j = 0; j < _problem->_actionVectorSize; j++) criticMiniBatch[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
        for (size_t j = 0; j < _problem->_stateVectorSize; j++) actorMiniBatch[i][j] = _stateHistory[expId][j];
      }

      criticNN->setInput(criticMiniBatch);
      criticNN->forward();

      actorNN->setInput(actorMiniBatch);
      actorNN->forward();

      for (size_t i = 1; i < criticNN->_layers.size(); i++)
      {
        for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticMeanSums[i][j] += criticNN->_layers[i]->_batchNormalizationMeans[j];
        for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticVarianceSums[i][j] += criticNN->_layers[i]->_batchNormalizationVariances[j];
      }

      for (size_t i = 1; i < actorNN->_layers.size(); i++)
      {
        for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorMeanSums[i][j] += actorNN->_layers[i]->_batchNormalizationMeans[j];
        for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorVarianceSums[i][j] += actorNN->_layers[i]->_batchNormalizationVariances[j];
      }
    }

    for (size_t i = 1; i < criticNN->_layers.size(); i++)
    {
      for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticNN->_layers[i]->_batchNormalizationMeans[j] = criticMeanSums[i][j] / _batchNormalizationCorrectionSteps;
      for (size_t j = 0; j < criticNN->_layers[i]->_nodeCount; j++) criticNN->_layers[i]->_batchNormalizationVariances[j] = criticVarianceSums[i][j] / (_batchNormalizationCorrectionSteps - 1.0);
    }

    for (size_t i = 1; i < actorNN->_layers.size(); i++)
    {
      for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorNN->_layers[i]->_batchNormalizationMeans[j] = actorMeanSums[i][j] / _batchNormalizationCorrectionSteps;
      for (size_t j = 0; j < actorNN->_layers[i]->_nodeCount; j++) actorNN->_layers[i]->_batchNormalizationVariances[j] = actorVarianceSums[i][j] / (_batchNormalizationCorrectionSteps - 1.0);
    }
  }

  /*********************************************************************
  * Updating hyperparameters and broadcasting them to the workers
  *********************************************************************/

  // Updating global configuration for workers to use
  knlohmann::json globals;

  // Getting q training hyperparameters to broadcast to workers
  _hyperparameters = _actorTrainingLearner->getHyperparameters();
  _actorInferenceLearner->setHyperparameters(_hyperparameters);

  // Broadcasting updated globals for all workers to have
  globals["Hyperparameters"] = _hyperparameters;
  _conduit->broadcastGlobals(globals);
}

void DDPG::getAction(Sample &sample)
{
  // Getting actor learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_actorInferenceExperiment._solver);

  // Updating actor learner with the latest hyperparameters
  learner->setHyperparameters(sample.globals()["Hyperparameters"]);

  // Getting current state
  auto state = KORALI_GET(std::vector<double>, sample, "State");

  // Forward-propagating state through actor NN
  auto action = learner->getEvaluation( { state } )[0];

  // Getting optimal action, based on the NN evaluation
  sample["Action"] = action;
}

void DDPG::printGenerationAfter()
{
  // Printing common experience information
  Agent::printGenerationAfter();

  _k->_logger->logInfo("Normal", "DDPG Statistics:\n");
  _k->_logger->logInfo("Normal", " + Experience Memory Size:          %lu/%lu\n", _stateHistory.size(), _replayMemoryMaximumSize);

  _k->_logger->logInfo("Normal", "Optimization Statistics:\n");
  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _k->_logger->logInfo("Normal", "Critic Network Information:\n");
  _criticTrainingExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _criticTrainingExperiment._solver->printGenerationAfter();
  _criticTrainingExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Actor Network Information:\n");
  _actorTrainingExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _actorTrainingExperiment._solver->printGenerationAfter();
  _actorTrainingExperiment._logger->setVerbosityLevel("Silent");
}

std::vector<double> DDPG::getAction(const std::vector<double> &state)
{
  // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_actorInferenceExperiment._solver);

  // Updating learner with the latest hyperparameters
  learner->setHyperparameters(_bestHyperparameters);

  // Forward-propagating state through actor NN
  auto action = learner->getEvaluation( { state } );

  printf("action: %f\n", action[0]);

  // Returning action, based on the NN evaluation
  return action;
}

} // namespace agent

} // namespace solver

} // namespace korali

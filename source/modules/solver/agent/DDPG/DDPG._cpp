#include "modules/solver/agent/DDPG/DDPG.hpp"
#include "engine.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
void DDPG::initializePolicy()
{
  // Checking if a correct action space was defined
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];

    if (_k->_variables[varIdx]->_values.size() > 0) KORALI_LOG_ERROR("Discrete action values (%lu, in total) have been defined for variable %lu (%s), but these are not required for DDPG\n", _k->_variables[varIdx]->_values.size(), varIdx, _k->_variables[varIdx]->_name.c_str());
    if (_k->_variables[varIdx]->_upperBound < _k->_variables[varIdx]->_lowerBound) KORALI_LOG_ERROR("Upper bound for variable %lu (%s) is lower than the lower bound (%f < %f).\n", varIdx, _k->_variables[varIdx]->_name.c_str(), _k->_variables[varIdx]->_upperBound, _k->_variables[varIdx]->_lowerBound);
    if (std::isfinite(_k->_variables[varIdx]->_lowerBound) == false) KORALI_LOG_ERROR("Lower bound for variable %lu (%s) is not finite or has not been specified.\n", varIdx, _k->_variables[varIdx]->_name.c_str());
    if (std::isfinite(_k->_variables[varIdx]->_upperBound) == false) KORALI_LOG_ERROR("Upper bound for variable %lu (%s) is not finite or has not been specified.\n", varIdx, _k->_variables[varIdx]->_name.c_str());
  }

  /*********************************************************************
   * Creating and running Actor Learning Experiments
   *********************************************************************/

  _policyExperiment["Problem"]["Type"] = "Supervised Learning";

  _policyExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _policyExperiment["Solver"]["Batch Normalization"]["Enabled"] = false;
  _policyExperiment["Solver"]["Loss Function"] = "Direct";
  _policyExperiment["Solver"]["Optimizer"] = _policyOptimizer;
  _policyExperiment["Solver"]["Steps Per Generation"] = 1;
  _policyExperiment["Solver"]["Neural Network"] = _policyNeuralNetwork;

  _policyExperiment["Console Output"]["Frequency"] = 0;
  _policyExperiment["Console Output"]["Verbosity"] = "Silent";
  _policyExperiment["File Output"]["Enabled"] = false;
  _policyExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _policyMiniBatchSize; i++)
  {
    for (size_t j = 0; j < _problem->_stateVectorSize; j++)
      _policyExperiment["Problem"]["Inputs"][i][j] = 0.0;

    for (size_t j = 0; j < _problem->_actionVectorSize; j++)
      _policyExperiment["Problem"]["Solution"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  korali::Engine engine;
  engine.initialize(_policyExperiment);

  // Getting learner pointers
  _policyProblem = dynamic_cast<problem::SupervisedLearning *>(_policyExperiment._problem);
  _policyLearner = dynamic_cast<solver::learner::DeepGD *>(_policyExperiment._solver);

  if (_k->_currentGeneration == 0)
  {
    // Get the initial set of policy NN hyperparameters
    _hyperparameters["Policy"] = _policyLearner->getHyperparameters();
  }

  // Storage for statistics
  avgGradients.resize(_problem->_actionVectorSize);

  // Assigning training hyperparameters to inference learner
  _policyLearner->setHyperparameters(_hyperparameters["Policy"]);
}

std::vector<double> DDPG::getTrainingAction(const std::vector<double> &state)
{
  // If we still do not have enough experiences to train the critic/policy, just return random action
  if (_stateHistory.size() < _replayMemoryStartSize) return getRandomAction(state);

  // Obtaining action from policy
  auto action = queryPolicy(state);

  // Checking for correctness of the action
  for (size_t i = 0; i < action.size(); i++)
  {
    if (std::isfinite(action[i]) == false)
      KORALI_LOG_ERROR(" + Action [%lu] is not finite (%f)\n", i, action[i]);
  }

  // Introducing random noise to the action
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];

    // Check whether the variable requires noise
    if (_k->_variables[varIdx]->_explorationNoiseEnabled == true)
    {
      double w = _k->_variables[varIdx]->_explorationNoiseDistribution->getRandomNumber();
      double noise = _k->_variables[varIdx]->_explorationNoiseTheta * _currentActionNoises[i] + w;
      action[i] += noise;
      //printf("Theta (%f) * Previous Noise: (%f) + W: (%f) = %f\n",  _k->_variables[varIdx]->_explorationNoiseTheta, _currentActionNoises[i], w, noise);
      _currentActionNoises[i] = noise;
    }
  }

  // Returning the action
  return action;
}

void DDPG::trainPolicy()
{
  /***********************************************************************************
   * Updating Actor/Policy/Pi Network by performing a Stochastic Gradient Descent step
   ***********************************************************************************/

  // Initializing gradient for statistics
  for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] = 0.0;

  // Creating state history indexes to choose from
  std::vector<size_t> stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) stateHistoryIndexes[i] = i;

  for (size_t i = 0; i < _policyMiniBatchSize; i++)
  {
    // Selecting a uniformly random selected, yet not repeated experience
    size_t expId = stateHistoryIndexes[_stateHistory.size() - _policyMiniBatchSize + i];

    // Storage to put together state and action
    std::vector<double> stateActionInput(_problem->_stateVectorSize + _problem->_actionVectorSize);

    // Now calculating Gradient of q with respect to state+action for the critic NN
    for (size_t j = 0; j < _problem->_stateVectorSize; j++) stateActionInput[j] = _stateHistory[expId][j];
    for (size_t j = 0; j < _problem->_actionVectorSize; j++) stateActionInput[_problem->_stateVectorSize + j] = _actionHistory[expId][j];

    _criticLearner->getEvaluation(stateActionInput);
    auto qGradients = _criticLearner->getGradients({{-1.0}}); // Identity, to estimate the exact gradient dQ/dAS
    std::vector<double> aGradients = std::vector<double>(qGradients.begin() + _problem->_stateVectorSize, qGradients.end());

    // Keeping track of gradient averages for statistics
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] += aGradients[i];

    for (size_t j = 0; j < _problem->_stateVectorSize; j++) _policyProblem->_inputs[i][j] = _stateHistory[expId][j];
    _policyProblem->_solution[i] = aGradients;
  }

  // Calculating gradient statistics
  for (size_t i = 0; i < _problem->_actionVectorSize; i++) avgGradients[i] /= _policyMiniBatchSize;

  // Running one generation of the optimization method on the actor NN with the given mini-batch
  _policyExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _policyExperiment._currentGeneration + _policyOptimizationSteps;
  _policyLearner->initialize();
  korali::Engine engine;
  engine.resume(_policyExperiment);

  auto policyHypeparameters = _policyLearner->getHyperparameters();

  /*********************************************************************
  * Updating hyperparameters and broadcasting them to the workers
  *********************************************************************/

  // Getting q training hyperparameters to broadcast to workers
  _hyperparameters["Policy"] = policyHypeparameters;
  _policyLearner->setHyperparameters(policyHypeparameters);
}

double DDPG::stateValueFunction(const std::vector<double>& state)
{
 auto curPolicyAction = queryPolicy(state);

 double vValue = 0.0;

 for (size_t iter = 0; iter < _stateValueFunctionSamples; iter++)
 {
  // Creating action based on the one given by the policy.
  auto evalAction = curPolicyAction;

  // Storage for the total action probability
  double pAction = 1.0;

  for (size_t i = 0; i < evalAction.size(); i++)
  {
   // Getting noise from the given action variable
   size_t varIdx = _problem->_actionVectorIndexes[i];
   double noise = _k->_variables[varIdx]->_explorationNoiseDistribution->getRandomNumber();

   // Adding noise to the action to evaluate
   evalAction[i] += noise;

   // Calculating the probability density of having produced the noise
   double pNoise = _k->_variables[varIdx]->_explorationNoiseDistribution->getDensity(noise);

   // Factoring the noise probability into the action probability, assuming independence
   pAction *= pNoise;
  }

  // Evaluating Q(s,a) for the given action
  double qValue = stateActionValueFunction(state, evalAction);

  // Now calculating pAction*Q(s,action) and adding it to V(s,a)
  vValue += pAction*qValue;
 }

 // Now averaging the V(s) over all sampled actions
 vValue = vValue / _stateValueFunctionSamples;

 return vValue;
}

double DDPG::getStateActionProbabilityDensity(const std::vector<double>& state, const std::vector<double>& action)
{
 // To calculate the probability density, we first subtract the taken action from the one that
 // the current policy would take. This will give us a value with zero mean, and with sigma
 // equivalent to the noise introduced.

 auto curPolicyAction = queryPolicy(state);

 double pDensity = 1.0;

 for (size_t i = 0; i < action.size(); i++)
 {
  // Getting the difference (noise) between used action and current policy action
  double noise = action[i] - curPolicyAction[i];

  // Getting the probability density of such noise for the current variable.
  size_t varIdx = _problem->_actionVectorIndexes[i];
  double pNoise = _k->_variables[varIdx]->_explorationNoiseDistribution->getDensity(noise);

  // Since we assume independent noise among variables, we can multiply them directly.
  pDensity = pDensity * pNoise;
 }

 return pDensity;
}

std::vector<double> DDPG::queryPolicy(const std::vector<double> &state)
{
  return _policyLearner->getEvaluation({state});
}

void DDPG::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  _policyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<double>>());

  // Re-initializing action noises to zero
  size_t actionCount = _problem->_actionVectorIndexes.size();
  _currentActionNoises.resize(actionCount);
  for (size_t i = 0; i < actionCount; i++) _currentActionNoises[i] = 0.0;
}

void DDPG::printPolicyInformation()
{
  _k->_logger->logInfo("Normal", " + Average Action Gradients: [");

  // Printing gradient averages
  _k->_logger->logData("Normal", " %e", avgGradients[0]);
  for (size_t i = 1; i < _problem->_actionVectorSize; i++) _k->_logger->logData("Normal", ", %e", avgGradients[i]);
  _k->_logger->logData("Normal", " ]\n");
}

} // namespace agent

} // namespace solver

} // namespace korali

{
 "Configuration Settings":
 [
  {
   "Name": [ "Discount Factor" ],
   "Type": "double",
   "Description": "Discount Factor for future rewards."
  }, 
  {
    "Name": [ "Trust Region Divergence Constraint" ],
    "Type": "double",
    "Description": "The constraint on the KL divergence for the calculation and adjustment to the trust region."
  },
  {
    "Name": [ "Off Policy Updates" ],
    "Type": "size_t",
    "Description": "The number of off-policy updates to perform, besides the obligatory on-policy update."
  },
  {
    "Name": [ "Critic", "Neural Network" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the underlying neural network to use for the critic."
  },
  {
    "Name": [ "Critic", "Optimizer" ],
    "Type": "knlohmann::json",
    "Description": "Represents the state and configuration of the solver algorithm for the weights and biases of the critic NN."
  },
  {
    "Name": [ "Policy", "Neural Network" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the underlying neural network to use for the policy."
  },
  {
    "Name": [ "Policy", "Optimizer" ],
    "Type": "knlohmann::json",
    "Description": "Represents the state and configuration of the optimizer for the weights and biases of the policy NN."
  },
  {
   "Name": [ "Policy", "Adoption Rate" ],
   "Type": "double",
   "Description": "The rate at which the new policy parameters are adoped. p_avg = adoptionRate * p_avg + (1 - adoptionRate) * p_new."
  },
  {
    "Name": [ "State Value Function Samples" ],
    "Type": "size_t",
    "Description": "Represents the number of samples from the noise distribution to evaluate for the calculation of V(s)."
  },
  {
   "Name": [ "Importance Weight Truncation" ],
   "Type": "double",
   "Description": "Maximum value (c) for the importance weight (to prevent it from exploding), before multiplying it by Lambda."
  }
 ],
 
 "Results":
 [
 
 ],
 
  "Termination Criteria":
 [

 ],

 "Variables Configuration":
 [
 
 ],

 "Internal Settings":
 [
  {
   "Name": [ "Critic", "Gradients" ],
   "Type": "std::vector<double>",
   "Description": "Stores the cumulative critic gradients for the current generation."
  },
  {
   "Name": [ "Policy", "Gradients" ],
   "Type": "std::vector<double>",
   "Description": "Stores the cumulative policy gradients for the current generation."
  },
  {
   "Name": [ "Policy", "Average Hyperparameters" ],
   "Type": "std::vector<double>",
   "Description": "Stores the average policy hyperparameters."
  },
  {
   "Name": [ "Policy", "Current Hyperparameters" ],
   "Type": "std::vector<double>",
   "Description": "Stores the current policy hyperparameters."
  },
  {
   "Name": [ "Policy", "Probability Function" ],
   "Type": "korali::neuralNetwork::Layer*",
   "Description": "Stores a layer containing the probability function (softmax)."
  }
 ],
 
 "Module Defaults":
 {
  "Trust Region Divergence Constraint": 1.0,
  
  "Critic": 
  {
    "Neural Network": { "Type": "Neural Network" }
  },
   
  "Policy":
  {
    "Neural Network": { "Type": "Neural Network" },
    "Adoption Rate": 0.99,
    "Probability Function":
    {
      "Type": "Layer/Dense",
      "Activation Function":  { "Type": "Softmax" }
    }
  },
  
  "State Value Function Samples": 30,
  "Importance Weight Truncation": 5.0
 },
 
 "Variable Defaults":
 {
 
 }
}

#include "engine.hpp"
#include "modules/solver/agent/ACER/continuous/continuous.hpp"
#include "sample/sample.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
namespace acer
{

void Continuous::initializeAgent()
{
  // Getting discrete problem pointer
  _continuousProblem = dynamic_cast<problem::reinforcementLearning::Continuous *>(_k->_problem);

  // Engine to initialize experiments with
  korali::Engine engine;

 /*********************************************************************
 * Initializing Normal Distributions for the Policy
 *********************************************************************/

 for (size_t i = 0; i < _problem->_actionVectorSize; i++)
 {
  auto varIdx = _continuousProblem->_actionVectorIndexes[i];
  double sigma = _k->_variables[varIdx]->_noiseSigma;
  if (sigma <= 0) KORALI_LOG_ERROR("Value of Noise Sigma (%f) for action variable %lu is not defined or invalid.\n", sigma, i);

  knlohmann::json js;
  js["Type"] = "Univariate/Normal";
  js["Mean"] = 0.0;
  js["Standard Deviation"] =  sigma;

  auto d = dynamic_cast<distribution::univariate::Normal*>(getModule(js, _k));

  _policyDistributions.push_back(d);

  // compute inverse variance
  _inverseVariances[i] = 1.0 / (sigma * sigma);
 }

 /*********************************************************************
 * Initializing V(s)
 *********************************************************************/

  _vExperiment["Problem"]["Type"] = "Supervised Learning";

  _vExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _vExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _vExperiment["Solver"]["Loss Function"] = "Direct";
  _vExperiment["Solver"]["Steps Per Generation"] = 1;
  _vExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;
  _vExperiment["Solver"]["Neural Network"]["Layers"][0]["Node Count"] = _problem->_stateVectorSize;

  _vExperiment["Console Output"]["Frequency"] = 0;
  _vExperiment["Console Output"]["Verbosity"] = "Silent";
  _vExperiment["File Output"]["Enabled"] = false;
  _vExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _problem->_stateVectorSize; i++)
   _vExperiment["Problem"]["Inputs"][0][i] = 0.0;
  _vExperiment["Problem"]["Solution"][0][0] = 0.0; // V(s)

  // Running initialization to verify that the configuration is correct
  engine.initialize(_vExperiment);

  // Getting learner pointers
  _vProblem = dynamic_cast<problem::SupervisedLearning *>(_vExperiment._problem);
  _vLearner = dynamic_cast<solver::learner::DeepGD *>(_vExperiment._solver);

 /*********************************************************************
 * Initializing A(s,a)
 *********************************************************************/

  _aExperiment["Problem"]["Type"] = "Supervised Learning";

  _aExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _aExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _aExperiment["Solver"]["Loss Function"] = "Direct";
  _aExperiment["Solver"]["Steps Per Generation"] = 1;

  _aExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;
  _aExperiment["Solver"]["Neural Network"]["Layers"][0]["Node Count"] = _problem->_stateVectorSize + _problem->_actionVectorSize;

  _aExperiment["Console Output"]["Frequency"] = 0;
  _aExperiment["Console Output"]["Verbosity"] = "Silent";
  _aExperiment["File Output"]["Enabled"] = false;
  _aExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _problem->_stateVectorSize + _problem->_actionVectorSize; i++)
   _aExperiment["Problem"]["Inputs"][0][i] = 0.0;
  _aExperiment["Problem"]["Solution"][0][0] = 0.0; // V(s,a)

  // Running initialization to verify that the configuration is correct
  engine.initialize(_aExperiment);

  // Getting learner pointers
  _aProblem = dynamic_cast<problem::SupervisedLearning *>(_aExperiment._problem);
  _aLearner = dynamic_cast<solver::learner::DeepGD *>(_aExperiment._solver);

  /*********************************************************************
  * Initializing Policy-Related Structures
  *********************************************************************/

  // Creating and running Actor Learning Experiments

  _policyExperiment["Problem"]["Type"] = "Supervised Learning";

  _policyExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _policyExperiment["Solver"]["Loss Function"] = "Direct";
  _policyExperiment["Solver"]["Optimizer"] = _policyOptimizer;
  _policyExperiment["Solver"]["Steps Per Generation"] = 1;
  _policyExperiment["Solver"]["Neural Network"] = _policyNeuralNetwork;

  _policyExperiment["Console Output"]["Frequency"] = 0;
  _policyExperiment["Console Output"]["Verbosity"] = "Silent";
  _policyExperiment["File Output"]["Enabled"] = false;
  _policyExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _problem->_stateVectorSize; i++)
    _policyExperiment["Problem"]["Inputs"][0][i] = 0.0;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    _policyExperiment["Problem"]["Solution"][0][i] = 0.0; // Gaussian Mean

  // Running initialization to verify that the configuration is correct
  engine.initialize(_policyExperiment);

  // Getting learner pointers
  _policyProblem = dynamic_cast<problem::SupervisedLearning *>(_policyExperiment._problem);
  _policyLearner = dynamic_cast<solver::learner::DeepGD *>(_policyExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Getting current hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getHyperparameters();

  // Storing initial average policy parameters
  if (_k->_currentGeneration == 0)
   _policyAverageHyperparameters = _policyCurrentHyperparameters;

  // Get the initial set of policy NN hyperparameters
  _hyperparameters["Policy"] = _policyCurrentHyperparameters;
}

std::vector<double> Continuous::queryPolicy()
{
 // Creating storage for action
 std::vector<double> action(_problem->_actionVectorSize);

 // Sampling value for the action variable for each of the problem's action dimensions
 for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   action[i] = _policyDistributions[i]->getRandomNumber();

 // Returning action
 return action;
}

void Continuous::getAction(korali::Sample &sample)
{
  // Getting current state
  auto state = sample["State"].get<std::vector<double>>();

  // Getting policy distribution parameters from the policy NN
  auto means = _policyLearner->getEvaluation(state);

  // Updating policy distributions
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)  _policyDistributions[i]->_mean = means[i];

  // Querying policy
  auto action = queryPolicy();

  // Storing current policy's mean and standard deviation for this state
  sample["Metadata"]["Means"] = means;

  // Storing action taken
  sample["Action"] = action;
}

double Continuous::stateActionValueFunction(const std::vector<double> &state, const std::vector<double> &action)
{
  // Getting V(s)
  auto V = stateValueFunction(state);

  // Allocating stateAction vector for sampling the advantage function A(x,a)
  std::vector<double> stateActionVector(state.size() + action.size());
  for (size_t i = 0; i < state.size(); i++) stateActionVector[i] = state[i];

  // Getting A(s,a)
  for (size_t i = 0; i < action.size(); i++) stateActionVector[state.size() + i] = action[i];
  auto A = _aLearner->getEvaluation(stateActionVector)[0];

  // Getting policy distribution parameters from the policy NN
  auto means = _policyLearner->getEvaluation(state);

  // Updating policy distributions
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)  _policyDistributions[i]->_mean = means[i];

  // Getting 1/n * sum_n(A(x,u)) with u ~ policy(x)
  double sumA = 0.0;
  for (size_t i = 0; i < _criticSamplingPopulation; i++)
  {
   // Sampling action (u) from the policy
   auto newAction = queryPolicy();

   // Calculating A(x,u) and adding it to the sum
   for (size_t i = 0; i < action.size(); i++) stateActionVector[state.size() + i] = newAction[i];
   sumA += _aLearner->getEvaluation(stateActionVector)[0];
  }

  // returning Qhat
  return V + A - sumA / (double)_criticSamplingPopulation;
}

double Continuous::stateValueFunction(const std::vector<double> &state)
{
 // Forward propagating state through the critic to get V(s)
 auto V = _vLearner->getEvaluation(state);

 // Returning V(s)
 return V[0];
}

void Continuous::processTrajectory(size_t startId, size_t endId)
{
   // Initializing inputs and solution for the critic and policy problems
   _vProblem->_inputs.clear();
   _vProblem->_solution.clear();
   _aProblem->_inputs.clear();
   _aProblem->_solution.clear();
   _policyProblem->_inputs.clear();
   _policyProblem->_solution.clear();

   // Going straight to the latest experience in the corresponding trajectory
   size_t curId = endId;

   // First, get the retrace value for the last experience (just the reward)
   double qRet = _experienceReplayHistory[curId]["Reward"];
   if (_experienceReplayHistory[curId]["Is Terminal"] == false)
   {
    std::vector<double> expState = _experienceReplayHistory[curId]["State"];
    qRet = stateValueFunction(expState);
   }

   // Setting Qopc, which is Qret without the truncated importance ratio
   double qOpc = qRet;

   while (curId > startId)
   {
    // Decreasing current experience index
    curId--;

    // Getting current reward
    double curReward = _experienceReplayHistory[curId]["Reward"];

    // Updating the value of qRet for this experience
    qRet = curReward + _discountFactor * qRet;
    qOpc = curReward + _discountFactor * qOpc;

    // Getting experience's state
    std::vector<double> expState = _experienceReplayHistory[curId]["State"];

    // Calculating V(state) with the current policy
    double vexpState = stateValueFunction(expState);

    // Getting experience's action
    std::vector<double> expAction = _experienceReplayHistory[curId]["Action"];

    // Getting policy distribution parameters from the current policy NN
    auto curMeans = _policyLearner->getEvaluation(expState);

    // Setting policy distributions to follow that of the current policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)  _policyDistributions[i]->_mean = curMeans[i];

    // Getting new action given the current policy
    auto newAction = queryPolicy();

    // Getting policy distribution parameters from the old policy NN
    std::vector<double> oldMeans = _experienceReplayHistory[curId]["Metadata"]["Means"];

    // Setting policy distributions to follow that of the old policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)  _policyDistributions[i]->_mean = oldMeans[i];

    // Getting p(s,a) for selected experience, given the old policy
    double pexpActionOldPolicy = 1.0;
    double pNewActionOldPolicy = 1.0;
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      pexpActionOldPolicy *= _policyDistributions[i]->getDensity(expAction[i]);
      pNewActionOldPolicy *= _policyDistributions[i]->getDensity(newAction[i]);
    }

    // Setting policy distributions back the current policy
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)  _policyDistributions[i]->_mean = curMeans[i];

    // Getting p(s,a) for the best action, given the current policy
    double pexpActionCurPolicy = 1.0;
    double pNewActionCurPolicy = 1.0;
    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
     pexpActionCurPolicy *= _policyDistributions[i]->getDensity(expAction[i]);
     pNewActionCurPolicy *= _policyDistributions[i]->getDensity(newAction[i]);
    }

    // Now calculating importance weight for s,a (old) and the s, a' (new)
    double oldImportanceWeight = pexpActionCurPolicy / pexpActionOldPolicy;
    double newImportanceWeight = pNewActionCurPolicy / pNewActionOldPolicy;

    /*****************************************
     * Policy Section
     *****************************************/

    // Now calculating truncated importance weight
    double truncatedImportanceWeight = std::min(_importanceWeightTruncation, oldImportanceWeight);

    // First part of the gradient vector
    double g1 = -truncatedImportanceWeight * (qOpc - vexpState);

    // Calculate Qtilde of the new action
    double qTilde = stateActionValueFunction(expState, newAction);

    // Now calculating r(a) - c / r(a) ratio
    double importanceRatio = 1.0 - (_importanceWeightTruncation / newImportanceWeight);

    // Now calculating the correction weight
    double correctionWeight = importanceRatio > 0.0 ? importanceRatio : 0.0;

    // Compute the expectation for the second part of the gradient vector
    double g2 = -correctionWeight * (qTilde - vexpState);

    // Now Calculating Acer gradient for all of the action's variables (mean and sigma)
    std::vector<double> gPolicy(_problem->_actionVectorSize, 0.0);
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) gPolicy[i] = (g1+g2)*_inverseVariances[i]*(expAction[i] - curMeans[i]);

    ///////////////// KL Calculation /////////////////////

    // Getting policy distribution parameters given the average policy
    _policyLearner->setHyperparameters(_policyAverageHyperparameters);

    // Getting policy distribution parameters from the average policy NN
    auto avgMeans = _policyLearner->getEvaluation(expState);

    // Obtaining KL Divergence gradients for the current state.
    std::vector<double> k(_problem->_actionVectorSize);
    for(size_t i = 0; i < _problem->_actionVectorSize; i++)  k[i] = _inverseVariances[i]*(curMeans[i] - avgMeans[i]);

     // Getting dot product between the gradient vector and k
     auto gkDotProduct = dotProduct(k, gPolicy);

     // Getting norm(k)^2, simply by dot product of k and itself
     auto kNormSquared = dotProduct(k, k);

     // Getting magnitude of adjustment
     double adjustmentMagnitude = std::max(0.0, (gkDotProduct - _trustRegionDivergenceConstraint) / kNormSquared);

     // Adjusting gradients to trust region
     for(size_t i = 0; i < gPolicy.size(); i++) gPolicy[i] = gPolicy[i] - adjustmentMagnitude * k[i];

     _policyProblem->_inputs.push_back(expState);
     _policyProblem->_solution.push_back(gPolicy);

     /*****************************************
     * Update Networks for Q estimation
     *****************************************/

    // Getting Q(s, a) for the selected experience
    double qExpState = stateActionValueFunction(expState, expAction);

    ///////////////// Gradients for the State Function V(s) /////////////////

      // Update the gradient vector with the value of the retrace function for g  = Q-Qret
      double gradV = ( qRet - qExpState );

      // Updating inputs to training learner
      _vProblem->_inputs.push_back(expState);
      _vProblem->_solution.push_back({ gradV });

      _cumulativeQStar += gradV;

    ///////////////// Gradients for the Advantage Function A(s) /////////////////

      // Now calculating unnormalized truncated importance weight with 1.0 as truncation factor
      double unnormalizedTruncatedImportance = std::min(1.0, oldImportanceWeight);

      // Update the gradient vector with the value of the retrace function for g  = Q-Qret
      double gradA = unnormalizedTruncatedImportance * ( qRet - qExpState );

      // Putting together state and action
      std::vector<double> stateActionVector(expState.size() + expAction.size());
      for (size_t i = 0; i < expState.size(); i++) stateActionVector[i] = expState[i];
      for (size_t i = 0; i < expAction.size(); i++) stateActionVector[expState.size() + i] = expAction[i];

      // Updating inputs to training learner
      _aProblem->_inputs.push_back(stateActionVector);
      _aProblem->_solution.push_back({ gradA });

     /*****************************************
      * Updating the value of qRet and qOpc
      *****************************************/

     // Now calculating truncated importance weight with 1.0 as truncation factor
     truncatedImportanceWeight = std::min(1.0, std::pow(oldImportanceWeight, 1.0 / _problem->_actionVectorSize));

     // Updating qRet and qOpc
     qRet = truncatedImportanceWeight * (qRet - qExpState) + vexpState;
     qOpc = (qOpc - qExpState) + vexpState;
   }

   // Declaring engine to launch experiments
   korali::Engine engine;

   // Running one generation of the optimization method with the given mini-batch
   _aExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _aExperiment._currentGeneration + 1;
   _aLearner->initialize();
   engine.resume(_aExperiment);

   // Running one generation of the optimization method with the given mini-batch
   _vExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _vExperiment._currentGeneration + 1;
   _vLearner->initialize();
   engine.resume(_vExperiment);

   // Running one generation of the optimization method with the given mini-batch
   _policyExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _policyExperiment._currentGeneration + 1;
   _policyLearner->initialize();
   engine.resume(_policyExperiment);

  /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

   // Randomly selecting experiences for the mini-batch and calculating their target Q
   // Creating state history indexes to choose from
   std::vector<size_t> experienceReplayIndexes(_experienceReplayHistory.size());
   for (size_t i = 0; i < _experienceReplayHistory.size()-1; i++) experienceReplayIndexes[i] = i;

   // Allocating memory for the mini batch set
   std::vector<std::vector<std::vector<double>>> stateMiniBatches(_normalizationSteps);
   std::vector<std::vector<std::vector<double>>> stateActionMiniBatches(_normalizationSteps);

   for (size_t i = 0; i < _normalizationSteps; i++)
   {
     stateMiniBatches[i].resize(_normalizationBatchSize);
     stateActionMiniBatches[i].resize(_normalizationBatchSize);
   }

   for (size_t i = 0; i < _normalizationSteps; i++)
     for (size_t j = 0; j < _normalizationBatchSize; j++)
     {
       stateMiniBatches[i][j].resize(_problem->_stateVectorSize);
       stateActionMiniBatches[i][j].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);
     }

   // Filling the minibatches
   for (size_t step = 0; step < _normalizationSteps; step++)
   {
     // Shuffling indexes to choose the mini batch from
     std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

     for (size_t i = 0; i < _normalizationBatchSize; i++)
     {
       // Selecting a uniformly random selected, yet not repeated experience
       size_t expId = experienceReplayIndexes[i];

       auto expState = _experienceReplayHistory[expId]["State"].get<std::vector<double>>();
       auto curAction = _experienceReplayHistory[expId]["Action"].get<std::vector<double>>();

       for (size_t j = 0; j < _problem->_stateVectorSize; j++)
        {
         stateMiniBatches[step][i][j] = expState[j];
         stateActionMiniBatches[step][i][j] = expState[j];
        }

       for (size_t j = 0; j < _problem->_actionVectorSize; j++)
        stateActionMiniBatches[step][i][_problem->_stateVectorSize + j] = curAction[j];
     }
   }

   _aLearner->_trainingNeuralNetwork->normalize(stateActionMiniBatches);
   _vLearner->_trainingNeuralNetwork->normalize(stateMiniBatches);
   _policyLearner->_trainingNeuralNetwork->normalize(stateMiniBatches);

   // Storing new inference parameters
   auto aHyperparameters = _aLearner->getHyperparameters();
   _aLearner->setHyperparameters(aHyperparameters);

   auto vHyperparameters = _vLearner->getHyperparameters();
   _vLearner->setHyperparameters(vHyperparameters);

  /****************************************************************************
  * Updating Policy
  ******************************************************************************/

   // Getting new policy hyperparameters
   _policyCurrentHyperparameters = _policyLearner->getHyperparameters();

   // Softly adopting the new parameters, using an adoption rate
   for (size_t i = 0; i < _policyCurrentHyperparameters.size(); i++)
    _policyAverageHyperparameters[i] = _policyAdoptionRate*_policyAverageHyperparameters[i] + (1-_policyAdoptionRate)*_policyCurrentHyperparameters[i];

   // Updating policy with new averaged parameters
   _policyLearner->setHyperparameters(_policyCurrentHyperparameters);

   // Storing average policy hyperparameters
   _hyperparameters["Policy"] = _policyCurrentHyperparameters;
}

void Continuous::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  _policyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<double>>());
}

void Continuous::printAgentInformation()
{
  // Updating average Q*, for statistical purposes
  _averageQStar = _cumulativeQStar / ((_offPolicyUpdates + 1) * _aProblem->_solution.size());

  _k->_logger->logInfo("Normal", "Critic Information:\n");

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _aExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _aExperiment._solver->printGenerationAfter();
  _aExperiment._logger->setVerbosityLevel("Silent");

  _vExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _vExperiment._solver->printGenerationAfter();
  _vExperiment._logger->setVerbosityLevel("Silent");

   // Resetting cumulative Q*, for statistical purposes
   _cumulativeQStar = 0.0;
}

} // namespace acer
} // namespace agent
} // namespace solver
} // namespace korali

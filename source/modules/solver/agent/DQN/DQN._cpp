#include "engine.hpp"
#include "modules/solver/agent/DQN/DQN.hpp"
#include "sample/sample.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{
namespace agent
{
void DQN::initializeAgent()
{

 /*********************************************************************
  * Verifying Problem's Definition
  *********************************************************************/

 for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
 {
   size_t varIdx = _problem->_actionVectorIndexes[i];
   if (_k->_variables[varIdx]->_values.size() == 0) KORALI_LOG_ERROR("No discrete action values have been defined for variable %lu (%s), required by DQN\n", varIdx, _k->_variables[varIdx]->_name.c_str());
 }

 /*********************************************************************
 * Initializing Critic-Related Structures
 *********************************************************************/

 korali::Engine engine; // Engine to initialize experiments with

 _criticExperiment["Problem"]["Type"] = "Supervised Learning";

 _criticExperiment["Solver"]["Type"] = "Learner/DeepGD";
 _criticExperiment["Solver"]["Optimizer"] = _criticOptimizer;
 _criticExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
 _criticExperiment["Solver"]["Steps Per Generation"] = 1;
 _criticExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

 _criticExperiment["Console Output"]["Frequency"] = 0;
 _criticExperiment["Console Output"]["Verbosity"] = "Silent";
 _criticExperiment["File Output"]["Enabled"] = false;
 _criticExperiment["Random Seed"] = _k->_randomSeed++;

 // Initializing experiment with an initial zero set
 for (size_t i = 0; i < _criticMiniBatchSize; i++)
 {
   _criticExperiment["Problem"]["Solution"][i][0] = 0.0;

   for (size_t j = 0; j < _k->_variables.size(); j++)
     _criticExperiment["Problem"]["Inputs"][i][j] = 0.0;
 }

 // Running initialization to verify that the configuration is correct
 engine.initialize(_criticExperiment);

 // Getting learner pointers
 _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
 _criticLearner = dynamic_cast<solver::learner::DeepGD *>(_criticExperiment._solver);

  /*********************************************************************
  * Initializing Policy-Related Structures
  *********************************************************************/

  // Creating Q-Maximizing argmax(Action) Experiment
  // Creating evaluation lambda function for optimization
  auto fc = [this](Sample &sample) { this->evaluateAction(sample, true); };

  _policyExperiment["Problem"]["Type"] = "Optimization";
  _policyExperiment["Problem"]["Objective Function"] = fc;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];

    _policyExperiment["Variables"][i]["Name"] = _k->_variables[varIdx]->_name;
    _policyExperiment["Variables"][i]["Lower Bound"] = _k->_variables[varIdx]->_lowerBound;
    _policyExperiment["Variables"][i]["Upper Bound"] = _k->_variables[varIdx]->_upperBound;

    double initialGuess = (_k->_variables[varIdx]->_upperBound + _k->_variables[varIdx]->_lowerBound) * 0.5;
    double initialStdDev = abs((_k->_variables[varIdx]->_upperBound - _k->_variables[varIdx]->_lowerBound));

    _policyExperiment["Variables"][i]["Initial Value"] = initialGuess;
    _policyExperiment["Variables"][i]["Initial Mean"] = initialGuess;
    _policyExperiment["Variables"][i]["Initial Standard Deviation"] = initialStdDev;
    _policyExperiment["Variables"][i]["Values"] = _k->_variables[varIdx]->_values;
  }

  _policyExperiment["Solver"] = _policyOptimizer;

  _policyExperiment["Console Output"]["Frequency"] = 0;
  _policyExperiment["Console Output"]["Verbosity"] = "Silent";
  _policyExperiment["File Output"]["Enabled"] = false;
  _policyExperiment["Random Seed"] = _k->_randomSeed++;

  // Running initialization to verify that the configuration is correct
  engine.initialize(_policyExperiment);

  // If initial generation, set initial DQN configuration
  if (_k->_currentGeneration == 0)
  {
    // Setting intial epsilon to 1.0 (initial exploratory phase)
    _policyEpsilonInitialized = false;
    _policyEpsilonCurrentValue = 1.0;

    // Settting Epsilon
    _hyperparameters["Epsilon"] = _policyEpsilonCurrentValue;
    _hyperparameters["Critic"] = _criticLearner->getHyperparameters();
  }

  // Setting Critic's hyperparameters
  _criticLearner->setHyperparameters(_hyperparameters["Critic"]);
}

double DQN::stateActionValueFunction(const std::vector<double> &state, const std::vector<double> &action)
{
  // Storage to put together state and action
  std::vector<double> stateActionInput(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Forward propagating state/action through the critic
  for (size_t j = 0; j < state.size(); j++) stateActionInput[j] = state[j];
  for (size_t j = 0; j < action.size(); j++) stateActionInput[j + state.size()] = action[j];
  auto evaluation = _criticLearner->getEvaluation(stateActionInput);

  // Getting the value of V(Xt), i.e., Q(xt, best action)
  return evaluation[0];
}

void DQN::trainAgent()
{
 /***********************************************************************************
   * Critic Training Phase
   **********************************************************************************/

   // Randomly selecting experiences for the mini-batch and calculating their target Q
   // Creating state history indexes to choose from
   std::vector<size_t> experienceReplayIndexes(_experienceReplayHistory.size());
   for (size_t i = 0; i < _experienceReplayHistory.size(); i++) experienceReplayIndexes[i] = i;

   // Calculating cumulative Q*, for statistical purposes
   _cumulativeQStar = 0.0;

   for (size_t step = 0; step < _criticOptimizationSteps; step++)
   {
     // Returning hyperparameters to its pre-training value
     _criticLearner->setHyperparameters(_hyperparameters["Critic"]);

     // Shuffling indexes to choose the mini batch from
     std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

     for (size_t i = 0; i < _criticMiniBatchSize; i++)
     {
       // Selecting a uniformly random selected, yet not repeated experience
       size_t expId = experienceReplayIndexes[i];

       // Qnew = max_a(q) with s' fixed
       // Q* = r + y*Qnew -- If not terminal state
       // Q* = r -- If terminal state

       // Calculating target Q value (solution) for Qnew on selected batch
       double qStar = 0.0;

       // Reward is the first factor in the QLearning algorithm
       qStar = _experienceReplayHistory[expId]["Reward"];

       // Getting experience's current state and action
       std::vector<double> curState = _experienceReplayHistory[expId]["State"];
       std::vector<double> curAction = _experienceReplayHistory[expId]["Action"];

       // If state is not terminal (next state is filled) then add Qnew to the Q value.
       if (_experienceReplayHistory[expId]["Is Terminal"] == false)
       {
        // Getting experience's next state and action
        std::vector<double> nextState = _experienceReplayHistory[expId+1]["State"];
        std::vector<double> nextAction = _experienceReplayHistory[expId+1]["Action"];

        qStar += _criticDiscountFactor * stateActionValueFunction(nextState, nextAction);
       }

       // Updating inputs to training learner
       _criticProblem->_inputs[i] = curState;
       _criticProblem->_inputs[i].insert(_criticProblem->_inputs[i].end(), curAction.begin(), curAction.end());
       _criticProblem->_solution[i][0] = qStar;

       // Keeping statistics
       _cumulativeQStar += qStar;
     }

     // Running one generation of the optimization method with the given mini-batch
     _criticExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _criticExperiment._currentGeneration + 1;
     _criticLearner->initialize();
     korali::Engine engine;
     engine.resume(_criticExperiment);

     // Increasing optimization step counter
     _optimizationStepCount++;
   }

   // Keeping statistics
   _averageQStar = (double)_cumulativeQStar / (double)(_criticOptimizationSteps * _criticMiniBatchSize);

   /****************************************************************************
  * If batch normalization is being used, we need to adjust mean and variances
  * by sampling a few more mini-batches after the optimization steps
  ******************************************************************************/

   normalizeNeuralNetwork(_criticLearner->_trainingNeuralNetwork);

   // Storing new Critic's hyperparameters
   _hyperparameters["Critic"] = _criticLearner->getHyperparameters();


  // Initializing the value of epsilon, if not already set
  if (_policyEpsilonInitialized == false)
  {
    _policyEpsilonCurrentValue = _policyEpsilonInitialValue;
    _policyEpsilonInitialized = true;
  }
  else
  {
    // Decreasing the value of epsilon
    _policyEpsilonCurrentValue = _policyEpsilonCurrentValue - _policyEpsilonDecreaseRate;

    // Without exceeding the lower bound
    if (_policyEpsilonCurrentValue < _policyEpsilonTargetValue) _policyEpsilonCurrentValue = _policyEpsilonTargetValue;
  }

  // Updating Epsilon
  _hyperparameters["Epsilon"] = _policyEpsilonCurrentValue;
}

void DQN::evaluateAction(Sample &sample, bool getGradients)
{
  // Setting weights and biases
  auto action = KORALI_GET(std::vector<double>, sample, "Parameters");
  auto state = sample.globals()["Current State"].get<std::vector<double>>();

  // Creating state/action input
  std::vector<double> stateActionInput(state.size() + action.size());
  for (size_t j = 0; j < state.size(); j++) stateActionInput[j] = state[j];
  for (size_t j = 0; j < action.size(); j++) stateActionInput[j + state.size()] = action[j];

  auto evaluation = _criticLearner->getEvaluation(stateActionInput);
  auto gradients = _criticLearner->getGradients({{-1.0}});

  sample["F(x)"] = evaluation[0];

  for (size_t i = 0; i < action.size(); i++)
  {
    size_t startIdx = state.size();
    sample["Gradient"][i] = gradients[startIdx + i];
  }
}

void DQN::getAction(korali::Sample& sample)
{
  // If we still do not have enough experiences to train the critic/policy, just return random action
  if (_experienceReplayHistory.size() < _experienceReplayStartSize)
  {
   sample["Action"] = getRandomAction();
   return;
  }

  // If currently on a training phase, add noise/random actions.
  bool isTraining = sample["Mode"] == "Training";

  if (isTraining)
  {
   // Getting p = U[0,1] for the epsilon strategy
   double p = _policyEpsilonGenerator->getRandomNumber();

   // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
   // If no states have been registered yet, use the random option in any case
   if (p < _policyEpsilonCurrentValue)
   {
    sample["Action"] = getRandomAction();
    return;
   }
  }

  // Getting current state
  auto state = sample["State"].get<std::vector<double>>();

  // Obtaining action from policy
  auto action = queryPolicy(state);

  // Storing the action
  sample["Action"] = action;
}

void DQN::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  _criticLearner->setHyperparameters(hyperparameters["Critic"].get<std::vector<double>>());
  _policyEpsilonCurrentValue = hyperparameters["Epsilon"].get<double>();
}

std::vector<double> DQN::queryPolicy(const std::vector<double> &state)
{
  _policyExperiment._globals["Current State"] = state;

  // Running optimization experiment to get best estimated action
  korali::Engine engine;
  engine.run(_policyExperiment);

  // Getting optimal action, based on the NN evaluation
  return _policyExperiment["Results"]["Best Sample"]["Parameters"].get<std::vector<double>>();
}

void DQN::printAgentInformation()
{
 _k->_logger->logInfo("Normal", "Critic Information:\n");

 if (_maxOptimizationSteps > 0)
   _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
 else
   _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

 _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
 _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

 _criticExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
 _criticExperiment._solver->printGenerationAfter();
 _criticExperiment._logger->setVerbosityLevel("Silent");

 _k->_logger->logInfo("Normal", "Policy Information:\n");

 _k->_logger->logInfo("Normal", " + Current Epsilon:                  %f -> %f\n", _policyEpsilonCurrentValue, _policyEpsilonTargetValue);
}

} // namespace agent

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  /*********************************************************************
  * Creating and running Critic Learning Experiments
  *********************************************************************/

  _criticExperiment["Problem"]["Type"] = "Supervised Learning";

  _criticExperiment["Solver"]["Type"] = "Learner/DeepGD";
  _criticExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
  _criticExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  _criticExperiment["Console Output"]["Frequency"] = 0;
  _criticExperiment["Console Output"]["Verbosity"] = "Silent";
  _criticExperiment["File Output"]["Enabled"] = false;
  _criticExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _criticMiniBatchSize; i++)
  {
    _criticExperiment["Problem"]["Solution"][i][0] = 0.0;

    for (size_t j = 0; j < _k->_variables.size(); j++)
      _criticExperiment["Problem"]["Inputs"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_criticExperiment);

  // Getting learner pointers
  _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
  _criticLearner = dynamic_cast<solver::learner::DeepGD *>(_criticExperiment._solver);

  // Initializing selected policy
  initializePolicy();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _suboptimalStepCounter = 0;
    _optimizationStepCount = 0;

    // Initialize best reward
    _bestAverageReward = -korali::Inf;

    // Getting Critic's hyperparameters
    _hyperparameters["Critic"] = _criticLearner->getHyperparameters();

    // Setting intial epsilon to 1.0 (initial exploratory phase)
    _policyEpsilonInitialized = false;
    _policyEpsilonCurrentValue = 1.0;

    // Get the initial set of hyperparameters
    _bestHyperparameters = _hyperparameters;
  }

  // Setting Critic's hyperparameters
  _criticLearner->setHyperparameters(_hyperparameters["Critic"]);

  // Settting Epsilon
  _hyperparameters["Epsilon"] = _policyEpsilonCurrentValue;

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting hyperparameters for all workers to use
  KORALI_BROADCAST("Hyperparameters", _hyperparameters);

  // Creating storage for agents
  std::vector<Sample> agents(_agentEpisodesPerGeneration);
  _currentEpisode += _agentEpisodesPerGeneration;

  // Initializing the agents and their environments
  for (size_t i = 0; i < _agentEpisodesPerGeneration; i++)
  {
    // Configuring Agent
    agents[i]["Sample Id"] = _k->_currentGeneration * _agentEpisodesPerGeneration + i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";

    // Launching agent initialization
    KORALI_START(agents[i]);
  }

  KORALI_WAITALL(agents);

  /*********************************************************************
   * Storing new experiences to the history
   *********************************************************************/

  size_t currentExperienceCount = 0;

  _maxExperienceCount = 0;
  _minExperienceCount = std::numeric_limits<size_t>::max();

  // Calculating the cumulative reward of this round of experiences
  double cumulativeReward = 0.0;

  for (size_t i = 0; i < _agentEpisodesPerGeneration; i++)
  {
    size_t experienceSize = agents[i]["Experience"]["States"].size();

    // Collecting entire episode's reward
    cumulativeReward += agents[i]["Cumulative Reward"].get<double>();

    // Storing statistics
    currentExperienceCount += experienceSize;
    if (experienceSize > _maxExperienceCount) _maxExperienceCount = experienceSize;
    if (experienceSize < _minExperienceCount) _minExperienceCount = experienceSize;

    size_t startExpId = experienceSize > _agentExperienceLimit ? experienceSize - _agentExperienceLimit : 0;

    for (size_t j = startExpId; j < experienceSize; j++)
    {
      // Storing action and state experience
      _stateHistory.push_back(agents[i]["Experience"]["States"][j].get<std::vector<double>>());
      _actionHistory.push_back(agents[i]["Experience"]["Actions"][j].get<std::vector<double>>());

      // Storing experience's reward
      double reward = agents[i]["Experience"]["Rewards"][j].get<double>();
      _rewardHistory.push_back(reward);

      // Storing whether this experience is terminal
      _terminalHistory.push_back(j == (experienceSize - 1));
    }
  }

  // Updating average experience count
  _totalExperienceCount += currentExperienceCount;
  _averageExperienceCount = ((double)currentExperienceCount) / ((double)_agentEpisodesPerGeneration);

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_stateHistory.size() > _replayMemoryMaximumSize)
  {
    size_t excess = _stateHistory.size() - _replayMemoryMaximumSize;
    _stateHistory.erase(_stateHistory.begin(), _stateHistory.begin() + excess);
    _actionHistory.erase(_actionHistory.begin(), _actionHistory.begin() + excess);
    _rewardHistory.erase(_rewardHistory.begin(), _rewardHistory.begin() + excess);
    _terminalHistory.erase(_terminalHistory.begin(), _terminalHistory.begin() + excess);
  }

  // If the minimum number of experiences have been reached update the algorithm-specific method
  if (_stateHistory.size() >= _replayMemoryStartSize)
  {
    trainCritic();
    trainPolicy();

    // Initializing the value of epsilon, if not already set
    if (_policyEpsilonInitialized == false)
    {
      _policyEpsilonCurrentValue = _policyEpsilonInitialValue;
      _policyEpsilonInitialized = true;
    }
    else
    {
      // Decreasing the value of epsilon
      _policyEpsilonCurrentValue = _policyEpsilonCurrentValue - _policyEpsilonDecreaseRate;

      // Without exceeding the lower bound
      if (_policyEpsilonCurrentValue < _policyEpsilonTargetValue) _policyEpsilonCurrentValue = _policyEpsilonTargetValue;
    }

    // Updating Epsilon
    _hyperparameters["Epsilon"] = _policyEpsilonCurrentValue;

    // Storing new Critic's hyperparameters
    _hyperparameters["Critic"] = _criticLearner->getHyperparameters();

    // Updating best reward so far, it the cumulative reward of this round exceeds it
    // and store the best hyperparameters used for that
    _averageReward = cumulativeReward / (double)_agentEpisodesPerGeneration;
    if (_averageReward > _bestAverageReward)
    {
      _bestAverageReward = _averageReward;
      _bestHyperparameters = _hyperparameters;
      _suboptimalStepCounter = 0;
    }
    else
    {
      // If not better, then increase the suboptimal step counter
      _suboptimalStepCounter++;
    }
  }
}

std::vector<double> Agent::getTrainingAction(const std::vector<double> &state)
{
  // Getting p = U[0,1] for the epsilon strategy
  double p = _uniformGenerator->getRandomNumber();

  // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
  // If no states have been registered yet, use the random option in any case
  if (p < _policyEpsilonCurrentValue)
  {
    // Storage for action
    std::vector<double> randomAction(_problem->_actionVectorSize);

    for (size_t i = 0; i < _problem->_actionVectorSize; i++)
    {
      size_t varIdx = _problem->_actionVectorIndexes[i];
      double x = _uniformGenerator->getRandomNumber();

      // If discrete value vector was not provided, use lower and upper bounds
      if (_k->_variables[varIdx]->_values.size() == 0)
      {
        double lowerBound = _k->_variables[varIdx]->_lowerBound;
        double upperBound = _k->_variables[varIdx]->_upperBound;
        randomAction[i] = lowerBound + x * (upperBound - lowerBound);
      }
      else
      {
        // Randomly select one of the actions provided in the value vector
        size_t valIdx = floor(x * _k->_variables[varIdx]->_values.size());
        randomAction[i] = _k->_variables[varIdx]->_values[valIdx];
      }
    }

    return randomAction;
  }

  // Obtaining action from policy
  auto action = queryPolicy(state);

  // Checking for correctness of the action
  for (size_t i = 0; i < action.size(); i++)
  {
    if (std::isfinite(action[i]) == false)
      KORALI_LOG_ERROR(" + Action [%lu] is not finite (%f)\n", i, action[i]);
  }

  // Introducing random noise to the action
  for (size_t i = 0; i < _problem->_actionVectorIndexes.size(); i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];

    // Check whether the variable requires noise
    if (_k->_variables[varIdx]->_explorationNoiseEnabled == true)
    {
      double w = _k->_variables[varIdx]->_explorationNoiseDistribution->getRandomNumber();
      double noise = _k->_variables[varIdx]->_explorationNoiseTheta * _currentActionNoises[i] + w;
      action[i] += noise;
      //printf("Theta (%f) * Previous Noise: (%f) + W: (%f) = %f\n",  _k->_variables[varIdx]->_explorationNoiseTheta, _currentActionNoises[i], w, noise);
      _currentActionNoises[i] = noise;
    }
  }

  // Returning the action
  return action;
}

void Agent::trainCritic()
{
  /***********************************************************************************
  * Randomly selecting experiences for the mini-batch and calculating their target Q
  ***********************************************************************************/

  // Creating state history indexes to choose from
  std::vector<size_t> stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) stateHistoryIndexes[i] = i;

  // Calculating cumulative Q*, for statistical purposes
  _cumulativeQStar = 0.0;

  for (size_t step = 0; step < _criticOptimizationSteps; step++)
  {
    // Returning hyperparameters to its pre-training value
    _criticLearner->setHyperparameters(_hyperparameters["Critic"]);

    // Shuffling indexes to choose the mini batch from
    std::shuffle(stateHistoryIndexes.begin(), stateHistoryIndexes.end(), *mt);

    for (size_t i = 0; i < _criticMiniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = stateHistoryIndexes[i];

      // Qnew = max_a(q) with s' fixed
      // Q* = r + y*Qnew -- If not terminal state
      // Q* = r -- If terminal state

      // Calculating target Q value (solution) for Qnew on selected batch
      double qStar = _rewardHistory[expId];

      // If state is not terminal (next state is filled) then add Qnew to the Q value.
      if (_terminalHistory[expId] == false)
      {
        // Updating current state
        auto nextState = _stateHistory[expId + 1];

        // Getting optimal action for the next state, based on the NN evaluation
        auto action = queryPolicy(nextState);

        // Storage to put together state and action
        std::vector<double> stateActionInput(_problem->_stateVectorSize + _problem->_actionVectorSize);

        // Forward propagating state/action through the critic inference NN
        for (size_t j = 0; j < nextState.size(); j++) stateActionInput[j] = nextState[j];
        for (size_t j = 0; j < action.size(); j++) stateActionInput[j + nextState.size()] = action[j];
        auto evaluation = _criticLearner->getEvaluation(stateActionInput);

        // Getting the value of V(Xt+1), i.e., Q(xt+1, best action)
        auto V = evaluation[0];

        // If using retrace, we calculate it's value here
        double qRetrace = 0.0;
        //if (_retraceEnabled == true) calculateRetrace(expId);

        // We add the expected remaining reward based on the optimization
        qStar += _criticDiscountFactor * V;

        // Debug only, print new experience values
        //  printf("State: %f %f %f %f \n", _stateHistory[expId][0], _stateHistory[expId][1], _stateHistory[expId][2], _stateHistory[expId][3]);
        //  printf("Action: %f\n", _actionHistory[expId][0]);
        //  printf("Reward: %f\n", _rewardHistory[expId]);
        //  printf("New State: %f %f %f %f\n", _stateHistory[expId+1][0], _stateHistory[expId+1][1], _stateHistory[expId+1][2], _stateHistory[expId+1][3]);
        //  printf("New Action: %f\n", action[0]);
        //  printf("V: %f\n", V);
        //  printf("Q* %f\n", qStar);
      }

      // Updating inputs to training learner
      for (size_t j = 0; j < _problem->_stateVectorSize; j++) _criticProblem->_inputs[i][j] = _stateHistory[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) _criticProblem->_inputs[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
      _criticProblem->_solution[i][0] = qStar;

      // Keeping statistics
      _cumulativeQStar += qStar;
    }

    // Running one generation of the optimization method with the given mini-batch
    _criticExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _criticExperiment._currentGeneration + 1;
    _criticLearner->initialize();
    _engine.resume(_criticExperiment);

    // Increasing optimization step counter
    _optimizationStepCount++;
  }

  // Keeping statistics
  _averageQStar = (double)_cumulativeQStar / (double)(_criticOptimizationSteps * _criticMiniBatchSize);

  /****************************************************************************
 * If batch normalization is being used, we need to adjust mean and variances
 * by sampling a few more mini-batches after the optimization steps
 ******************************************************************************/

  normalizeNeuralNetwork(_criticLearner->_trainingNeuralNetwork);
}

void Agent::normalizeNeuralNetwork(korali::NeuralNetwork *nn)
{
  size_t pCount = nn->_normalizationParameterCount;

  std::vector<double> newMeans(pCount, 0.0);
  std::vector<double> newVariances(pCount, 0.0);

  std::vector<std::vector<double>> miniBatch(_criticMiniBatchSize);
  for (size_t i = 0; i < _criticMiniBatchSize; i++) miniBatch[i].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Creating state history indexes to choose from
  std::vector<size_t> stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) stateHistoryIndexes[i] = i;

  for (size_t step = 0; step < _criticNormalizationSteps; step++)
  {
    // Shuffling indexes to choose the mini batch from
    std::shuffle(stateHistoryIndexes.begin(), stateHistoryIndexes.end(), *mt);

    for (size_t i = 0; i < _criticMiniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = stateHistoryIndexes[i];

      for (size_t j = 0; j < _problem->_stateVectorSize; j++) miniBatch[i][j] = _stateHistory[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) miniBatch[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
    }

    nn->setInput(miniBatch);
    nn->forward();

    // Getting this mini-batch's normalization means and variances
    auto newMeans = nn->getNormalizationMeans();
    auto newVariances = nn->getNormalizationVariances();

    // Accuulating their values into the new vector
    for (size_t i = 0; i < pCount; i++) newMeans[i] += newMeans[i];
    for (size_t i = 0; i < pCount; i++) newVariances[i] += newVariances[i];
  }

  // Calculating the actual mean and variance averages
  for (size_t i = 0; i < pCount; i++)
  {
    newMeans[i] = newMeans[i] / _criticNormalizationSteps;
    newVariances[i] = newVariances[i] / (_criticNormalizationSteps - 1.0);
  }

  // Setting the new adjusted means and variances
  nn->setNormalizationMeans(newMeans);
  nn->setNormalizationVariances(newVariances);
}

std::vector<double> Agent::getInferenceAction(const std::vector<double> &state)
{
  // Updating the policy's hyperparameters with the best we found so far
  updateHyperparameters(_bestHyperparameters);

  // Querying the policy for the action, given the state
  auto action = queryPolicy(state);

  // Returning action
  return action;
}

void Agent::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  // Updating epsilon value
  _policyEpsilonCurrentValue = hyperparameters["Epsilon"].get<double>();

  // Re-initializing action noises to zero
  size_t actionCount = _problem->_actionVectorIndexes.size();
  _currentActionNoises.resize(actionCount);
  for (size_t i = 0; i < actionCount; i++) _currentActionNoises[i] = 0.0;

  // Updating policy hyperparameters
  updatePolicyHyperparameters(hyperparameters);
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:          %lu/%lu\n", _stateHistory.size(), _replayMemoryMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:            %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:            %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:          %lu/%lu\n", _totalExperienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:          %lu\n", _totalExperienceCount);

  _k->_logger->logInfo("Normal", " + Current Max Experiences/Episode: %lu\n", _maxExperienceCount);
  _k->_logger->logInfo("Normal", " + Current Avg Experiences/Episode: %.0f\n", _averageExperienceCount);
  _k->_logger->logInfo("Normal", " + Current Min Experiences/Episode: %lu\n", _minExperienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  _k->_logger->logInfo("Normal", " + Average Reward:                  %f\n", _averageReward);

  if (_targetAverageReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward:             %f/%f\n", _bestAverageReward, _targetAverageReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward:             %f\n", _bestAverageReward);

  if (_maxSuboptimalSteps > 0)
    _k->_logger->logInfo("Normal", " + Steps with Suboptimal Reward:    %lu/%lu\n", _suboptimalStepCounter, _maxSuboptimalSteps);
  else
    _k->_logger->logInfo("Normal", " + Steps with Suboptimal Reward:    %lu\n", _suboptimalStepCounter);

  _k->_logger->logInfo("Normal", "Critic Information:\n");

  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _criticExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _criticExperiment._solver->printGenerationAfter();
  _criticExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Policy Information:\n");
  _k->_logger->logInfo("Normal", " + Current Epsilon:                  %f -> %f\n", _policyEpsilonCurrentValue, _policyEpsilonTargetValue);
  printPolicyInformation();
}

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _totalExperienceCount = 0;
    _currentExperienceCount = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting hyperparameters for all workers to use
  KORALI_BROADCAST("Hyperparameters", _hyperparameters);

  // Configuring current agent
  Sample agent;

  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";

  // Launching agent
  KORALI_START(agent);
  KORALI_WAIT(agent);

  /*********************************************************************
   * Storing new experiences to the history
   *********************************************************************/

  // Adding to the number of episodes
   _currentEpisode++;

  // Calculating the cumulative reward of this round of experiences
  _currentReward = 0.0;
  _currentExperienceCount = agent["Experiences"].size();
  size_t startExpId = _currentExperienceCount > _agentExperienceLimit ? _currentExperienceCount - _agentExperienceLimit : 0;

  // Storing episode's experiences
  for (size_t j = startExpId; j < _currentExperienceCount; j++)
  {
    _experienceReplayHistory.push_back(agent["Experiences"][j]);
    _currentReward += agent["Experiences"][j]["Reward"].get<double>();
  }

  // Updating average experience count
  _totalExperienceCount += _currentExperienceCount;

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_experienceReplayHistory.size() > _experienceReplayMaximumSize)
  {
    size_t excess = _experienceReplayHistory.size() - _experienceReplayMaximumSize;
    _experienceReplayHistory.erase(_experienceReplayHistory.begin(), _experienceReplayHistory.begin() + excess);
  }

  // If the minimum number of experiences has not yet been reached, there's not much else to do
  if (_experienceReplayHistory.size() < _experienceReplayStartSize) return;

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward) _bestTrainingReward = _currentReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward > _trainingRewardThreshold)
  {
    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> tests(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      tests[i]["Sample Id"] = _currentSampleID++;
      tests[i]["Module"] = "Problem";
      tests[i]["Operation"] = "Run Episode";
      tests[i]["Mode"] = "Testing";

      // Launching agent initialization
      KORALI_START(tests[i]);
    }

    KORALI_WAITALL(tests);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
     for (size_t j = 0; j < tests[i]["Experiences"].size(); j++)
      _averageTestingReward += tests[i]["Experiences"][j]["Reward"].get<double>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;

  // If we reached our target testing reward, there's nothing else to do
  if (_targetAverageTestingReward > -korali::Inf)
    if (_bestAverageTestingReward >= _targetAverageTestingReward) return;

  // Otherwise, optimize agent's critic/policy learners.
  for (size_t i = 0; i < _agentOptimizationStepsPerUpdate; i++)
   trainAgent();
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Episode's Experience Count:  %lu\n", _currentExperienceCount);

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplayHistory.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
  _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
  _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
  _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _totalExperienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:    %lu\n", _totalExperienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f\n", _bestTrainingReward);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
  _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
  _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

  printAgentInformation();
}

} // namespace solver

} // namespace korali

#include "auxiliar/fs.hpp"
#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  _variableCount = _k->_variables.size();

  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _experienceReplay.resize(_experienceReplayMaximumSize);

  //  Pre-allocating space for state time sequence
  _stateTimeSequence.resize(_timeSequenceLength);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _testingCandidateCount = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;
    _experienceCount = 0;
    _experienceReplayMaxPriority = 0.0f;

    // Initializing training and episode statistics
    _testingAverageReward = -korali::Inf;
    _testingStdevReward = +korali::Inf;
    _testingBestReward = -korali::Inf;
    _testingWorstReward = +korali::Inf;
    _trainingBestReward = -korali::Inf;
    _trainingBestEpisodeId = 0;
    _trainingAverageReward = 0.0f;
    _testingPreviousAverageReward = -korali::Inf;
    _testingBestAverageReward = -korali::Inf;
    _testingBestEpisodeId = 0;

    // Initializing training hyperparameters
    _trainingState = getTrainingState();

    // Getting initial policy hyperparameters (for agents to generate actions)
    _hyperparameters = getAgentPolicy();

    // Initialize best hyperparameters
    _testingBestHyperparameters = _hyperparameters;
    _trainingBestHyperparameters = _hyperparameters;

    // Initializing REFER information
    _experienceReplayREFERCurrentRatio = 0.0f;
    _experienceReplayREFERCurrentCutoff = _experienceReplayREFERCutoffScale + 1.0f;
    _experienceReplayREFERCurrentBeta = _experienceReplayREFEREnabled ? _experienceReplayREFERInitialBeta : 1.0;
    _experienceReplayREFERCurrentLearningRate = _learningRate;
  }

  // If this continues a previous training run, deserialize previous input experience replay
  if (_mode == "Training")
    if (_k->_currentGeneration > 0)
      deserializeExperienceReplay();

  // Initializing session-wise profiling timers
  _sessionRunningTime = 0.0;
  _sessionSerializationTime = 0.0;
  _sessionAgentComputationTime = 0.0;
  _sessionAgentCommunicationTime = 0.0;
  _sessionAgentPolicyEvaluationTime = 0.0;
  _sessionPolicyUpdateTime = 0.0;
  _sessionAgentAttendingTime = 0.0;

  // Initializing session-specific counters
  _sessionExperienceCount = 0;
  _sessionEpisodeCount = 0;
  _sessionGeneration = 1;
  _sessionPolicyUpdateCount = 0;

  // Calculating how many more experiences do we need in this session to reach the starting size
  _sessionExperiencesUntilStartSize = _experienceReplay.size() > _experienceReplayStartSize ? 0 : _experienceReplayStartSize - _experienceReplay.size();

  // Creating storate for _agents and their status
  if (_mode == "Training") _agents.resize(_agentCount);
  if (_mode == "Testing") _agents.resize(_testingSampleIds.size());
  _isAgentRunning.resize(_agentCount, false);

  // Fixing termination criteria for testing mode
  if (_mode == "Testing") _maxGenerations = _k->_currentGeneration + 1;

  // Setting testing policy to best testing hyperparameters if not custom-set by the user
  if (_mode == "Testing")
    if (_testingPolicy.empty()) _testingPolicy = _testingBestHyperparameters;

  // Checking if there's testing samples defined
  if (_mode == "Testing")
    if (_testingSampleIds.size() == 0)
      KORALI_LOG_ERROR("For testing, you need to indicate the sample ids to run in the ['Testing']['Sample Ids'] field.\n");

  // Prepare storage for rewards from tested samples
  _testingReward.resize(_testingSampleIds.size());

  // Storage for the entire episodes before adding it to the replay memory
  _episodes.resize(_agentCount);
}

void Agent::runGeneration()
{
  if (_mode == "Training") trainingGeneration();
  if (_mode == "Testing") testingGeneration();
}

void Agent::trainingGeneration()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Setting generation-specific timers
  _generationRunningTime = 0.0;
  _generationSerializationTime = 0.0;
  _generationAgentComputationTime = 0.0;
  _generationAgentCommunicationTime = 0.0;
  _generationAgentPolicyEvaluationTime = 0.0;
  _generationPolicyUpdateTime = 0.0;
  _generationAgentAttendingTime = 0.0;

  // Setting training state if continuing from previous results
  if (_k->_currentGeneration > 0) setTrainingState(_trainingState);

  // Running until all _agents have finished
  while (_sessionEpisodeCount < _episodesPerGeneration * _sessionGeneration)
  {
    // Launching (or re-launching) agents
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == false)
      {
        _agents[agentId]["Sample Id"] = _currentEpisode++;
        _agents[agentId]["Module"] = "Problem";
        _agents[agentId]["Operation"] = "Run Training Episode";
        _agents[agentId]["Hyperparameters"] = _hyperparameters;

        KORALI_START(_agents[agentId]);
        _isAgentRunning[agentId] = true;
      }

    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    // Attending to running agents, checking if any experience has been received
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
        attendAgent(agentId);

    // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
    if (_sessionExperienceCount > _sessionExperiencesUntilStartSize)
    {
      // If we accumulated enough experiences between updates in this session, update now
      while (_sessionExperienceCount > (_experiencesBetweenPolicyUpdates * _sessionPolicyUpdateCount + _sessionExperiencesUntilStartSize))
      {
        auto beginTime = std::chrono::steady_clock::now(); // Profiling

        // Calling the algorithm specific policy training algorithm
        trainPolicy();

        // Getting new policy hyperparameters (for agents to generate actions)
        _hyperparameters = getAgentPolicy();

        // Increasing policy update counters
        _policyUpdateCount++;
        _sessionPolicyUpdateCount++;

        // If using REFER, updating its information
        if (_experienceReplayREFEREnabled == true) updateREFERInformation();

        auto endTime = std::chrono::steady_clock::now();                                                                  // Profiling
        _sessionPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
        _generationPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
      }
    }
  }

  // Gathering their training hyperparameters (for checkpoint/resume purposes)
  _trainingState = getTrainingState();

  // Updating experience probabilities
  if (_miniBatchStrategy == "Prioritized") updateExperienceReplayProbabilities();

  // Now serializing experience replay database
  if (_k->_fileOutputEnabled)
    if (_k->_fileOutputFrequency > 0)
      if (_k->_currentGeneration % _k->_fileOutputFrequency == 0)
        serializeExperienceReplay();

  // Measuring generation time
  auto endTime = std::chrono::steady_clock::now();                                                             // Profiling
  _sessionRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  // Increasing session's generation count
  _sessionGeneration++;
}

void Agent::testingGeneration()
{
  // Launching  agents
  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
  {
    _agents[agentId]["Sample Id"] = _testingSampleIds[agentId];
    _agents[agentId]["Module"] = "Problem";
    _agents[agentId]["Operation"] = "Run Testing Episode";
    _agents[agentId]["Hyperparameters"] = _testingPolicy;

    KORALI_START(_agents[agentId]);
  }

  KORALI_WAITALL(_agents);

  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
  {
    _testingReward[agentId] = _agents[agentId]["Testing Reward"].get<float>();
  }
}

void Agent::attendAgent(size_t agentId)
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Storage for the incoming experience
  knlohmann::json experience;

  // Retrieving the experience, if any has arrived for the current agent.
  if (_agents[agentId].retrievePendingMessage(experience))
  {
    // Storage to determine terminal state
    bool isTerminal = false;

    // Storing experience in the episode
    _episodes[agentId].push_back(experience);

    // Check if experience is terminal
    isTerminal = experience["Termination"] != "Non Terminal";

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true)
      KORALI_SEND_MSG_TO_SAMPLE(_agents[agentId], _hyperparameters);

    if (isTerminal)
    {
      //  Now that we have the entire episode, process its experiences (add them to replay memory)
      processEpisode(_episodes[agentId]);

      // Increasing total experience counters
      _experienceCount += _episodes[agentId].size();
      _sessionExperienceCount += _episodes[agentId].size();

      // Now clear the episode of all its current experiences
      _episodes[agentId].clear();

      // Waiting for the agent to come back with all the information
      KORALI_WAIT(_agents[agentId]);

      // Getting the training reward of the latest episode
      _trainingLastReward = _agents[agentId]["Training Reward"].get<float>();

      // Keeping training statistics. Updating if exceeded best training policy so far.
      if (_trainingLastReward > _trainingBestReward)
      {
        _trainingBestReward = _trainingLastReward;
        _trainingBestEpisodeId = _agents[agentId]["Sample Id"];
        _trainingBestHyperparameters = _agents[agentId]["Hyperparameters"];
      }

      // If the policy has exceeded the threshold during training, we gather its statistics
      if (_agents[agentId]["Tested Policy"] == true)
      {
        _testingCandidateCount++;

        _testingPreviousAverageReward = _testingAverageReward;
        _testingAverageReward = _agents[agentId]["Average Testing Reward"].get<float>();
        _testingStdevReward = _agents[agentId]["Stdev Testing Reward"].get<float>();
        _testingBestReward = _agents[agentId]["Best Testing Reward"].get<float>();
        _testingWorstReward = _agents[agentId]["Worst Testing Reward"].get<float>();

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_testingAverageReward > _testingBestAverageReward)
        {
          _testingBestAverageReward = _testingAverageReward;
          _testingBestEpisodeId = _agents[agentId]["Sample Id"];
          _testingBestHyperparameters = _agents[agentId]["Hyperparameters"];
        }
      }

      // Obtaining profiling information
      _sessionAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _sessionAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _sessionAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();
      _generationAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _generationAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _generationAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();

      // Set agent as finished
      _isAgentRunning[agentId] = false;

      // Increasing session episode count
      _sessionEpisodeCount++;
    }
  }

  auto endTime = std::chrono::steady_clock::now();                                                                    // Profiling
  _sessionAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void Agent::processEpisode(std::vector<knlohmann::json> &episode)
{
  // Storage for cumulative reward
  float cumulativeReward = 0.0f;

  // Now adding episode's experiences into the replay memory
  for (size_t expId = 0; expId < episode.size(); expId++)
  {
    // Storage for new experience
    experience_t e;

    // Creating caches for the new experience
    e.cache.setMaxAge(_cachePersistence);
    e.cache.setTimer(&_policyUpdateCount);

    // Getting state, action, reward, and whether it's terminal
    e.state = episode[expId]["State"].get<std::vector<float>>();
    e.action = episode[expId]["Action"].get<std::vector<float>>();
    e.reward = episode[expId]["Reward"].get<float>();

    if (episode[expId]["Termination"] == "Non Terminal") e.termination = e_nonTerminal;
    if (episode[expId]["Termination"] == "Terminal") e.termination = e_terminal;
    if (episode[expId]["Termination"] == "Truncated")
    {
      e.termination = e_truncated;
      e.truncatedState = episode[expId]["Truncated State"].get<std::vector<float>>();
    }

    // Updating experience's importance weight in its cache. Initially assumed to be 1.0 because its freshly produced
    e.cache.set("Importance Weight", 1.0f);

    // Updating metadata for prioritized minibatch selection
    e.metadata["Mini Batch"]["Priority"] = _experienceReplayMaxPriority;
    e.metadata["Mini Batch"]["Probability"] = 0.0;

    // Storing policy
    e.policy = episode[expId]["Policy"];

    // Adding experience
    _experienceReplay.add(e);

    // Adding to cumulative reward
    cumulativeReward += e.reward;
  }

  // Storing history information
  _trainingRewardHistory.push_back(cumulativeReward);
  _trainingExperienceHistory.push_back(episode.size());

  // Updating average cumulative reward statistics
  _trainingAverageReward = 0.0f;
  ssize_t startEpisodeId = _trainingRewardHistory.size() - _trainingAverageDepth;
  ssize_t endEpisodeId = _trainingRewardHistory.size() - 1;
  if (startEpisodeId < 0) startEpisodeId = 0;
  for (ssize_t e = startEpisodeId; e <= endEpisodeId; e++)
    _trainingAverageReward += _trainingRewardHistory[e];
  _trainingAverageReward /= (float)(endEpisodeId - startEpisodeId + 1);
}

void Agent::updateREFERInformation()
{
  // First, re-calculate the off-policy ratio of experiences in the ER

  // Counting number of off-policy experiences
  size_t expCount = _experienceReplay.size();
  size_t offPolicyCount = 0;

  // Count off-policy experiences
#pragma omp parallel for reduction(+:offPolicyCount)
  for (size_t expId = 0; expId < expCount; expId++)
  {
    // Getting the last calculated value of the experience's importance weight. We can tolerate this approximation.
    float expImportanceWeight = _experienceReplay[expId].cache.get("Importance Weight");

    // It is off policy if it is outside the [1/cutoff, cutoff] region
    if ((expImportanceWeight < (1.0f / _experienceReplayREFERCurrentCutoff)) ||
        (expImportanceWeight > _experienceReplayREFERCurrentCutoff)) offPolicyCount++;
  }

  // Calculating off policy ratio
  _experienceReplayREFERCurrentRatio = (float)offPolicyCount / (float)expCount;

  // Now updating the rest of the REFER variables
  _experienceReplayREFERCurrentCutoff = 1.0f + _experienceReplayREFERCutoffScale / (1.0f + _experienceReplayREFERAnnealingRate * (float)_policyUpdateCount);

  // Updating current learning rate
  _experienceReplayREFERCurrentLearningRate = _learningRate / (1.0f + _experienceReplayREFERAnnealingRate * _policyUpdateCount);

  // Updating beta parameter
  if (_experienceReplayREFERCurrentRatio > _experienceReplayREFERTarget)
    _experienceReplayREFERCurrentBeta = (1.0f - _experienceReplayREFERCurrentLearningRate) * _experienceReplayREFERCurrentBeta;
  else
    _experienceReplayREFERCurrentBeta = (1.0f - _experienceReplayREFERCurrentLearningRate) * _experienceReplayREFERCurrentBeta + _experienceReplayREFERCurrentLearningRate;
}

void Agent::updateExperienceReplayProbabilities()
{
  // Rank priorities
  auto experienceRanking = std::vector<std::pair<float, size_t>>(_experienceReplay.size());
  for (size_t i = 0; i < _experienceReplay.size(); ++i)
    experienceRanking[i] = std::make_pair(_experienceReplay[i].metadata["Mini Batch"]["Priority"].get<float>(), i + 1);
  sort(experienceRanking.begin(), experienceRanking.end());

  // Calculate rank based probabilities
  float prioritySum = 0.0;
  for (size_t i = 1; i < _experienceReplay.size() + 1; ++i)
    prioritySum += std::pow((float)i, _priorityAnnealingRate);

  // Update probabilities
  for (size_t i = 0; i < _experienceReplay.size(); ++i)
  {
    float priority = std::pow((float)experienceRanking[i].second, _priorityAnnealingRate) / (prioritySum);
    _experienceReplay[i].metadata["Mini Batch"]["Probability"] = priority;
    if (priority > _experienceReplayMaxPriority) _experienceReplayMaxPriority = priority;
  }
}

std::vector<size_t> Agent::generateMiniBatch(size_t size)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(size);

  if (_miniBatchStrategy == "Uniform")
  {
    for (size_t i = 0; i < size; i++)
    {
      // Producing random (uniform) number for the selection of the experience
      float x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      size_t expId = std::floor(x * (float)(_experienceReplay.size() - 1));

      // Setting experience
      miniBatch[i] = expId;
    }
  }

  if (_miniBatchStrategy == "Prioritized")
  {
    // Allocating storage for uniform random numbers
    auto pvalues = std::vector<float>(size);

    for (size_t i = 0; i < size; i++)
      pvalues[i] = _uniformGenerator->getRandomNumber();

    // Sort selections ascending for traversal
    std::sort(pvalues.begin(), pvalues.end());

    size_t expId = 0;
    size_t batchIdx = 0;
    float cumulativeProbability = 0;

    // Sampling from multinomial distribution using inverse transform
    cumulativeProbability += _experienceReplay[0].metadata["Mini Batch"]["Probability"].get<float>();
    while (batchIdx < size && expId < _experienceReplay.size())
    {
      if (cumulativeProbability > pvalues[batchIdx])
        miniBatch[batchIdx++] = expId;
      else
        cumulativeProbability += _experienceReplay[expId++].metadata["Mini Batch"]["Probability"].get<float>();
    }

    // Uniform fill if miniBatch not full due to numerical precision during probability accumulation (cumulative probability < 1)
    while (batchIdx < size)
      miniBatch[batchIdx++] = std::floor(_uniformGenerator->getRandomNumber() * (float)(_experienceReplay.size() - 1));
  }

  return miniBatch;
}

size_t Agent::getTimeSequenceStartExpId(size_t expId)
{
  size_t startId = expId;

  // Adding (tmax-1) time sequences to the given experience
  for (size_t t = 0; t < _timeSequenceLength - 1; t++)
  {
    // If we reached the start of the ER, this is the starting episode in the sequence
    if (startId == 0) break;

    // Now going back one experience
    startId--;

    // If we reached the end of the previous episode, then add one (this covers the case where the provided experience is also terminal) and break.
    if (_experienceReplay[startId].termination != e_nonTerminal)
    {
      startId++;
      break;
    }
  }

  return startId;
}

float Agent::retraceFunction(size_t expId)
{
  // Finding last experience in the episode that corresponds to expId
  ssize_t startId = expId;
  ssize_t endId = startId;
  while (_experienceReplay[endId].termination == e_nonTerminal) endId++;

  // Calculating initial Vtbc
  float Vtbc = 0.0f;

  // If it was a truncated episode, add the value function for the terminal state to Vtbc
  if (_experienceReplay[endId].termination == e_truncated)
  {
    // Get truncated state sequence, adding the truncated state to it and removing first time element
    auto expTruncatedStateSequence = getTruncatedStateTimeSequence(endId);
    float terminalV = _experienceReplay[endId].cache.access("Terminal State Value", [this, expTruncatedStateSequence]() { return this->stateValueFunction(expTruncatedStateSequence); });
    Vtbc = terminalV;
  }

  // Now iterating backwards to calculate the rest of vTbc
  for (ssize_t curId = endId; curId >= startId; curId--)
  {
    // Getting current reward, action, and state
    auto curReward = _experienceReplay[curId].reward;
    auto expStateSequence = getStateTimeSequence(curId);
    auto expAction = _experienceReplay[curId].action;

    // Calculating state value function
    float curV = _experienceReplay[curId].cache.access("State Value", [this, expStateSequence]() { return this->stateValueFunction(expStateSequence); });

    // Calculating importance weight
    float importanceWeight = _experienceReplay[curId].cache.access("Importance Weight", [this, curId]() { return getExperienceImportanceWeight(curId); });

    // Truncate importance weight
    float truncatedImportanceWeight = std::min(1.0f, importanceWeight);

    // Calculating Vtbc
    Vtbc = curV + truncatedImportanceWeight * (curReward + _discountFactor * Vtbc - curV);
  }

  // Returning Vtbc
  return Vtbc;
}

void Agent::resetTimeSequence()
{
  _stateTimeSequence.clear();
}

std::vector<std::vector<float>> Agent::getStateTimeSequence(size_t expId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding states
  for (size_t e = startId; e <= expId; e++)
    timeSequence.push_back(_experienceReplay[e].state);

  return timeSequence;
}

std::vector<std::vector<float>> Agent::getTruncatedStateTimeSequence(size_t expId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding states, except for the initial one
  for (size_t e = startId + 1; e <= expId; e++)
    timeSequence.push_back(_experienceReplay[e].state);

  // Lastly, adding truncated state
  timeSequence.push_back(_experienceReplay[expId].truncatedState);

  return timeSequence;
}

std::vector<std::vector<float>> Agent::getStateActionTimeSequence(size_t expId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding state/action vectors
  for (size_t e = startId; e <= expId; e++)
  {
    std::vector<float> stateAction(_problem->_stateVectorSize + _problem->_actionVectorSize);
    for (size_t i = 0; i < _problem->_stateVectorSize; i++) stateAction[i] = _experienceReplay[e].state[i];
    for (size_t i = 0; i < _problem->_actionVectorSize; i++) stateAction[_problem->_stateVectorSize + i] = _experienceReplay[e].action[i];
    timeSequence.push_back(stateAction);
  }

  return timeSequence;
}

void Agent::finalize()
{
  if (_k->_fileOutputEnabled)
    serializeExperienceReplay();

  _k->_logger->logInfo("Normal", "Waiting for pending agents to finish...\n");

  // Waiting for pending agents to finish
  bool agentsRemain = true;
  while (agentsRemain == true)
  {
    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    agentsRemain = false;
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }
  }
}

void Agent::serializeExperienceReplay()
{
  _k->_logger->logInfo("Normal", "Serializing Experience Replay database...\n");
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json experienceReplayDatabase;

  // Serializing agent's database into the JSON storage
  for (size_t i = 0; i < _experienceReplay.size(); i++)
  {
    experienceReplayDatabase[i]["State"] = _experienceReplay[i].state;
    experienceReplayDatabase[i]["Action"] = _experienceReplay[i].action;
    experienceReplayDatabase[i]["Reward"] = _experienceReplay[i].reward;
    experienceReplayDatabase[i]["Termination"] = _experienceReplay[i].termination;
    experienceReplayDatabase[i]["Truncated State"] = _experienceReplay[i].truncatedState;
    experienceReplayDatabase[i]["Policy"] = _experienceReplay[i].policy;
    experienceReplayDatabase[i]["Metadata"] = _experienceReplay[i].metadata;
    experienceReplayDatabase[i]["Cache"]["Keys"] = _experienceReplay[i].cache.getKeys();
    experienceReplayDatabase[i]["Cache"]["Values"] = _experienceReplay[i].cache.getValues();
    experienceReplayDatabase[i]["Cache"]["Times"] = _experienceReplay[i].cache.getTimes();
  }

  // If results directory doesn't exist, create it
  if (!dirExists(_k->_fileOutputPath)) mkdir(_k->_fileOutputPath);

  // Resolving file path
  std::string dbPath = "./" + _k->_fileOutputPath + "/ERDb.json";

  // Storing database to file
  if (saveJsonToFile(dbPath.c_str(), experienceReplayDatabase) != 0)
    KORALI_LOG_ERROR("Could not serialize experience replay database into file %s\n", dbPath.c_str());

  auto endTime = std::chrono::steady_clock::now();                                                                   // Profiling
  _sessionSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void Agent::deserializeExperienceReplay()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json experienceReplayDatabase;

  // Resolving file path
  std::string dbPath = "./" + _k->_fileOutputPath + "/ERDb.json";

  // Loading database from file
  _k->_logger->logInfo("Detailed", "Loading previous run ER database from file %s...\n", dbPath.c_str());
  if (loadJsonFromFile(experienceReplayDatabase, dbPath.c_str()) == false)
    KORALI_LOG_ERROR("Could not deserialize experience replay database.\n");

  // Clearing existing database
  _experienceReplay.clear();

  // Deserializing database from JSON to the agent's database
  for (size_t i = 0; i < experienceReplayDatabase.size(); i++)
  {
    experience_t e;
    e.state = experienceReplayDatabase[i]["State"].get<std::vector<float>>();
    e.action = experienceReplayDatabase[i]["Action"].get<std::vector<float>>();
    e.reward = experienceReplayDatabase[i]["Reward"].get<float>();
    e.termination = experienceReplayDatabase[i]["Termination"].get<termination_t>();
    e.truncatedState = experienceReplayDatabase[i]["Truncated State"].get<std::vector<float>>();
    e.policy = experienceReplayDatabase[i]["Policy"];
    e.metadata = experienceReplayDatabase[i]["Metadata"];

    _experienceReplay.add(e);
  }

  // Now updating the cache for every experience
  for (size_t i = 0; i < _experienceReplay.size(); i++)
  {
    // Creating caches for the new experience
    _experienceReplay[i].cache.setMaxAge(_cachePersistence);
    _experienceReplay[i].cache.setTimer(&_policyUpdateCount);

    for (size_t c = 0; c < experienceReplayDatabase[i]["Cache"]["Keys"].size(); c++)
    {
      auto key = experienceReplayDatabase[i]["Cache"]["Keys"][c].get<std::string>();
      auto val = experienceReplayDatabase[i]["Cache"]["Values"][c].get<float>();
      auto time = experienceReplayDatabase[i]["Cache"]["Times"][c].get<size_t>();
      _experienceReplay[i].cache.set(key, val, time);
    }
  }

  auto endTime = std::chrono::steady_clock::now();                                                                         // Profiling
  double deserializationTime = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count() / 1.0e+9; // Profiling
  _k->_logger->logInfo("Detailed", "Took %fs to deserialize ER database.\n", deserializationTime);
}

void Agent::printGenerationAfter()
{
  if (_mode == "Training")
  {
    _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

    _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplay.size(), _experienceReplayMaximumSize);

    if (_maxEpisodes > 0)
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
    else
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

    if (_maxExperiences > 0)
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
    else
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

    if (_experienceReplayREFEREnabled == true)
    {
      _k->_logger->logInfo("Normal", "REFER Statistics:\n");
      _k->_logger->logInfo("Normal", " + Current Off-Policy Ratio:    %f (Target: %f)\n", _experienceReplayREFERCurrentRatio, _experienceReplayREFERTarget);
      _k->_logger->logInfo("Normal", " + Importance Weight Cutoff:    [%.3f, %.3f]\n", 1.0f / _experienceReplayREFERCurrentCutoff, _experienceReplayREFERCurrentCutoff);
      _k->_logger->logInfo("Normal", " + Beta Factor:                 %f\n", _experienceReplayREFERCurrentBeta);
    }

    _k->_logger->logInfo("Normal", "Training Statistics:\n");

    if (_maxPolicyUpdates > 0)
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
    else
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

    _k->_logger->logInfo("Normal", " + Latest Reward:               %f/%f\n", _trainingLastReward, _problem->_trainingRewardThreshold);
    _k->_logger->logInfo("Normal", " + %lu-Deep Average Reward:      %f\n", _trainingAverageDepth, _trainingAverageReward);
    _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _trainingBestReward, _trainingBestEpisodeId);

    _k->_logger->logInfo("Normal", "Testing Statistics:\n");

    _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _testingCandidateCount);

    _k->_logger->logInfo("Normal", " + Latest Average (Stdev / Worst / Best) Reward: %f (%f / %f / %f)\n", _testingAverageReward, _testingStdevReward, _testingWorstReward, _testingBestReward);

    if (_testingTargetAverageReward > -korali::Inf)
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f/%f (%lu)\n", _testingBestAverageReward, _testingTargetAverageReward, _testingBestEpisodeId);
    else
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f (%lu)\n", _testingBestAverageReward, _testingBestEpisodeId);

    printAgentInformation();

    _k->_logger->logInfo("Detailed", "Profiling Information:                  [Generation] - [Session]\n");
    _k->_logger->logInfo("Detailed", " + Policy Update Time:                  [%5.3fs] - [%3.3fs]\n", _generationPolicyUpdateTime / 1.0e+9, _sessionPolicyUpdateTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Experience Serialization Time:       [%5.3fs] - [%3.3fs]\n", _generationSerializationTime / 1.0e+9, _sessionSerializationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Agent Attending Time:                [%5.3fs] - [%3.3fs]\n", _generationAgentAttendingTime / 1.0e+9, _sessionAgentAttendingTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Computation Time:          [%5.3fs] - [%3.3fs]\n", _generationAgentComputationTime / 1.0e+9, _sessionAgentComputationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Communication/Wait Time:   [%5.3fs] - [%3.3fs]\n", _generationAgentCommunicationTime / 1.0e+9, _sessionAgentCommunicationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Policy Evaluation Time:    [%5.3fs] - [%3.3fs]\n", _generationAgentPolicyEvaluationTime / 1.0e+9, _sessionAgentPolicyEvaluationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Running Time:                        [%5.3fs] - [%3.3fs]\n", _generationRunningTime / 1.0e+9, _sessionRunningTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Experience Serialization Time:       [%5.3fs] - [%3.3fs]\n", _generationSerializationTime / 1.0e+9, _sessionSerializationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + [I/O] Result File Saving Time:        %5.3fs\n", _k->_resultSavingTime / 1.0e+9);
  }

  if (_mode == "Testing")
  {
    _k->_logger->logInfo("Normal", "Testing Results:\n");
    for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
    {
      _k->_logger->logInfo("Normal", " + Sample %lu:\n", _testingSampleIds[agentId]);
      _k->_logger->logInfo("Normal", "   + Cumulative Reward               %f\n", _testingReward[agentId]);
    }
  }
}

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // Pre-allocating space for the experience replay memory
  _experienceReplayStates.reserve(_experienceReplayMaximumSize);
  _experienceReplayActions.reserve(_experienceReplayMaximumSize);
  _experienceReplayRewards.reserve(_experienceReplayMaximumSize);
  _experienceReplayTerminal.reserve(_experienceReplayMaximumSize);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
     _randomActionProbabilityCurrentValue = _randomActionProbabilityInitialValue;

    _currentEpisode = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting initial hyperparameters for all workers to use
  KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

  // Calculating effective value for random action probability
  _randomActionProbabilityEffectiveValue = _experienceReplayStates.size() > _experienceReplayStartSize ? _randomActionProbabilityCurrentValue : 1.0;

  // Configuring current agent
  Sample agent;

  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";
  agent["Random Action Probability"] = _randomActionProbabilityEffectiveValue;

  // Initializing statistical information
  _currentReward = 0.0;

  // Launching agent
  KORALI_START(agent);

  // Storage to keep experience information
  knlohmann::json experience;

  // Counter for experiences between updates
  size_t _curExperienceCount = 0;

  // Keep listening to incoming experiences until we reached a terminal state
  do
  {
    // Listening for incoming experiences
    experience = KORALI_RECV_MSG_FROM_SAMPLE(agent);

    _experienceReplayStates.push_back(experience["State"].get<std::vector<float>>());
    _experienceReplayActions.push_back(experience["Action"].get<std::vector<float>>());
    _experienceReplayRewards.push_back(experience["Reward"].get<float>());
    _experienceReplayTerminal.push_back(experience["Is Terminal"].get<bool>());
    if(isDefined(experience, "Metadata", "Action Index")) _experienceReplayActionIndexes.push_back(experience["Metadata"]["Action Index"].get<size_t>());
    if(isDefined(experience, "Metadata", "Action Probabilities")) _experienceReplayActionProbabilities.push_back(experience["Metadata"]["Action Probabilities"].get<std::vector<float>>());
    if(isDefined(experience, "Metadata", "Action Means")) _experienceReplayActionMeans.push_back(experience["Metadata"]["Action Means"].get<std::vector<float>>());
    if(isDefined(experience, "Metadata", "Action Sigmas")) _experienceReplayActionSigmas.push_back(experience["Metadata"]["Action Sigmas"].get<std::vector<float>>());

    _curExperienceCount++;

    if (_curExperienceCount >= _experiencesBetweenUpdates || experience["Is Terminal"] == true) // If we accumulated enough experiences between updates, update now
    if (_experienceReplayStates.size() > _experienceReplayStartSize) // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
    {
     for (size_t i = 0; i < _optimizationStepsPerUpdate; i++) trainAgent();
     _optimizationStepCount += _optimizationStepsPerUpdate;
     _curExperienceCount = 0;
    }

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true)
      KORALI_SEND_MSG_TO_SAMPLE(agent, _hyperparameters);

    // Adding experience's reward to episode's reward
    _currentReward += experience["Reward"].get<float>();

  } while (experience["Is Terminal"] == false);

  // Waiting for agent to finish
  KORALI_WAIT(agent);

  // Adding to the number of episodes
  _currentEpisode++;

  /*********************************************************************
   * Updating experience replay memory
   *********************************************************************/

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_experienceReplayStates.size() > _experienceReplayMaximumSize)
  {
    size_t excess = _experienceReplayStates.size() - _experienceReplayMaximumSize;
    _experienceReplayStates.erase(_experienceReplayStates.begin(), _experienceReplayStates.begin() + excess);
    _experienceReplayActions.erase(_experienceReplayActions.begin(), _experienceReplayActions.begin() + excess);
    _experienceReplayRewards.erase(_experienceReplayRewards.begin(), _experienceReplayRewards.begin() + excess);
    _experienceReplayTerminal.erase(_experienceReplayTerminal.begin(), _experienceReplayTerminal.begin() + excess);

    if (_experienceReplayActionIndexes.size() > 0)
     _experienceReplayActionIndexes.erase(_experienceReplayActionIndexes.begin(), _experienceReplayActionIndexes.begin() + excess);

    if (_experienceReplayActionProbabilities.size() > 0)
     _experienceReplayActionProbabilities.erase(_experienceReplayActionProbabilities.begin(), _experienceReplayActionProbabilities.begin() + excess);

    if (_experienceReplayActionMeans.size() > 0)
     _experienceReplayActionMeans.erase(_experienceReplayActionMeans.begin(), _experienceReplayActionMeans.begin() + excess);

    if (_experienceReplayActionSigmas.size() > 0)
     _experienceReplayActionSigmas.erase(_experienceReplayActionSigmas.begin(), _experienceReplayActionSigmas.begin() + excess);
  }

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward) _bestTrainingReward = _currentReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward >= _trainingRewardThreshold)
  {
    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> agents(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      agents[i]["Sample Id"] = _currentSampleID++;
      agents[i]["Module"] = "Problem";
      agents[i]["Action Source"] = "Policy";
      agents[i]["Operation"] = "Run Episode";
      agents[i]["Mode"] = "Testing";
      agents[i]["Random Action Probability"] = 0.0;

      // Launching agent initialization
      KORALI_START(agents[i]);
    }

    KORALI_WAITALL(agents);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += agents[i]["Cumulative Reward"].get<float>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  /////////////// Updating the probability of a random action (epsilon)

  // If we reached the starting size
  if (_experienceReplayStates.size() > _experienceReplayStartSize)
  {
   // Decreasing the value of epsilon
   _randomActionProbabilityCurrentValue = _randomActionProbabilityCurrentValue - _randomActionProbabilityDecreaseRate;

   // Without exceeding the lower bound
   if (_randomActionProbabilityCurrentValue < _randomActionProbabilityTargetValue) _randomActionProbabilityCurrentValue = _randomActionProbabilityTargetValue;
  }

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;
}

void Agent::normalizeStateActionNeuralNetwork(NeuralNetwork* neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
 // Allocating memory for the mini batch set
 std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

 for (size_t i = 0; i < normalizationSteps; i++)
   miniBatches[i].resize(miniBatchSize);

 for (size_t i = 0; i < normalizationSteps; i++)
   for (size_t j = 0; j < miniBatchSize; j++)
     miniBatches[i][j].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

 // Creating storage for state history indexes to choose from
 std::vector<size_t> experienceReplayIndexes(_experienceReplayStates.size());
 for (size_t i = 0; i < _experienceReplayStates.size()-1; i++) experienceReplayIndexes[i] = i;

 // Filling the minibatches
 for (size_t step = 0; step < normalizationSteps; step++)
 {
   // Shuffling indexes to choose the mini batch from
   std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

   for (size_t i = 0; i < miniBatchSize; i++)
   {
     // Selecting a uniformly random selected, yet not repeated experience
     size_t expId = experienceReplayIndexes[i];

     for (size_t j = 0; j < _problem->_stateVectorSize; j++) miniBatches[step][i][j] = _experienceReplayStates[expId][j];
     for (size_t j = 0; j < _problem->_actionVectorSize; j++) miniBatches[step][i][_problem->_stateVectorSize + j] = _experienceReplayActions[expId][j];
   }
 }

 neuralNetwork->normalize(miniBatches);
}

void Agent::normalizeStateNeuralNetwork(NeuralNetwork* neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
 // Allocating memory for the mini batch set
 std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

 for (size_t i = 0; i < normalizationSteps; i++)
   miniBatches[i].resize(miniBatchSize);

 for (size_t i = 0; i < normalizationSteps; i++)
   for (size_t j = 0; j < miniBatchSize; j++)
     miniBatches[i][j].resize(_problem->_stateVectorSize);

 // Creating storage for state history indexes to choose from
 std::vector<size_t> experienceReplayIndexes(_experienceReplayStates.size());
 for (size_t i = 0; i < _experienceReplayStates.size()-1; i++) experienceReplayIndexes[i] = i;

 // Filling the minibatches
 for (size_t step = 0; step < normalizationSteps; step++)
 {
   // Shuffling indexes to choose the mini batch from
   std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

   for (size_t i = 0; i < miniBatchSize; i++)
   {
     // Selecting a uniformly random selected, yet not repeated experience
     size_t expId = experienceReplayIndexes[i];
     miniBatches[step][i] = _experienceReplayStates[expId];
   }
 }

 neuralNetwork->normalize(miniBatches);
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplayStates.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceReplayStates.size(), _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:    %lu\n", _experienceReplayStates.size());


  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f\n", _bestTrainingReward);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

 _k->_logger->logInfo("Normal", " + Current p(Random Action):       %f\n", _randomActionProbabilityEffectiveValue);

  printAgentInformation();
}

} // namespace solver

} // namespace korali

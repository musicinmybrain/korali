#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestAverageTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting hyperparameters for all workers to use
  KORALI_BROADCAST("Hyperparameters", _hyperparameters);

  // Creating storage for agents
  std::vector<Sample> agents(_agentEpisodesPerGeneration);
  _currentEpisode += _agentEpisodesPerGeneration;

  // Initializing the agents and their environments
  for (size_t i = 0; i < _agentEpisodesPerGeneration; i++)
  {
    // Configuring Agent
    agents[i]["Sample Id"] = _currentSampleID++;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Training";

    // Launching agent initialization
    KORALI_START(agents[i]);
  }

  KORALI_WAITALL(agents);

  /*********************************************************************
   * Storing new experiences to the history
   *********************************************************************/

  size_t currentExperienceCount = 0;

  _maxExperienceCount = 0;
  _minExperienceCount = std::numeric_limits<size_t>::max();

  // Calculating the cumulative reward of this round of experiences
  double cumulativeReward = 0.0;

  for (size_t i = 0; i < _agentEpisodesPerGeneration; i++)
  {
    size_t experienceSize = agents[i]["Experience"]["States"].size();

    // Collecting entire episode's reward
    cumulativeReward += agents[i]["Cumulative Reward"].get<double>();

    // Storing statistics
    currentExperienceCount += experienceSize;
    if (experienceSize > _maxExperienceCount) _maxExperienceCount = experienceSize;
    if (experienceSize < _minExperienceCount) _minExperienceCount = experienceSize;

    size_t startExpId = experienceSize > _agentExperienceLimit ? experienceSize - _agentExperienceLimit : 0;

    for (size_t j = startExpId; j < experienceSize; j++)
    {
      // Storing action/state/reward/probability density experience
      _stateHistory.push_back(agents[i]["Experience"]["States"][j].get<std::vector<double>>());
      _actionHistory.push_back(agents[i]["Experience"]["Actions"][j].get<std::vector<double>>());
      _probabilityDensityHistory.push_back(agents[i]["Experience"]["Densities"][j].get<double>());
      _rewardHistory.push_back(agents[i]["Experience"]["Rewards"][j].get<double>());

      // Storing whether this experience is terminal
      _terminalHistory.push_back(j == (experienceSize - 1));
    }
  }

  // Updating average experience count
  _totalExperienceCount += currentExperienceCount;
  _averageExperienceCount = ((double)currentExperienceCount) / ((double)_agentEpisodesPerGeneration);

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_stateHistory.size() > _replayMemoryMaximumSize)
  {
    size_t excess = _stateHistory.size() - _replayMemoryMaximumSize;
    _stateHistory.erase(_stateHistory.begin(), _stateHistory.begin() + excess);
    _actionHistory.erase(_actionHistory.begin(), _actionHistory.begin() + excess);
    _rewardHistory.erase(_rewardHistory.begin(), _rewardHistory.begin() + excess);
    _probabilityDensityHistory.erase(_probabilityDensityHistory.begin(), _probabilityDensityHistory.begin() + excess);
    _terminalHistory.erase(_terminalHistory.begin(), _terminalHistory.begin() + excess);
  }

  // If the minimum number of experiences has not yet been reached, there's not much else to do
  if (_stateHistory.size() < _replayMemoryStartSize) return;

  // Updating best reward so far, it the cumulative reward of this round exceeds it
  // and store the best hyperparameters used for that
  _averageTrainingReward = cumulativeReward / (double)_agentEpisodesPerGeneration;

  // Updating if exceeded best training policy so far.
  if (_averageTrainingReward > _bestAverageTrainingReward) _bestAverageTrainingReward = _averageTrainingReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_averageTrainingReward > _averageTrainingRewardThreshold)
  {
    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> tests(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      tests[i]["Sample Id"] = _currentSampleID++;
      tests[i]["Module"] = "Problem";
      tests[i]["Operation"] = "Run Testing";

      // Launching agent initialization
      KORALI_START(tests[i]);
    }

    KORALI_WAITALL(tests);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += tests[i]["Cumulative Reward"].get<double>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;

  // If we reached our target testing reward, there's nothing else to do
  if (_targetAverageTestingReward > -korali::Inf)
    if (_bestAverageTestingReward >= _targetAverageTestingReward) return;

  // Otherwise, continue training agent.
  trainAgent();
}

std::vector<double> Agent::getRandomAction(const std::vector<double> &state)
{
  // Storage for action
  std::vector<double> randomAction(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    double x = _randomActionGenerator->getRandomNumber();

    // If discrete value vector was not provided, use lower and upper bounds
    if (_k->_variables[varIdx]->_values.size() == 0)
    {
      double lowerBound = _k->_variables[varIdx]->_lowerBound;
      double upperBound = _k->_variables[varIdx]->_upperBound;
      randomAction[i] = lowerBound + x * (upperBound - lowerBound);
    }
    else
    {
      // Randomly select one of the actions provided in the value vector
      size_t valIdx = floor(x * _k->_variables[varIdx]->_values.size());
      randomAction[i] = _k->_variables[varIdx]->_values[valIdx];
    }
  }

  return randomAction;
}

void Agent::normalizeNeuralNetwork(korali::NeuralNetwork *nn)
{
  size_t pCount = nn->_normalizationParameterCount;

  std::vector<double> newMeans(pCount, 0.0);
  std::vector<double> newVariances(pCount, 0.0);

  std::vector<std::vector<double>> miniBatch(nn->_batchSize);
  for (size_t i = 0; i < nn->_batchSize; i++) miniBatch[i].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Creating state history indexes to choose from
  std::vector<size_t> stateHistoryIndexes(_stateHistory.size());
  for (size_t i = 0; i < _stateHistory.size(); i++) stateHistoryIndexes[i] = i;

  for (size_t step = 0; step < _normalizationSteps; step++)
  {
    // Shuffling indexes to choose the mini batch from
    std::shuffle(stateHistoryIndexes.begin(), stateHistoryIndexes.end(), *mt);

    for (size_t i = 0; i < nn->_batchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = stateHistoryIndexes[i];

      for (size_t j = 0; j < _problem->_stateVectorSize; j++) miniBatch[i][j] = _stateHistory[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) miniBatch[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
    }

    nn->setInput(miniBatch);
    nn->forward();

    // Getting this mini-batch's normalization means and variances
    auto newMeans = nn->getNormalizationMeans();
    auto newVariances = nn->getNormalizationVariances();

    // Accuulating their values into the new vector
    for (size_t i = 0; i < pCount; i++) newMeans[i] += newMeans[i];
    for (size_t i = 0; i < pCount; i++) newVariances[i] += newVariances[i];
  }

  // Calculating the actual mean and variance averages
  for (size_t i = 0; i < pCount; i++)
  {
    newMeans[i] = newMeans[i] / _normalizationSteps;
    newVariances[i] = newVariances[i] / (_normalizationSteps - 1.0);
  }

  // Setting the new adjusted means and variances
  nn->setNormalizationMeans(newMeans);
  nn->setNormalizationVariances(newVariances);
}

std::vector<double> Agent::getInferenceAction(const std::vector<double> &state)
{
  // Querying the policy for the action, given the state
  auto action = queryPolicy(state);

  // Returning action
  return action;
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:            %lu/%lu\n", _stateHistory.size(), _replayMemoryMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:            %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:            %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:          %lu/%lu\n", _totalExperienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:          %lu\n", _totalExperienceCount);

  _k->_logger->logInfo("Normal", " + Current Max Experiences/Episode: %lu\n", _maxExperienceCount);
  _k->_logger->logInfo("Normal", " + Current Avg Experiences/Episode: %.0f\n", _averageExperienceCount);
  _k->_logger->logInfo("Normal", " + Current Min Experiences/Episode: %lu\n", _minExperienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  _k->_logger->logInfo("Normal", " + Average Training Reward:         %f/%f\n", _averageTrainingReward, _averageTrainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Average Training Reward:    %f\n", _bestAverageTrainingReward);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies Tested:       %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Testing Reward:   %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Testing Reward:     %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Testing Reward:     %f\n", _bestAverageTestingReward);

  printAgentInformation();
}

} // namespace solver

} // namespace korali

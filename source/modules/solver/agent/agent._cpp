#include "auxiliar/fs.hpp"
#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <algorithm>
#include <chrono>

#define COSREWARD

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  _variableCount = _k->_variables.size();

  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _stateVector.resize(_experienceReplayMaximumSize);
  _actionVector.resize(_experienceReplayMaximumSize);
  _featureVector.resize(_experienceReplayMaximumSize);
  _retraceValueVector.resize(_experienceReplayMaximumSize);
  _retraceLastUpdateVector.resize(_experienceReplayMaximumSize);
  _rewardVector.resize(_experienceReplayMaximumSize);
  _stateValueVector.resize(_experienceReplayMaximumSize);
  _importanceWeightVector.resize(_experienceReplayMaximumSize);
  _truncatedImportanceWeightVector.resize(_experienceReplayMaximumSize);
  _truncatedStateValueVector.resize(_experienceReplayMaximumSize);
  _truncatedStateVector.resize(_experienceReplayMaximumSize);
  _terminationVector.resize(_experienceReplayMaximumSize);
  _expPolicyVector.resize(_experienceReplayMaximumSize);
  _curPolicyVector.resize(_experienceReplayMaximumSize);
  _isOnPolicyVector.resize(_experienceReplayMaximumSize);
  _episodePosVector.resize(_experienceReplayMaximumSize);
  _episodeIdVector.resize(_experienceReplayMaximumSize);

  // Pre-allocate space for policies
  _policyVector.resize(_experienceReplayMaximumSize);

  // Initialize background samples
  _backgroundSampleSize = 0;
  _statisticLogPartitionFunction.resize(0);
  _statisticFusionLogPartitionFunction.resize(0);
  _statisticFeatureWeights.resize(0);
  _statisticCumulativeRewards.resize(0);

  //  Pre-allocating space for state time sequence
  _stateTimeSequence.resize(_timeSequenceLength);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _testingCandidateCount = 0;
    _currentSampleID = 0;
    _experienceCount = 0;

    // Initializing training and episode statistics
    _testingAverageReward = -korali::Inf;
    _testingStdevReward = +korali::Inf;
    _testingBestReward = -korali::Inf;
    _testingWorstReward = +korali::Inf;
    _trainingBestReward = -korali::Inf;
    _trainingBestEpisodeId = 0;
    _trainingAverageReward = 0.0f;
    _testingPreviousAverageReward = -korali::Inf;
    _testingBestAverageReward = -korali::Inf;
    _testingBestEpisodeId = 0;

    // Initializing REFER information
    _experienceReplayOffPolicyRatio = 0.0f;
    _experienceReplayOffPolicyCurrentCutoff = _experienceReplayOffPolicyCutoffScale + 1.0f;
    _currentLearningRate = _learningRate;

    _stateRescalingMeans.resize(_problem->_stateVectorSize, 0.0);
    _stateRescalingSdevs.resize(_problem->_stateVectorSize, 1.0);

    _rewardRescalingMean = 0.0;
    _rewardRescalingSdev = 1.0;
    _rewardRescalingCount = 0;
  }

  // If this continues a previous training run, deserialize previous input experience replay
  if (_k->_currentGeneration > 0)
    if (_mode == "Training" || _testingBestPolicy.empty())
      deserializeExperienceReplay();

  // Getting agent's initial policy
  _trainingCurrentPolicy = getAgentPolicy();

  // Initializing session-wise profiling timers
  _sessionRunningTime = 0.0;
  _sessionSerializationTime = 0.0;
  _sessionAgentComputationTime = 0.0;
  _sessionAgentCommunicationTime = 0.0;
  _sessionAgentPolicyEvaluationTime = 0.0;
  _sessionPolicyUpdateTime = 0.0;
  _sessionAgentAttendingTime = 0.0;
  _sessionAgentTrajectoryLogProbilityUpdateTime = 0.0;

  // Initializing session-specific counters
  _sessionExperienceCount = 0;
  _sessionEpisodeCount = 0;
  _sessionGeneration = 1;
  _sessionPolicyUpdateCount = 0;

  // Calculating how many more experiences do we need in this session to reach the starting size
  _sessionExperiencesUntilStartSize = _stateVector.size() > _experienceReplayStartSize ? 0 : _experienceReplayStartSize - _stateVector.size();

  if (_mode == "Training")
  {
    // Creating storate for _agents and their status
    _agents.resize(_agentCount);
    _isAgentRunning.resize(_agentCount, false);
  }

  if (_mode == "Testing")
  {
    // Fixing termination criteria for testing mode
    _maxGenerations = _k->_currentGeneration + 1;

    // Setting testing policy to best testing hyperparameters if not custom-set by the user
    if (_testingPolicy.empty())
    {
      // Checking if testing policies have been generated
      if (_testingBestPolicy.empty())
      {
        _k->_logger->logWarning("Minimal", "Trying to test policy, but no testing policies have been generated during training yet or given in the configuration. Using current training policy instead.\n");
        _testingPolicy = _trainingCurrentPolicy;
      }
      else
      {
        _testingPolicy = _testingBestPolicy;
      }
    }

    // Checking if there's testing samples defined
    if (_testingSampleIds.size() == 0)
      KORALI_LOG_ERROR("For testing, you need to indicate the sample ids to run in the ['Testing']['Sample Ids'] field.\n");

    // Prepare storage for rewards from tested samples
    _testingReward.resize(_testingSampleIds.size());
  }

  _featureWeights.resize(_problem->_featureVectorSize, 1.0 / (float)_problem->_featureVectorSize);
  _featureWeightGradient.resize(_problem->_featureVectorSize, 0.0);
  _softMaxFeatureWeights.resize(_problem->_featureVectorSize, 1.0 / (float)_problem->_featureVectorSize);
}

void Agent::runGeneration()
{
  if (_mode == "Training") trainingGeneration();
  if (_mode == "Testing") testingGeneration();
}

void Agent::trainingGeneration()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Setting generation-specific timers
  _generationRunningTime = 0.0;
  _generationSerializationTime = 0.0;
  _generationAgentComputationTime = 0.0;
  _generationAgentCommunicationTime = 0.0;
  _generationAgentPolicyEvaluationTime = 0.0;
  _generationPolicyUpdateTime = 0.0;
  _generationAgentAttendingTime = 0.0;

  // Running until all _agents have finished
  while (_sessionEpisodeCount < _episodesPerGeneration * _sessionGeneration)
  {
    // Launching (or re-launching) agents
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == false)
      {
        _agents[agentId]["Sample Id"] = _currentEpisode++;
        _agents[agentId]["Module"] = "Problem";
        _agents[agentId]["Operation"] = "Run Training Episode";
        _agents[agentId]["Policy Hyperparameters"] = _trainingCurrentPolicy;
        _agents[agentId]["State Rescaling"]["Means"] = _stateRescalingMeans;
        _agents[agentId]["State Rescaling"]["Standard Deviations"] = _stateRescalingSdevs;

        KORALI_START(_agents[agentId]);
        _isAgentRunning[agentId] = true;
      }

    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    // Attending to running agents, checking if any experience has been received
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
        attendAgent(agentId);

    // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
    if (_experienceCount >= _experienceReplayStartSize)
    {
      // Update the reward function based on guided cost learning
      if (_sessionExperienceCount > (_experiencesBetweenRewardUpdates * _sessionPolicyUpdateCount + _sessionExperiencesUntilStartSize))
      {
        updateBackgroundBatch();
        updateDemonstrationBatch();
        updateRewardFunction();
      }

      if (_backgroundSampleSize >= (_statisticLogPartitionFunction.size()) * (_backgroundBatchSize * 10))
      {
        printf("bss %zu, slpf %zu\n", _backgroundSampleSize, _statisticLogPartitionFunction.size());
        partitionFunctionStat();
      }

      // If we performed enough policy updates, we rescale rewards again
      // if (_policyUpdateCount >= _updatesBetweenRewardRescaling * _rewardRescalingCount)
      //  rescaleRewards();

      //      // If we accumulated enough experiences, we rescale the states (once)
      //      if(_policyUpdateCount == 0)
      //      {
      //        // Calculation of state moments
      //        std::vector<float> sumStates(_problem->_stateVectorSize, 0.0);
      //        std::vector<float> squaredSumStates(_problem->_stateVectorSize, 0.0);
      //        for(size_t i = 0; i < _stateVector.size(); ++i)
      //         for(size_t d = 0; d < _problem->_stateVectorSize; ++d)
      //         {
      //          sumStates[d] += _stateVector[i][d];
      //          squaredSumStates[d] += _stateVector[i][d]*_stateVector[i][d];
      //         }
      //
      //        for(size_t d = 0; d < _problem->_stateVectorSize; ++d)
      //        {
      //            printf("state [%zu] : sum %lf squaredsum  %lf\n", d, sumStates[d], squaredSumStates[d]);
      //            _stateRescalingMeans[d] = sumStates[d]/(float) _stateVector.size();
      //            if(std::isfinite(_stateRescalingMeans[d]) == false) _stateRescalingMeans[d] = 0.;
      //            _stateRescalingSdevs[d] = std::sqrt( squaredSumStates[d] /(float) _stateVector.size() - _stateRescalingMeans[d]*_stateRescalingMeans[d] );
      //            if(std::isfinite(_stateRescalingSdevs[d]) == false) _stateRescalingSdevs[d] = 1.;
      //            if(_stateRescalingSdevs[d] <= 1e-9 ) _stateRescalingSdevs[d] = 1.;
      //            printf("state [%zu] : mean %f sdev %f\n", d, _stateRescalingMeans[d], _stateRescalingSdevs[d]);
      //        }
      //
      //       // Actual rescaling of initial states
      //       for(size_t i = 0; i < _stateVector.size(); ++i)
      //            for(size_t d = 0; d < _problem->_stateVectorSize; ++d)
      //                _stateVector[i][d] = (_stateVector[i][d] - _stateRescalingMeans[d])/_stateRescalingSdevs[d];
      //      }

      // If we accumulated enough experiences between updates in this session, update now
      while (_sessionExperienceCount > (_experiencesBetweenPolicyUpdates * _sessionPolicyUpdateCount + _sessionExperiencesUntilStartSize))
      {
        auto beginTime = std::chrono::steady_clock::now(); // Profiling

        // Calling the algorithm specific policy training algorithm
        trainPolicy();

        auto endTime = std::chrono::steady_clock::now();                                                                  // Profiling
        _sessionPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
        _generationPolicyUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

        // Increasing policy update counters
        _policyUpdateCount++;
        _sessionPolicyUpdateCount++;

        // Updating REFER learning rate and beta parameters
        _currentLearningRate = _learningRate / (1.0f + _experienceReplayOffPolicyAnnealingRate * (float)_policyUpdateCount);
        if (_experienceReplayOffPolicyRatio > _experienceReplayOffPolicyTarget)
          _experienceReplayOffPolicyREFERBeta = (1.0f - _currentLearningRate) * _experienceReplayOffPolicyREFERBeta;
        else
          _experienceReplayOffPolicyREFERBeta = (1.0f - _currentLearningRate) * _experienceReplayOffPolicyREFERBeta + _currentLearningRate;
      }

      // Getting new policy hyperparameters (for agents to generate actions)
      _trainingCurrentPolicy = getAgentPolicy();
    }
  }

  // Now serializing experience replay database
  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      if (_k->_fileOutputFrequency > 0)
        if (_k->_currentGeneration % _k->_fileOutputFrequency == 0)
          serializeExperienceReplay();

  // Measuring generation time
  auto endTime = std::chrono::steady_clock::now();                                                             // Profiling
  _sessionRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  // Increasing session's generation count
  _sessionGeneration++;
}

void Agent::testingGeneration()
{
  // Allocating testing agents
  std::vector<Sample> testingAgents(_testingSampleIds.size());

  // Launching  agents
  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
  {
    testingAgents[agentId]["Sample Id"] = _testingSampleIds[agentId];
    testingAgents[agentId]["Module"] = "Problem";
    testingAgents[agentId]["Operation"] = "Run Testing Episode";
    testingAgents[agentId]["Policy Hyperparameters"] = _testingPolicy;

    KORALI_START(testingAgents[agentId]);
  }

  KORALI_WAITALL(testingAgents);

  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
    _testingReward[agentId] = testingAgents[agentId]["Testing Reward"].get<float>();
}

float Agent::calculateReward(const std::vector<float> &features)
{
  float reward = 0.0;
#ifdef COSREWARD
  reward = std::cos(features[0] - _featureWeights[0]);
#else
  for (size_t i = 0; i < _problem->_featureVectorSize; ++i) reward += _softMaxFeatureWeights[i] * features[i];
#endif
  return reward;
}

std::vector<float> Agent::calculateRewardGradient(const std::vector<float> &features)
{
  std::vector<float> rewardGrad(features.size(), 0.0);
#ifdef COSREWARD
  rewardGrad[0] = std::sin(features[0] - _featureWeights[0]);
#else
  for (size_t i = 0; i < _problem->_featureVectorSize; ++i)
    for (size_t j = 0; j < _problem->_featureVectorSize; ++j)
    {
      if (i == j)
        rewardGrad[i] += _softMaxFeatureWeights[i] * (1. - _softMaxFeatureWeights[i]) * features[i];
      else
        rewardGrad[j] -= _softMaxFeatureWeights[i] * _softMaxFeatureWeights[j] * features[i];
    }
#endif
  return rewardGrad;
}

float Agent::evaluateTrajectoryLogProbability(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions, const std::vector<float> &policyHyperparameter)
{
  knlohmann::json policy;
  policy["Policy"] = policyHyperparameter;
  setAgentPolicy(policy);

  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    auto evaluation = runPolicy({{states[t]}})[0];
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation.actionMeans[d], evaluation.actionSigmas[d]);
  }

  return trajectoryLogProbability;
}

float Agent::evaluateTrajectoryLogProbabilityWithLinearPolicy(const std::vector<std::vector<float>> &states, const std::vector<std::vector<float>> &actions, const std::vector<std::vector<float>> &linearWeights, const std::vector<float> &sigmas)
{
  float trajectoryLogProbability = 0.0;
  // Evaluate all states within a single trajectory and calculate probability of trajectory
  for (size_t t = 0; t < states.size(); ++t)
  {
    std::vector<float> evaluation(_problem->_actionVectorSize);
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
    {
      // Predict action with linear policy
      evaluation[d] = linearWeights[d][0];
      for (size_t j = 0; j < _problem->_stateVectorSize; ++j)
        evaluation[d] += states[t][j] * linearWeights[d][j + 1];
    }
    for (size_t d = 0; d < _problem->_actionVectorSize; ++d)
      trajectoryLogProbability += normalLogDensity(actions[t][d], evaluation[d], sigmas[d]);
  }

  return trajectoryLogProbability;
}

void Agent::updateBackgroundBatch()
{
  auto startTime = std::chrono::steady_clock::now();

  // Initialize background batch
  if (_backgroundSampleSize == 0)
  {
    printf("Initializing log probabilities background batch ... ");

    // Getting index of last experience
    size_t expId = _terminationVector.size() - 1;

    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      size_t episodeLength = _episodePosVector[expId];
      size_t episodeStartIdx = expId - episodeLength;

      // Allocate container for trajectory information
      std::vector<std::vector<float>> trajectoryStates(episodeLength, std::vector<float>(_problem->_stateVectorSize));
      std::vector<std::vector<float>> trajectoryActions(episodeLength, std::vector<float>(_problem->_actionVectorSize));
      std::vector<std::vector<float>> trajectoryFeatures(episodeLength, std::vector<float>(_problem->_featureVectorSize));

      // Store background trajectory
      for (size_t i = 0; i < episodeLength; ++i)
      {
        for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
          trajectoryStates[i][d] = _stateVector[episodeStartIdx + i][d];
        for (size_t a = 0; a < _problem->_actionVectorSize; ++a)
          trajectoryActions[i][a] = _actionVector[episodeStartIdx + i][a];
        for (size_t f = 0; f < _problem->_featureVectorSize; ++f)
          trajectoryFeatures[i][f] = _featureVector[episodeStartIdx + i][f];
      }
      _backgroundTrajectoryStates.push_back(trajectoryStates);
      _backgroundTrajectoryActions.push_back(trajectoryActions);
      _backgroundTrajectoryFeatures.push_back(trajectoryFeatures);
      _backgroundPolicyHyperparameter.push_back(_policyVector[episodeStartIdx]);

      // Increase background sample counter
      _backgroundSampleSize++;

      if (episodeStartIdx == 0)
        KORALI_LOG_ERROR("Increase exploration phase, not enough trajectories sampled");

      // Decrease counter to move to the next previous last experience of a trajectory
      expId = episodeStartIdx - 1;

      if (_terminationVector[expId] == e_nonTerminal)
        KORALI_LOG_ERROR("Experience %zu is not the start of a trajectory.", expId);
    }

    if (_backgroundSampleSize != _backgroundBatchSize)
      KORALI_LOG_ERROR("Error during background batch intialization or update. Size is %zu but should be %zu.", _backgroundSampleSize, _backgroundBatchSize);

    // Evaluate all trajectory logprobabilities, at the beginning all trajectories sampled from same arbitrary policy
    _backgroundTrajectoryLogProbabilities.resize(_backgroundSampleSize);
    for (size_t i = 0; i < _backgroundSampleSize; ++i)
    {
      _backgroundTrajectoryLogProbabilities[i].resize(_backgroundSampleSize + 1);
      float trajectoryLogP = evaluateTrajectoryLogProbability(_backgroundTrajectoryStates[i], _backgroundTrajectoryActions[i], _backgroundPolicyHyperparameter[i]);
      if (_useFusionDistribution)
      {
        // Insert probability from linear policy first
        _backgroundTrajectoryLogProbabilities[i][0] = evaluateTrajectoryLogProbabilityWithLinearPolicy(_backgroundTrajectoryStates[i], _backgroundTrajectoryActions[i], _problem->_observationsApproximatorWeights, _problem->_observationsApproximatorSigmas);
        for (size_t j = 0; j < _backgroundSampleSize; ++j)
          _backgroundTrajectoryLogProbabilities[i][j + 1] = trajectoryLogP;
      }
      else
      {
        _backgroundTrajectoryLogProbabilities[i][i + 1] = trajectoryLogP;
      }
    }
  }
  // Insert latest trajectory to background batch
  else
  {
    printf("Updating log probabilities background batch ... ");

    size_t expId = _terminationVector.size() - 1;
    size_t episodeLength = _episodePosVector[expId];
    size_t episodeStartIdx = expId - episodeLength;

    // Allocate container for trajectory information
    std::vector<std::vector<float>> trajectoryStates(episodeLength, std::vector<float>(_problem->_stateVectorSize));
    std::vector<std::vector<float>> trajectoryActions(episodeLength, std::vector<float>(_problem->_actionVectorSize));
    std::vector<std::vector<float>> trajectoryFeatures(episodeLength, std::vector<float>(_problem->_featureVectorSize));

    // Store background trajectory
    for (size_t i = 0; i < episodeLength; ++i)
    {
      for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
        trajectoryStates[i][d] = _stateVector[episodeStartIdx + i][d];
      for (size_t a = 0; a < _problem->_actionVectorSize; ++a)
        trajectoryActions[i][a] = _actionVector[episodeStartIdx + i][a];
      for (size_t f = 0; f < _problem->_featureVectorSize; ++f)
        trajectoryFeatures[i][f] = _featureVector[episodeStartIdx + i][f];
    }
    _backgroundTrajectoryStates.push_back(trajectoryStates);
    _backgroundTrajectoryActions.push_back(trajectoryActions);
    _backgroundTrajectoryFeatures.push_back(trajectoryFeatures);
    _backgroundPolicyHyperparameter.push_back(_policyVector[episodeStartIdx]);

    // Increase background sample counter
    _backgroundSampleSize++;
    if (_backgroundPolicyHyperparameter.size() != _backgroundSampleSize) KORALI_LOG_ERROR("Vector mismatch");
    if (_terminationVector[episodeStartIdx - 1] == e_nonTerminal)
      KORALI_LOG_ERROR("Experience %zu is not the start of a trajectory.", episodeStartIdx - 1);

    // For all previous background trajectories evaluate log probability with newest policy
    if (_useFusionDistribution)
      for (size_t i = 0; i < _backgroundSampleSize - 1; ++i)
        _backgroundTrajectoryLogProbabilities[i].push_back(evaluateTrajectoryLogProbability(_backgroundTrajectoryStates[i], _backgroundTrajectoryActions[i], _backgroundPolicyHyperparameter[_backgroundSampleSize - 1]));

    // For newest policy evaluate trajectory log probability with all previous policies and the current one
    std::vector<float> logProbabilitiesNewTrajectory(_backgroundSampleSize + 1);
    if (_useFusionDistribution)
    {
      logProbabilitiesNewTrajectory[0] = evaluateTrajectoryLogProbabilityWithLinearPolicy(_backgroundTrajectoryStates[_backgroundSampleSize - 1], _backgroundTrajectoryActions[_backgroundSampleSize - 1], _problem->_observationsApproximatorWeights, _problem->_observationsApproximatorSigmas);
      for (size_t i = 0; i < _backgroundSampleSize; ++i)
        logProbabilitiesNewTrajectory[i + 1] = evaluateTrajectoryLogProbability(_backgroundTrajectoryStates[_backgroundSampleSize - 1], _backgroundTrajectoryActions[_backgroundSampleSize - 1], _backgroundPolicyHyperparameter[i]);
    }
    else
      logProbabilitiesNewTrajectory[_backgroundSampleSize] = evaluateTrajectoryLogProbability(_backgroundTrajectoryStates[_backgroundSampleSize - 1], _backgroundTrajectoryActions[_backgroundSampleSize - 1], _backgroundPolicyHyperparameter[_backgroundSampleSize - 1]);

    _backgroundTrajectoryLogProbabilities.push_back(logProbabilitiesNewTrajectory);

    auto endTime = std::chrono::steady_clock::now();
    double duration = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - startTime).count();
    _sessionAgentTrajectoryLogProbilityUpdateTime += duration;
  }

  auto endTime = std::chrono::steady_clock::now();
  double duration = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - startTime).count();
  _sessionAgentTrajectoryLogProbilityUpdateTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - startTime).count();
  printf("Done (%3.3fs)!\n", duration / 1.0e9);
}

void Agent::updateDemonstrationBatch()
{
  auto startTime = std::chrono::steady_clock::now();

  if (_demonstrationTrajectoryLogProbabilities.size() == 0)
  {
    printf("Initializing log probabilities demonstration batch ... ");
    _demonstrationTrajectoryLogProbabilities.resize(_problem->_numberObservedTrajectories);
    for (size_t m = 0; m < _problem->_numberObservedTrajectories; ++m)
    {
      _demonstrationTrajectoryLogProbabilities[m].resize(_backgroundSampleSize + 1);
      _demonstrationTrajectoryLogProbabilities[m][0] = evaluateTrajectoryLogProbabilityWithLinearPolicy(_problem->_observationsStates[m], _problem->_observationsActions[m], _problem->_observationsApproximatorWeights, _problem->_observationsApproximatorSigmas);

      if (_useFusionDistribution)
        for (size_t i = 0; i < _backgroundSampleSize; ++i)
        {
          float trajectoryLogP = evaluateTrajectoryLogProbability(_problem->_observationsStates[m], _problem->_observationsActions[m], _backgroundPolicyHyperparameter[i]);
          _demonstrationTrajectoryLogProbabilities[m][i + 1] = trajectoryLogP;
        }
    }
  }
  else
  // Evaluate demonstrations with latest policy
  {
    if (_useFusionDistribution)
    {
      printf("Updating log probabilities demonstration batch ... ");
      // For all previous demonstration trajectories evaluate log probability with newest policy
      for (size_t i = 0; i < _problem->_numberObservedTrajectories; ++i)
      {
        if (_demonstrationTrajectoryLogProbabilities[i].size() != _backgroundSampleSize)
          KORALI_LOG_ERROR("Sth strange, demo trans log probabilites vector (%zu) shall be of size background sample size (%zu)\n", _demonstrationTrajectoryLogProbabilities[i].size(), _backgroundSampleSize);
        _demonstrationTrajectoryLogProbabilities[i].push_back(evaluateTrajectoryLogProbability(_problem->_observationsStates[i], _problem->_observationsActions[i], _backgroundPolicyHyperparameter[_backgroundSampleSize - 1]));
      }
    }
  }

  auto endTime = std::chrono::steady_clock::now();
  double duration = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - startTime).count();
  _sessionAgentTrajectoryLogProbilityUpdateTime += duration;
  printf("Done (%3.3fs)!\n", duration / 1.0e9);
}

void Agent::partitionFunctionStat()
{
  std::vector<std::vector<float>> stats;
  std::vector<float> logpf(_backgroundSampleSize);
  std::vector<float> fusionLogpf(_backgroundSampleSize);
  
  // Calculate cumulative rewards for background batch
  std::vector<float> cumulativeRewardsBackgroundBatch(_backgroundSampleSize, 0.0);
#pragma omp parallel for
  for (size_t m = 0; m < _backgroundSampleSize; ++m)
  {
    size_t backgroundTrajectoryLength = _backgroundTrajectoryFeatures[m].size();

    float cumReward = 0.;
    for (size_t t = 0; t < backgroundTrajectoryLength; ++t)
      cumReward += calculateReward(_backgroundTrajectoryFeatures[m][t]);
    cumulativeRewardsBackgroundBatch[m] = cumReward;
  }

  unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
  std::minstd_rand0 generator(seed);

  // Randomize demonstration batch
  std::vector<size_t> randomDemonstrationIndexes(_problem->_numberObservedTrajectories);
  std::iota(std::begin(randomDemonstrationIndexes), std::end(randomDemonstrationIndexes), 0);
  std::shuffle(randomDemonstrationIndexes.begin(), randomDemonstrationIndexes.end(), generator);

  // Calculate cumulative rewards for demonstration batch
  std::vector<float> cumulativeRewardsDemonstrationBatch(_demonstrationBatchSize, 0.0);
#pragma omp parallel for
  for (size_t n = 0; n < _demonstrationBatchSize; ++n)
  {
    size_t obsIdx = randomDemonstrationIndexes[n];
    size_t observationTrajectoryLength = _problem->_observationsFeatures[obsIdx].size();

    float cumReward = 0.;
    for (size_t t = 0; t < observationTrajectoryLength; ++t)
      cumReward += calculateReward(_problem->_observationsFeatures[obsIdx][t]);
    cumulativeRewardsDemonstrationBatch[n] = cumReward;
  }

#pragma omp parallel for
  for (size_t batchSize = 1; batchSize <= _backgroundSampleSize; ++batchSize)
  {

    // Get background trajectory log probabilities
    std::vector<std::vector<float>> backgroundTrajectoryLogProbabilities(batchSize, std::vector<float>(batchSize + 1));
    for (size_t m = 0; m < batchSize; ++m)
      for (size_t i = 0; i < batchSize + 1; ++i)
      {
        backgroundTrajectoryLogProbabilities[m][i] = _backgroundTrajectoryLogProbabilities[m][i];
      }

    // Get demonstration trajectory log probabilities
    std::vector<std::vector<float>> demoTrajectoryLogProbabilities(_demonstrationBatchSize, std::vector<float>(batchSize + 1));
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      size_t obsIdx = randomDemonstrationIndexes[n];
      for (size_t i = 0; i < batchSize + 1; ++i)
      {
        demoTrajectoryLogProbabilities[n][i] = _demonstrationTrajectoryLogProbabilities[obsIdx][i];
      }
    }
    
    // Calculate importance weights background batch
    std::vector<float> backgroundBatchLogImportanceWeights(batchSize);
    std::vector<float> fusionBackgroundBatchLogImportanceWeights(batchSize);
    for (size_t m = 0; m < batchSize; ++m)
    {
      backgroundBatchLogImportanceWeights[m] = -backgroundTrajectoryLogProbabilities[m][m + 1];
      // Calculate fusion importance weight (1/K sum_k q_k(T))^-1
      fusionBackgroundBatchLogImportanceWeights[m] = std::log((float)batchSize + 1) - logSumExp(backgroundTrajectoryLogProbabilities[m]);
    }

    // Calculate importance weights demonstration batch
    std::vector<float> demoBatchLogImportanceWeights(_demonstrationBatchSize);
    std::vector<float> fusionDemoBatchLogImportanceWeights(_demonstrationBatchSize);
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      demoBatchLogImportanceWeights[n] = -demoTrajectoryLogProbabilities[n][0];
      // Calculate fusion importance weight (1/K sum_k q_k(T))^-1
      fusionDemoBatchLogImportanceWeights[n] = std::log((float)batchSize + 1) - logSumExp(demoTrajectoryLogProbabilities[n]);
    }
    
    // Preparation for calculation of log partition function with log-sum-exp trick
    float maxExp = -Inf;
    for (size_t m = 0; m < batchSize; ++m)
    {
      float exp = backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      if (exp > maxExp) maxExp = exp;
    }

    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = demoBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      if (exp > maxExp) maxExp = exp;
    }

    // Preparation for calculation of fusion log partition function with log-sum-exp trick
    float fusionMaxExp = -Inf;
    for (size_t m = 0; m < batchSize; ++m)
    {
      float exp = fusionBackgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      if (exp > fusionMaxExp) fusionMaxExp = exp;
    }

    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = fusionDemoBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      if (exp > fusionMaxExp) fusionMaxExp = exp;
    }

    float sumExpNoMax = 0.0;
    for (size_t m = 0; m < batchSize; ++m)
    {
      float exp = backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      sumExpNoMax += std::exp(exp - maxExp);
    }
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = demoBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      sumExpNoMax += std::exp(exp - maxExp);
    }

    // Calculate log of partition function
    float logPartitionFunction = std::log(sumExpNoMax) + maxExp - std::log((float)batchSize + (float)_demonstrationBatchSize);
    logpf[batchSize - 1] = logPartitionFunction;

    float fusionSumExpNoMax = 0.0;

    for (size_t m = 0; m < batchSize; ++m)
    {
      float exp = fusionBackgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      fusionSumExpNoMax += std::exp(exp - fusionMaxExp);
    }
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = fusionDemoBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      fusionSumExpNoMax += std::exp(exp - fusionMaxExp);
    }

    // Calculate log of fusion partition function
    float fusionLogPartitionFunction = std::log(fusionSumExpNoMax) + fusionMaxExp - std::log((float)batchSize + (float)_demonstrationBatchSize);
    fusionLogpf[batchSize - 1] = fusionLogPartitionFunction;
  }

  _statisticCumulativeRewards.push_back(cumulativeRewardsBackgroundBatch);
  _statisticLogPartitionFunction.push_back(logpf);
  _statisticFusionLogPartitionFunction.push_back(fusionLogpf);
  _statisticFeatureWeights.push_back(_featureWeights);
}

void Agent::updateRewardFunction()
{
  size_t stepsPerUpdate = 1;
  for (size_t stepNum = 0; stepNum < stepsPerUpdate; ++stepNum)
  {
    unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();
    std::minstd_rand0 generator(seed);

    // Randomize demonstration batch
    std::vector<size_t> randomDemonstrationIndexes(_problem->_numberObservedTrajectories);
    std::iota(std::begin(randomDemonstrationIndexes), std::end(randomDemonstrationIndexes), 0);
    std::shuffle(randomDemonstrationIndexes.begin(), randomDemonstrationIndexes.end(), generator);

    // Randomize background batch
    std::vector<size_t> randomBackgroundIndexes(_backgroundSampleSize);
    std::iota(std::begin(randomBackgroundIndexes), std::end(randomBackgroundIndexes), 0);
    std::shuffle(randomBackgroundIndexes.begin(), randomBackgroundIndexes.end(), generator);

    // Calculate cumulative rewards for demonstration batch and extract trajectory probabilities
    std::vector<float> cumulativeRewardsDemonstrationBatch(_demonstrationBatchSize, 0.0);
    std::vector<std::vector<float>> demonstrationTrajectoryLogProbabilities(_demonstrationBatchSize, std::vector<float>(_backgroundBatchSize + 1));
#pragma omp parallel for schedule(dynamic, 1)
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      size_t demIdx = randomDemonstrationIndexes[n];
      size_t observedTrajectoryLength = _problem->_observationsFeatures[demIdx].size();

      float cumReward = 0.0;
      for (size_t t = 0; t < observedTrajectoryLength; ++t)
        cumReward += calculateReward(_problem->_observationsFeatures[n][t]);
      cumulativeRewardsDemonstrationBatch[n] = cumReward;

      demonstrationTrajectoryLogProbabilities[n][0] = _demonstrationTrajectoryLogProbabilities[demIdx][0];
      for (size_t i = 0; i < _backgroundBatchSize; ++i)
      {
        size_t bckIdx = randomBackgroundIndexes[i];
        demonstrationTrajectoryLogProbabilities[n][i + 1] = _demonstrationTrajectoryLogProbabilities[demIdx][bckIdx + 1];
      }
    }

    // Calculate cumulative rewards for randomized background batch and extract trajectory probabilities
    std::vector<float> cumulativeRewardsBackgroundBatch(_backgroundBatchSize, 0.0);
    std::vector<std::vector<float>> backgroundTrajectoryLogProbabilities(_backgroundBatchSize, std::vector<float>(_backgroundBatchSize + 1));
#pragma omp parallel for schedule(dynamic, 1)
    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      size_t bckIdx = randomBackgroundIndexes[m];
      size_t backgroundTrajectoryLength = _backgroundTrajectoryFeatures[bckIdx].size();

      float cumReward = 0.;
      for (size_t t = 0; t < backgroundTrajectoryLength; ++t)
        cumReward += calculateReward(_backgroundTrajectoryFeatures[bckIdx][t]);
      cumulativeRewardsBackgroundBatch[m] = cumReward;
      backgroundTrajectoryLogProbabilities[m][0] = _backgroundTrajectoryLogProbabilities[bckIdx][0]; // probability from linear policy
      for (size_t i = 0; i < _backgroundBatchSize; ++i)
      {
        size_t bckIdx2 = randomBackgroundIndexes[i];
        backgroundTrajectoryLogProbabilities[m][i + 1] = _backgroundTrajectoryLogProbabilities[bckIdx][bckIdx2 + 1];
      }
    }

    // Calculate importance weights of background batch
    std::vector<float> backgroundBatchLogImportanceWeights(_backgroundBatchSize);
#pragma omp parallel for schedule(dynamic, 1)
    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      // Caclculate importance weight (1/K sum_k q_k(T))^-1
      if (_useFusionDistribution)
        backgroundBatchLogImportanceWeights[m] = std::log((float)_backgroundBatchSize + 1.) - logSumExp(backgroundTrajectoryLogProbabilities[m]);
      else
        backgroundBatchLogImportanceWeights[m] = -backgroundTrajectoryLogProbabilities[m][m + 1];
      //printf("BbIw %f cr %f (tot %f)\n", backgroundBatchLogImportanceWeights[m], cumulativeRewardsBackgroundBatch[m], backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m]);
    }

    // Calculate importance weights of demonstration batch
    std::vector<float> demonstrationBatchLogImportanceWeights(_demonstrationBatchSize);
#pragma omp parallel for schedule(dynamic, 1)
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      // Caclculate importance weight (1/K sum_k q_k(T))^-1
      if (_useFusionDistribution)
        demonstrationBatchLogImportanceWeights[n] = std::log((float)_backgroundBatchSize + 1.) - logSumExp(demonstrationTrajectoryLogProbabilities[n]);
      else
        demonstrationBatchLogImportanceWeights[n] = -demonstrationTrajectoryLogProbabilities[n][0];
      //printf("DbIw %f cr %f (tot %f)\n", demonstrationBatchLogImportanceWeights[n], cumulativeRewardsDemonstrationBatch[n], demonstrationBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n]);
    }

    // Preparation for calculation of log partition function with log-sum-exp trick
    float maxExp = -Inf;
    float maxSquaredExp = -Inf;
    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      float exp = backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      if (exp > maxExp) maxExp = exp;
      if (2. * exp > maxSquaredExp) maxSquaredExp = 2. * exp;
    }

    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = demonstrationBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      if (exp > maxExp) maxExp = exp;
      if (2. * exp > maxSquaredExp) maxSquaredExp = 2. * exp;
    }

    float sumExpNoMax = 0.0;
    float sumSquaredExpNoMax = 0.0;

    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      float exp = backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m];
      sumExpNoMax += std::exp(exp - maxExp);
      sumSquaredExpNoMax += std::exp(2. * exp - maxSquaredExp);
    }

    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      float exp = demonstrationBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n];
      sumExpNoMax += std::exp(exp - maxExp);
      sumSquaredExpNoMax += std::exp(2. * exp - maxSquaredExp);
    }

    // Calculate log of partition function
    float totalBatchSize = _backgroundBatchSize + _demonstrationBatchSize;
    _logPartitionFunction = std::log(sumExpNoMax) + maxExp - std::log(totalBatchSize);
    //printf("lpf %f\n", _logPartitionFunction);

    // Reset gradient
    std::fill(_featureWeightGradient.begin(), _featureWeightGradient.end(), 0);

    // Calculate gradient of background batch trajectory returns
    std::vector<std::vector<float>> gradBackgroundTrajReturn(_backgroundBatchSize, std::vector<float>(_problem->_featureVectorSize, 0.0));
#pragma omp parallel for
    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      size_t bckIdx = randomBackgroundIndexes[m];
      size_t backgroundTrajectoryLength = _backgroundTrajectoryFeatures[bckIdx].size();
      for (size_t t = 0; t < backgroundTrajectoryLength; ++t)
      {
        auto gradReward = calculateRewardGradient(_backgroundTrajectoryFeatures[bckIdx][t]);
        for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
          gradBackgroundTrajReturn[m][k] += gradReward[k];
      }
    }

    // Calculate gradient of demonstration batch trajectory returns
    std::vector<std::vector<float>> gradDemoTrajReturn(_backgroundBatchSize, std::vector<float>(_problem->_featureVectorSize, 0.0));
#pragma omp parallel for
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      size_t obsIdx = randomDemonstrationIndexes[n];
      size_t demonstrationTrajectoryLength = _problem->_observationsFeatures[obsIdx].size();
      for (size_t t = 0; t < demonstrationTrajectoryLength; ++t)
      {
        auto gradReward = calculateRewardGradient(_problem->_observationsFeatures[obsIdx][t]);
        for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
          gradDemoTrajReturn[n][k] += gradReward[k];
      }
    }

    // Calculate gradient of loglikelihood wrt. feature weights (contribution from partition function & background batch)
    float invTotalBatchSize = 1. / totalBatchSize;
#pragma omp parallel for
    for (size_t m = 0; m < _backgroundBatchSize; ++m)
    {
      for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
      {
        _featureWeightGradient[k] += std::exp(backgroundBatchLogImportanceWeights[m] + cumulativeRewardsBackgroundBatch[m] - _logPartitionFunction) * gradBackgroundTrajReturn[m][k] * invTotalBatchSize;
        //printf("grad bb %f %f\n", gradBackgroundTrajReturn[m][k], _featureWeightGradient[k]);
      }
    }

    float invDemoBatchSize = 1. / _demonstrationBatchSize;
    // Calculate gradient of loglikelihood wrt. feature weights (contribution from partition function, demonstration return & demonstration batch)
#pragma omp parallel for
    for (size_t n = 0; n < _demonstrationBatchSize; ++n)
    {
      for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
      {
        // Contribution from partition function
        _featureWeightGradient[k] += std::exp(demonstrationBatchLogImportanceWeights[n] + cumulativeRewardsDemonstrationBatch[n] - _logPartitionFunction) * gradDemoTrajReturn[n][k] * invTotalBatchSize;
        //printf("grad db %f %f\n", gradDemoTrajReturn[n][k], _featureWeightGradient[k]);

        // Contribution from demonstration return
        _featureWeightGradient[k] += invDemoBatchSize * gradDemoTrajReturn[n][k];
      }
    }

    //for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
    //printf("fwg[%zu] %f \n", k, _featureWeightGradient[k]);

    // L1 penalization
    /*
  float lambda = 1e-2;
  for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
  {
      if(_featureWeights[k] > 0.0)
        featureWeightGradient[k] -= lambda*_featureWeights[k];
      else
        featureWeightGradient[k] += lambda*_featureWeights[k];
  }
  */

    for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
    {
      _featureWeights[k] += _rewardfunctionLearningRate * _featureWeightGradient[k];
      //printf("fw [%zu] %f (%f)\n", k, _featureWeights[k], _featureWeightGradient[k]);
    }
  }

  // Calculate softmax of feature weights
  float normalization = 0.;
  for (size_t i = 0; i < _problem->_featureVectorSize; ++i)
  {
    float expFeature = std::exp(_featureWeights[i]);
    _softMaxFeatureWeights[i] = expFeature;
    normalization += expFeature;
  }

  for (size_t i = 0; i < _problem->_featureVectorSize; ++i)
  {
    _softMaxFeatureWeights[i] /= normalization;
    //printf("smfw[%zu] %f %f %f \n", i, _featureWeights[i], _featureWeightGradient[i], _softMaxFeatureWeights[i]);
  }
}

void Agent::rescaleRewards()
{
  float sumReward = 0.0;
  float sumReward2 = 0.0;

  // Calculate mean and standard deviation of unscaled rewards.
  for (size_t i = 0; i < _rewardVector.size(); i++)
  {
    float reward = _rewardVector[i];
    sumReward += reward;
    sumReward2 += reward * reward;
  }

  float newRewardRescalingSdev = std::sqrt(sumReward2 / ((float)_rewardVector.size() + 1e-9));

  printf("previous reward mean and sdev %f %f\n", _rewardRescalingMean, _rewardRescalingSdev);

  _rewardRescalingSdev = newRewardRescalingSdev;
  _rewardRescalingCount++;

  printf("new reward mean and sdev %f %f\n", _rewardRescalingMean, _rewardRescalingSdev);
}

void Agent::attendAgent(size_t agentId)
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Storage for the incoming message
  knlohmann::json message;

  // Retrieving the experience, if any has arrived for the current agent.
  if (_agents[agentId].retrievePendingMessage(message))
  {
    // Getting episode Id
    size_t episodeId = message["Sample Id"];

    // If agent requested new policy, send the new hyperparameters
    if (message["Action"] == "Request New Policy")
      KORALI_SEND_MSG_TO_SAMPLE(_agents[agentId], _trainingCurrentPolicy);

    if (message["Action"] == "Send Episode")
    {
      //  Now that we have the entire episode, process its experiences (add them to replay memory)
      processEpisode(episodeId, message["Experiences"], _agents[agentId]["Policy Hyperparameters"]["Policy"].get<std::vector<float>>());

      // Increasing total experience counters
      _experienceCount += message["Experiences"].size();
      _sessionExperienceCount += message["Experiences"].size();

      // Waiting for the agent to come back with all the information
      KORALI_WAIT(_agents[agentId]);

      // Getting the training reward of the latest episode
      _trainingLastReward = _agents[agentId]["Training Reward"].get<float>();

      // Keeping training statistics. Updating if exceeded best training policy so far.
      if (_trainingLastReward > _trainingBestReward)
      {
        _trainingBestReward = _trainingLastReward;
        _trainingBestEpisodeId = episodeId;
        _trainingBestPolicy = _agents[agentId]["Policy Hyperparameters"];
      }

      // If the policy has exceeded the threshold during training, we gather its statistics
      if (_agents[agentId]["Tested Policy"] == true)
      {
        _testingCandidateCount++;

        _testingPreviousAverageReward = _testingAverageReward;
        _testingAverageReward = _agents[agentId]["Average Testing Reward"].get<float>();
        _testingStdevReward = _agents[agentId]["Stdev Testing Reward"].get<float>();
        _testingBestReward = _agents[agentId]["Best Testing Reward"].get<float>();
        _testingWorstReward = _agents[agentId]["Worst Testing Reward"].get<float>();

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_testingAverageReward > _testingBestAverageReward)
        {
          _testingBestAverageReward = _testingAverageReward;
          _testingBestEpisodeId = episodeId;
          _testingBestPolicy = _agents[agentId]["Policy Hyperparameters"];
        }
      }

      // Obtaining profiling information
      _sessionAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _sessionAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _sessionAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();
      _generationAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _generationAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _generationAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();

      // Set agent as finished
      _isAgentRunning[agentId] = false;

      // Increasing session episode count
      _sessionEpisodeCount++;
    }
  }

  auto endTime = std::chrono::steady_clock::now();                                                                    // Profiling
  _sessionAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void Agent::processEpisode(size_t episodeId, knlohmann::json &episode, std::vector<float> policyHyperparameters)
{
  /*********************************************************************
  * Adding episode's experiences into the replay memory
  *********************************************************************/

  // Storage for the episode's cumulative reward
  float cumulativeReward = 0.0f;

  for (size_t expId = 0; expId < episode.size(); expId++)
  {
    // Getting state, action
    _stateVector.add(episode[expId]["State"].get<std::vector<float>>());
    _actionVector.add(episode[expId]["Action"].get<std::vector<float>>());
    _featureVector.add(episode[expId]["Features"].get<std::vector<float>>());
    _policyVector.add(policyHyperparameters);

    // Getting and normalizing reward
    const float unscaledReward = episode[expId]["Reward"].get<float>();
    const float reward = (unscaledReward - _rewardRescalingMean) / _rewardRescalingSdev;
    _rewardVector.add(reward);

    // Checking experience termination status and truncated state
    termination_t termination;
    std::vector<float> truncatedState;

    if (episode[expId]["Termination"] == "Non Terminal") termination = e_nonTerminal;
    if (episode[expId]["Termination"] == "Terminal") termination = e_terminal;
    if (episode[expId]["Termination"] == "Truncated")
    {
      termination = e_truncated;
      truncatedState = episode[expId]["Truncated State"].get<std::vector<float>>();
    }

    _terminationVector.add(termination);
    _truncatedStateVector.add(truncatedState);

    // Getting policy information and state value
    policy_t expPolicy;
    float stateValue;

    if (isDefined(episode[expId], "Policy", "State Value"))
    {
      expPolicy.stateValue = episode[expId]["Policy"]["State Value"].get<float>();
      stateValue = episode[expId]["Policy"]["State Value"].get<float>();
    }
    else
    {
      KORALI_LOG_ERROR("Policy has not produced state value for the current experience.\n");
    }

    if (isDefined(episode[expId], "Policy", "Action Means"))
      expPolicy.actionMeans = episode[expId]["Policy"]["Action Means"].get<std::vector<float>>();

    if (isDefined(episode[expId], "Policy", "Action Sigmas"))
      expPolicy.actionSigmas = episode[expId]["Policy"]["Action Sigmas"].get<std::vector<float>>();

    if (isDefined(episode[expId], "Policy", "Action Index"))
      expPolicy.actionIndex = episode[expId]["Policy"]["Action Index"].get<size_t>();

    if (isDefined(episode[expId], "Policy", "Action Probabilities"))
      expPolicy.actionProbabilities = episode[expId]["Policy"]["Action Probabilities"].get<std::vector<float>>();

    // Storing policy information
    _expPolicyVector.add(expPolicy);
    _curPolicyVector.add(expPolicy);
    _stateValueVector.add(stateValue);

    // Storing Episode information
    _episodeIdVector.add(episodeId);
    _episodePosVector.add(expId);

    // Keeping statistics
    cumulativeReward += calculateReward(episode[expId]["Features"].get<std::vector<float>>());

    // Adding placeholder for retrace value
    _retraceValueVector.add(0.0f);
    _retraceLastUpdateVector.add(0);

    // Updating experience's importance weight. Initially assumed to be 1.0 because its freshly produced
    _importanceWeightVector.add(1.0f);
    _truncatedImportanceWeightVector.add(1.0f);
    _isOnPolicyVector.add(true);
  }

  /*********************************************************************
   * Computing initial retrace value for the newly added experiences
   *********************************************************************/

  // Storage for the retrace value
  float retV = 0.0f;

  // Getting position of the final experience of the episode in the replay memory
  ssize_t endId = (ssize_t)_stateVector.size() - 1;

  // Getting the starting ID of the initial experience of the episode in the replay memory
  ssize_t startId = endId - (ssize_t)episode.size() + 1;

  // If it was a truncated episode, add the value function for the terminal state to retV
  if (_terminationVector[endId] == e_truncated)
  {
    // Get state sequence, appending the truncated state to it and removing first time element
    auto expTruncatedStateSequence = getTruncatedStateSequence(endId);

    // Calculating the state value function of the truncated state
    auto truncatedPolicy = runPolicy({expTruncatedStateSequence})[0];
    float truncatedV = truncatedPolicy.stateValue;

    // Sanity checks for truncated state value
    if (std::isfinite(truncatedV) == false)
      KORALI_LOG_ERROR("Calculated state value for truncated state returned an invalid value: %f\n", truncatedV);

    // Adding truncated state value to the retrace value
    retV += _discountFactor * truncatedV;
  }

  // Now going backwards, setting the retrace value of every experience
  for (ssize_t expId = endId; expId >= startId; expId--)
  {
    // Calculating retrace value with the discount factor. Importance weight is 1.0f because the policy is current.
    retV = _discountFactor * retV + calculateReward(_featureVector[expId]) * _rewardRescalingSdev;

    // Setting initial retrace value in the experience's cache
    _retraceValueVector[expId] = retV;
    _retraceLastUpdateVector[expId] = _policyUpdateCount;
  }

  /*********************************************************************
   * Updating statistics/bookkeeping
   *********************************************************************/

  // Storing history information
  _trainingRewardHistory.push_back(cumulativeReward);
  _trainingExperienceHistory.push_back(episode.size());

  // Updating average cumulative reward statistics
  _trainingAverageReward = 0.0f;
  ssize_t startEpisodeId = _trainingRewardHistory.size() - _trainingAverageDepth;
  ssize_t endEpisodeId = _trainingRewardHistory.size() - 1;
  if (startEpisodeId < 0) startEpisodeId = 0;
  for (ssize_t e = startEpisodeId; e <= endEpisodeId; e++)
    _trainingAverageReward += _trainingRewardHistory[e];
  _trainingAverageReward /= (float)(endEpisodeId - startEpisodeId + 1);
}

std::vector<size_t> Agent::generateMiniBatch(size_t miniBatchSize)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(miniBatchSize);

  for (size_t i = 0; i < miniBatchSize; i++)
  {
    // Producing random (uniform) number for the selection of the experience
    float x = _uniformGenerator->getRandomNumber();

    // Selecting experience
    size_t expId = std::floor(x * (float)(_stateVector.size() - 1));

    // Setting experience
    miniBatch[i] = expId;
  }

  // Sorting minibatch -- this helps with locality and also
  // to quickly detect duplicates when updating metadata
  std::sort(miniBatch.begin(), miniBatch.end());

  // Returning generated minibatch
  return miniBatch;
}

void Agent::updateExperienceMetadata(const std::vector<size_t> &miniBatch, const std::vector<policy_t> &policyData)
{
  const size_t miniBatchSize = miniBatch.size();

  // Creating a selection of unique experiences from the mini batch
  // Important: this assumes the minibatch ids are sorted.
  std::vector<size_t> updateBatch;
  updateBatch.push_back(0);
  for (size_t i = 1; i < miniBatchSize; i++)
    if (miniBatch[i] != miniBatch[i - 1]) updateBatch.push_back(i);

  // Storage to keep track of off-policy experiences
  size_t offPolicyCount = 0;

#pragma omp parallel for reduction(+ \
                                   : offPolicyCount)
  for (size_t i = 0; i < updateBatch.size(); i++)
  {
    auto batchId = updateBatch[i];
    auto expId = miniBatch[batchId];

    // Get state, action, mean, Sigma for this experience
    const auto &expAction = _actionVector[expId];
    const auto &expPolicy = _expPolicyVector[expId];
    const auto &curPolicy = policyData[batchId];

    // Grabbing state value from the latenst policy
    auto stateValue = curPolicy.stateValue;

    // Sanity checks for state value
    if (std::isfinite(stateValue) == false)
      KORALI_LOG_ERROR("Calculated state value for state returned an invalid value: %f\n", stateValue);

    // Compute importance weight
    const float importanceWeight = calculateImportanceWeight(expAction, curPolicy, expPolicy);
    const float truncatedImportanceWeight = std::min(1.0f, importanceWeight);

    // Sanity checks for state value
    if (std::isfinite(importanceWeight) == false)
      KORALI_LOG_ERROR("Calculated state value for importanceWeight returned an invalid value: %f\n", importanceWeight);

    // If this is the truncated experience of an episode, then obtain truncated state value
    float truncatedV = 0.0f;
    if (_terminationVector[expId] == e_truncated)
    {
      auto truncatedPolicy = runPolicy({getTruncatedStateSequence(expId)})[0];
      truncatedV = truncatedPolicy.stateValue;
    }

    // Checking if experience is still on policy
    bool isOnPolicy = (importanceWeight > (1.0f / _experienceReplayOffPolicyCurrentCutoff)) && (importanceWeight < _experienceReplayOffPolicyCurrentCutoff);

    // Store computed information for use in replay memory.
    _curPolicyVector[expId] = curPolicy;
    _stateValueVector[expId] = stateValue;
    _truncatedStateValueVector[expId] = truncatedV;
    _importanceWeightVector[expId] = importanceWeight;
    _isOnPolicyVector[expId] = isOnPolicy;
    _truncatedImportanceWeightVector[expId] = truncatedImportanceWeight;

    if (isOnPolicy == false) offPolicyCount++;
  }

  // Updating the off policy Ratio
  float alpha = 0.01f; // ~100 updates to reach 63.2% of the target
  _experienceReplayOffPolicyRatio = alpha * (float)offPolicyCount / (float)updateBatch.size() + (1.0f - alpha) * _experienceReplayOffPolicyRatio;

  // Updating the off policy Cutoff
  _experienceReplayOffPolicyCurrentCutoff = 1.0f + _experienceReplayOffPolicyCutoffScale / (1.0f + _experienceReplayOffPolicyAnnealingRate * (float)_policyUpdateCount);

  // Now filtering experiences from the same episode
  std::vector<size_t> retraceMiniBatch;

  // Adding last experience from the sorted minibatch
  retraceMiniBatch.push_back(miniBatch[miniBatchSize - 1]);

  // Adding experiences so long as they do not repeat episodes
  for (ssize_t i = miniBatchSize - 2; i >= 0; i--)
  {
    size_t currExpId = miniBatch[i];
    size_t nextExpId = miniBatch[i + 1];
    size_t curEpisode = _episodeIdVector[currExpId];
    size_t nextEpisode = _episodeIdVector[nextExpId];
    if (curEpisode != nextEpisode) retraceMiniBatch.push_back(currExpId);
  }

// Calculating retrace value for the oldest experiences of unique episodes
#pragma omp parallel for schedule(guided, 1)
  for (size_t i = 0; i < retraceMiniBatch.size(); i++)
  {
    // Finding the earliest experience corresponding to the same episode as this experience
    ssize_t endId = retraceMiniBatch[i];
    ssize_t startId = endId - _episodePosVector[endId];

    // If the starting experience has already been discarded, take the earliest one that still remains
    if (startId < 0) startId = 0;

    // Storage for the retrace value
    float retV = 0.0f;

    // If it was a truncated episode, add the value function for the terminal state to retV
    if (_terminationVector[endId] == e_truncated)
      retV = _truncatedStateValueVector[endId];

    if (_terminationVector[endId] == e_nonTerminal)
      retV = _retraceValueVector[endId + 1];

    // Now iterating backwards to calculate the rest of vTbc
    for (ssize_t curId = endId; curId >= startId; curId--)
    {
      // If this retrace value (and previous ones) was calculated recently, skipping the entire process
      if (_policyUpdateCount - _retraceLastUpdateVector[curId] < _retraceUpdateDelay) break;

      // Getting current reward, action, and state
      const float curReward = _rewardVector[curId] * _rewardRescalingSdev;

      // Calculating state value function
      const float curV = _stateValueVector[curId];

      // Truncate importance weight
      const float truncatedImportanceWeight = _truncatedImportanceWeightVector[curId];

      // Calculating retrace value
      retV = curV + truncatedImportanceWeight * (curReward + _discountFactor * retV - curV);

      // Storing retrace value into the experience's cache
      _retraceValueVector[curId] = retV;
      _retraceLastUpdateVector[curId] = _policyUpdateCount;
    }
  }
}

size_t Agent::getTimeSequenceStartExpId(size_t expId)
{
  size_t startId = expId;

  // Adding (tmax-1) time sequences to the given experience
  for (size_t t = 0; t < _timeSequenceLength - 1; t++)
  {
    // If we reached the start of the ER, this is the starting episode in the sequence
    if (startId == 0) break;

    // Now going back one experience
    startId--;

    // If we reached the end of the previous episode, then add one (this covers the case where the provided experience is also terminal) and break.
    if (_terminationVector[startId] != e_nonTerminal)
    {
      startId++;
      break;
    }
  }

  return startId;
}

void Agent::resetTimeSequence()
{
  _stateTimeSequence.clear();
}

std::vector<std::vector<std::vector<float>>> Agent::getMiniBatchStateSequence(const std::vector<size_t> &miniBatch, const bool includeAction)
{
  // Getting mini batch size
  const size_t miniBatchSize = miniBatch.size();

  // Allocating state sequence vector
  std::vector<std::vector<std::vector<float>>> stateSequence(miniBatchSize);

  // Calculating size of state vector
  const size_t stateSize = includeAction ? _problem->_stateVectorSize + _problem->_actionVectorSize : _problem->_stateVectorSize;

#pragma omp parallel for
  for (size_t b = 0; b < miniBatch.size(); b++)
  {
    // Getting current expId
    const size_t expId = miniBatch[b];

    // Getting starting expId
    const size_t startId = getTimeSequenceStartExpId(expId);

    // Calculating time sequence length
    const size_t T = expId - startId + 1;

    // Resizing state sequence vector to the correct time sequence length
    stateSequence[b].resize(T);

    // Now adding states (and actions, if required)
    for (size_t t = 0; t < T; t++)
    {
      size_t curId = startId + t;
      stateSequence[b][t].reserve(stateSize);
      stateSequence[b][t].insert(stateSequence[b][t].begin(), _stateVector[curId].begin(), _stateVector[curId].end());
      if (includeAction) stateSequence[b][t].insert(stateSequence[b][t].begin(), _actionVector[curId].begin(), _actionVector[curId].end());
    }
  }

  return stateSequence;
}

std::vector<std::vector<float>> Agent::getTruncatedStateSequence(size_t expId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding states, except for the initial one
  for (size_t e = startId + 1; e <= expId; e++)
    timeSequence.push_back(_stateVector[e]);

  // Lastly, adding truncated state
  timeSequence.push_back(_truncatedStateVector[expId]);

  return timeSequence;
}

void Agent::finalize()
{
  if (_mode != "Training") return;

  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      serializeExperienceReplay();

  _k->_logger->logInfo("Normal", "Waiting for pending agents to finish...\n");

  // Waiting for pending agents to finish
  bool agentsRemain = true;
  do
  {
    agentsRemain = false;
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }

    if (agentsRemain) KORALI_LISTEN(_agents);
  } while (agentsRemain == true);
}

void Agent::serializeExperienceReplay()
{
  _k->_logger->logInfo("Detailed", "Serializing Agent's Training State...\n");
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Serializing agent's database into the JSON storage
  for (size_t i = 0; i < _stateVector.size(); i++)
  {
    stateJson["Experience Replay"][i]["Episode Id"] = _episodeIdVector[i];
    stateJson["Experience Replay"][i]["Episode Pos"] = _episodePosVector[i];
    stateJson["Experience Replay"][i]["State"] = _stateVector[i];
    stateJson["Experience Replay"][i]["Action"] = _actionVector[i];
    stateJson["Experience Replay"][i]["Reward"] = _rewardVector[i];
    stateJson["Experience Replay"][i]["Feature Vector"] = _featureVector[i];
    stateJson["Experience Replay"][i]["State Value"] = _stateValueVector[i];
    stateJson["Experience Replay"][i]["Retrace Value"] = _retraceValueVector[i];
    stateJson["Experience Replay"][i]["Retrace Last Update"] = _retraceLastUpdateVector[i];
    stateJson["Experience Replay"][i]["Importance Weight"] = _importanceWeightVector[i];
    stateJson["Experience Replay"][i]["Truncated Importance Weight"] = _truncatedImportanceWeightVector[i];
    stateJson["Experience Replay"][i]["Is On Policy"] = _isOnPolicyVector[i];
    stateJson["Experience Replay"][i]["Truncated State"] = _truncatedStateVector[i];
    stateJson["Experience Replay"][i]["Truncated State Value"] = _truncatedStateValueVector[i];
    stateJson["Experience Replay"][i]["Termination"] = _terminationVector[i];

    stateJson["Experience Replay"][i]["Experience Policy"]["State Value"] = _expPolicyVector[i].stateValue;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Means"] = _expPolicyVector[i].actionMeans;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Sigmas"] = _expPolicyVector[i].actionSigmas;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Index"] = _expPolicyVector[i].actionIndex;
    stateJson["Experience Replay"][i]["Experience Policy"]["Action Probabilities"] = _expPolicyVector[i].actionProbabilities;

    stateJson["Experience Replay"][i]["Current Policy"]["State Value"] = _curPolicyVector[i].stateValue;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Means"] = _curPolicyVector[i].actionMeans;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Sigmas"] = _curPolicyVector[i].actionSigmas;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Index"] = _curPolicyVector[i].actionIndex;
    stateJson["Experience Replay"][i]["Current Policy"]["Action Probabilities"] = _curPolicyVector[i].actionProbabilities;
  }

  // Storing training/testing policies
  stateJson["Training"]["Current Policy"] = _trainingCurrentPolicy;
  stateJson["Training"]["Best Policy"] = _trainingBestPolicy;
  stateJson["Testing"]["Best Policy"] = _testingBestPolicy;

  // If results directory doesn't exist, create it
  if (!dirExists(_k->_fileOutputPath)) mkdir(_k->_fileOutputPath);

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Storing database to file
  if (saveJsonToFile(statePath.c_str(), stateJson) != 0)
    KORALI_LOG_ERROR("Could not serialize training state into file %s\n", statePath.c_str());

  auto endTime = std::chrono::steady_clock::now();                                                                   // Profiling
  _sessionSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void Agent::deserializeExperienceReplay()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Loading database from file
  _k->_logger->logInfo("Detailed", "Loading previous run training state from file %s...\n", statePath.c_str());
  if (loadJsonFromFile(stateJson, statePath.c_str()) == false)
    KORALI_LOG_ERROR("Trying to resume training or test policy but could not find or deserialize agent's state from file %s...\n", statePath.c_str());

  // Clearing existing database
  _stateVector.clear();
  _actionVector.clear();
  _retraceValueVector.clear();
  _rewardVector.clear();
  _stateValueVector.clear();
  _importanceWeightVector.clear();
  _truncatedImportanceWeightVector.clear();
  _truncatedStateValueVector.clear();
  _truncatedStateVector.clear();
  _terminationVector.clear();
  _expPolicyVector.clear();
  _curPolicyVector.clear();
  _isOnPolicyVector.clear();
  _priorityVector.clear();
  _probabilityVector.clear();
  _episodePosVector.clear();
  _episodeIdVector.clear();

  // Deserializing database from JSON to the agent's state
  for (size_t i = 0; i < stateJson["Experience Replay"].size(); i++)
  {
    _episodeIdVector.add(stateJson["Experience Replay"][i]["Episode Id"].get<size_t>());
    _episodePosVector.add(stateJson["Experience Replay"][i]["Episode Pos"].get<size_t>());
    _stateVector.add(stateJson["Experience Replay"][i]["State"].get<std::vector<float>>());
    _actionVector.add(stateJson["Experience Replay"][i]["Action"].get<std::vector<float>>());
    _rewardVector.add(stateJson["Experience Replay"][i]["Reward"].get<float>());
    _stateValueVector.add(stateJson["Experience Replay"][i]["State Value"].get<float>());
    _retraceValueVector.add(stateJson["Experience Replay"][i]["Retrace Value"].get<float>());
    _retraceLastUpdateVector.add(stateJson["Experience Replay"][i]["Retrace Last Update"].get<float>());
    _importanceWeightVector.add(stateJson["Experience Replay"][i]["Importance Weight"].get<float>());
    _truncatedImportanceWeightVector.add(stateJson["Experience Replay"][i]["Truncated Importance Weight"].get<float>());
    _isOnPolicyVector.add(stateJson["Experience Replay"][i]["Is On Policy"].get<bool>());
    _truncatedStateVector.add(stateJson["Experience Replay"][i]["Truncated State"].get<std::vector<float>>());
    _truncatedStateValueVector.add(stateJson["Experience Replay"][i]["Truncated State Value"].get<float>());
    _terminationVector.add(stateJson["Experience Replay"][i]["Termination"].get<termination_t>());

    policy_t expPolicy;
    expPolicy.stateValue = stateJson["Experience Replay"][i]["Experience Policy"]["State Value"].get<float>();
    expPolicy.actionMeans = stateJson["Experience Replay"][i]["Experience Policy"]["Action Means"].get<std::vector<float>>();
    expPolicy.actionSigmas = stateJson["Experience Replay"][i]["Experience Policy"]["Action Sigmas"].get<std::vector<float>>();
    expPolicy.actionIndex = stateJson["Experience Replay"][i]["Experience Policy"]["Action Index"].get<size_t>();
    expPolicy.actionProbabilities = stateJson["Experience Replay"][i]["Experience Policy"]["Action Probabilities"].get<std::vector<float>>();
    _expPolicyVector.add(expPolicy);

    policy_t curPolicy;
    curPolicy.stateValue = stateJson["Experience Replay"][i]["Current Policy"]["State Value"].get<float>();
    curPolicy.actionMeans = stateJson["Experience Replay"][i]["Current Policy"]["Action Means"].get<std::vector<float>>();
    curPolicy.actionSigmas = stateJson["Experience Replay"][i]["Current Policy"]["Action Sigmas"].get<std::vector<float>>();
    curPolicy.actionIndex = stateJson["Experience Replay"][i]["Current Policy"]["Action Index"].get<size_t>();
    curPolicy.actionProbabilities = stateJson["Experience Replay"][i]["Current Policy"]["Action Probabilities"].get<std::vector<float>>();
    _curPolicyVector.add(curPolicy);
  }

  // Restoring training/testing policies
  _trainingCurrentPolicy = stateJson["Training"]["Current Policy"];
  _trainingBestPolicy = stateJson["Training"]["Best Policy"];
  _testingBestPolicy = stateJson["Testing"]["Best Policy"];

  // Setting current agent's training state
  setAgentPolicy(_trainingCurrentPolicy);

  auto endTime = std::chrono::steady_clock::now();                                                                         // Profiling
  double deserializationTime = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count() / 1.0e+9; // Profiling
  _k->_logger->logInfo("Detailed", "Took %fs to deserialize training state.\n", deserializationTime);
}

void Agent::printGenerationAfter()
{
  if (_mode == "Training")
  {
    _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

    _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _stateVector.size(), _experienceReplayMaximumSize);

    if (_maxEpisodes > 0)
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
    else
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

    if (_maxExperiences > 0)
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
    else
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

    _k->_logger->logInfo("Normal", "Off-Policy Statistics:\n");
    _k->_logger->logInfo("Normal", " + Off-Policy Ratio:            %f (Target: < %f)\n", _experienceReplayOffPolicyRatio, _experienceReplayOffPolicyTarget);
    _k->_logger->logInfo("Normal", " + Importance Weight Cutoff:    [%.3f, %.3f]\n", 1.0f / _experienceReplayOffPolicyCurrentCutoff, _experienceReplayOffPolicyCurrentCutoff);
    _k->_logger->logInfo("Normal", " + REFER Beta Factor:           %f\n", _experienceReplayOffPolicyREFERBeta);

    _k->_logger->logInfo("Normal", "Training Statistics:\n");

    if (_maxPolicyUpdates > 0)
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
    else
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

    _k->_logger->logInfo("Normal", " + Latest Reward:               %f/%f\n", _trainingLastReward, _problem->_trainingRewardThreshold);
    _k->_logger->logInfo("Normal", " + %lu-Episode Average Reward:  %f\n", _trainingAverageDepth, _trainingAverageReward);
    _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _trainingBestReward, _trainingBestEpisodeId);

    if (isinf(_problem->_trainingRewardThreshold) == false)
    {
      _k->_logger->logInfo("Normal", "Testing Statistics:\n");

      _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _testingCandidateCount);

      _k->_logger->logInfo("Normal", " + Latest Average (Stdev / Worst / Best) Reward: %f (%f / %f / %f)\n", _testingAverageReward, _testingStdevReward, _testingWorstReward, _testingBestReward);
    }

    if (_testingTargetAverageReward > -korali::Inf)
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f/%f (%lu)\n", _testingBestAverageReward, _testingTargetAverageReward, _testingBestEpisodeId);
    else
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f (%lu)\n", _testingBestAverageReward, _testingBestEpisodeId);

    printAgentInformation();
    _k->_logger->logInfo("Normal", " + Current Learning Rate:           %.3e\n", _currentLearningRate);

    _k->_logger->logInfo("Detailed", "Profiling Information:                     [Generation] - [Session]\n");
    _k->_logger->logInfo("Detailed", " + Experience Serialization Time:          [%5.3fs] - [%3.3fs]\n", _generationSerializationTime / 1.0e+9, _sessionSerializationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Agent Attending Time:                   [%5.3fs] - [%3.3fs]\n", _generationAgentAttendingTime / 1.0e+9, _sessionAgentAttendingTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Computation Time:             [%5.3fs] - [%3.3fs]\n", _generationAgentComputationTime / 1.0e+9, _sessionAgentComputationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Communication/Wait Time:      [%5.3fs] - [%3.3fs]\n", _generationAgentCommunicationTime / 1.0e+9, _sessionAgentCommunicationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Policy Evaluation Time:       [%5.3fs] - [%3.3fs]\n", _generationAgentPolicyEvaluationTime / 1.0e+9, _sessionAgentPolicyEvaluationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Policy Update Time:                     [%5.3fs] - [%3.3fs]\n", _generationPolicyUpdateTime / 1.0e+9, _sessionPolicyUpdateTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Running Time:                           [%5.3fs] - [%3.3fs]\n", _generationRunningTime / 1.0e+9, _sessionRunningTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Trajectory Log Probability Update Time: [%5.3fs] \n", _sessionAgentTrajectoryLogProbilityUpdateTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + [I/O] Result File Saving Time:        %5.3fs\n", _k->_resultSavingTime / 1.0e+9);
  }

  _k->_logger->logInfo("Normal", "Number Background Samples %zu\n", _backgroundSampleSize);
  _k->_logger->logInfo("Normal", "Log Partition Function %f (%f)\n", _logPartitionFunction, _logSdevPartitionFunction);
  for (size_t k = 0; k < _problem->_featureVectorSize; ++k)
  {
    _k->_logger->logInfo("Normal", "fw [%zu] %f \n", k, _featureWeights[k]);
    _k->_logger->logInfo("Normal", "fw softmax [%zu] %f \n", k, _softMaxFeatureWeights[k]);
    _k->_logger->logInfo("Normal", "fw gradient [%zu] %f\n", k, _featureWeightGradient[k]);
  }

  if (_mode == "Testing")
  {
    _k->_logger->logInfo("Normal", "Testing Results:\n");
    for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
    {
      _k->_logger->logInfo("Normal", " + Sample %lu:\n", _testingSampleIds[agentId]);
      _k->_logger->logInfo("Normal", "   + Cumulative Reward               %f\n", _testingReward[agentId]);
    }
  }
}

} // namespace solver

} // namespace korali

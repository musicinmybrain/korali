#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _currentExperienceCount = 0;
    _currentTrajectoryCount = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting initial hyperparameters for all workers to use
  KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

  // Configuring current agent
  Sample agent;

  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";

  // Initializing statistical information
  _currentReward = 0.0;

  // Launching agent
  KORALI_START(agent);

  // Storage to keep experience information
  std::vector<knlohmann::json> trajectory;
  knlohmann::json experience;

  // Keep listening to incoming experiences until we reached a terminal state
  do
  {
    // Listening for incoming experiences
    experience = KORALI_RECV_MSG_FROM_SAMPLE(agent);

    // Setting trajectory position on the experience and whether it is the latest in the trajectory
    experience["Trajectory Position"] = trajectory.size();
    experience["Trajectory End"] = false;

    // Adding experience to trajectory
    trajectory.push_back(experience);

    // If reached desired trajectory size or last experience of the episode, run an optimization step
    if (trajectory.size() == _trajectorySize || experience["Is Terminal"] == true)
    {
     // Now this experience is the last in the trajectory
     trajectory[trajectory.size()-1]["Trajectory End"] = true;

     // Add trajectory to experience
     _experienceReplayHistory.insert(_experienceReplayHistory.end(), trajectory.begin(), trajectory.end());
     _currentTrajectoryCount++;

     // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
     if (_experienceReplayHistory.size() > _experienceReplayStartSize)
     for (size_t i = 0; i < _optimizationStepsPerTrajectory; i++)
       trainAgent();

      // Update the number of optimization steps, for workers to know it has been updated
      _optimizationStepCount += _optimizationStepsPerTrajectory;

      // Now cleaning trajectory, to create a new one.
      trajectory.clear();
    }

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true)
      KORALI_SEND_MSG_TO_SAMPLE(agent, _hyperparameters);

    // Adding experience's reward to episode's reward
    _currentReward += experience["Reward"].get<double>();

    // Increasing experience counts
    _currentExperienceCount++;

  } while (experience["Is Terminal"] == false);

  // Add final trajectory to experience
  _experienceReplayHistory.insert(_experienceReplayHistory.end(), trajectory.begin(), trajectory.end());
  _currentTrajectoryCount++;

  // Waiting for agent to finish
  KORALI_WAIT(agent);

  // Adding to the number of episodes
  _currentEpisode++;

  /*********************************************************************
   * Updating experience replay memory
   *********************************************************************/

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_experienceReplayHistory.size() > _experienceReplayMaximumSize)
  {
    size_t excess = _experienceReplayHistory.size() - _experienceReplayMaximumSize;
    _experienceReplayHistory.erase(_experienceReplayHistory.begin(), _experienceReplayHistory.begin() + excess);
  }

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward) _bestTrainingReward = _currentReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward > _trainingRewardThreshold)
  {
    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> tests(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      tests[i]["Sample Id"] = _currentSampleID++;
      tests[i]["Module"] = "Problem";
      tests[i]["Operation"] = "Run Episode";
      tests[i]["Mode"] = "Testing";

      // Launching agent initialization
      KORALI_START(tests[i]);
    }

    KORALI_WAITALL(tests);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += tests[i]["Cumulative Reward"].get<double>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplayHistory.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _currentExperienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:    %lu\n", _currentExperienceCount);

  _k->_logger->logInfo("Normal", " + Total Trajectory Count:    %lu\n", _currentTrajectoryCount);


  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f\n", _bestTrainingReward);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

  printAgentInformation();
}

} // namespace solver

} // namespace korali

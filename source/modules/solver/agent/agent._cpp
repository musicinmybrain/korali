#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _experienceReplay.resize(_experienceReplayMaximumSize);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;
    _experienceCount = 0;
    _experienceReplayMaxPriority = 0.0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTestingReward = -korali::Inf;
    _worstTestingReward = +korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestTrainingEpisode = 0;
    _bestAverageTestingReward = -korali::Inf;

    // Initialize best hyperparameters
    _bestHyperparameters = _hyperparameters;

    // Initializing training hyperparameters
    _trainingState = getTrainingState();
  }

  // On restart, the number of experineces is equal to that of the experience replay
  _experienceCount = _experienceReplay.size();

  // Setting initial or loaded hyperparameters from a previous run (or save file)
  setTrainingState(_trainingState);

  // If this continues a previous run, deserialize previous input experience replay
  deserializeExperienceReplay();

  // Initializing profiling timers
  _serializationTime = 0.0;

  // Creating storate for _agents and their status
  _agents.resize(_agentCount);
  _isAgentRunning.resize(_agentCount, false);

  // Storage for the entire episodes before adding it to the replay memory
  _episodes.resize(_agentCount);
}

void Agent::runGeneration()
{
  auto beginTime = std::chrono::steady_clock::now();

  // Running until all _agents have finished
  while (_experienceCount < _experiencesPerGeneration * _k->_currentGeneration)
  {
    // Launching (or re-launching) agents
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == false)
      {
        _agents[agentId]["Sample Id"] = _currentSampleID++;
        _agents[agentId]["Module"] = "Problem";
        _agents[agentId]["Operation"] = "Run Episode";
        _agents[agentId]["Hyperparameters"] = _hyperparameters;

        KORALI_START(_agents[agentId]);
        _isAgentRunning[agentId] = true;
      }

    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    // Attending to running agents, checking if any experience has been received
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
        attendAgent(agentId);

    // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
    if (_experienceReplay.size() >= _experienceReplayStartSize)
    {
      // Defining new experience count threshold
      size_t curThreshold = _experiencesBetweenPolicyUpdates * _policyUpdateCount + _experienceReplayStartSize;

      // If we accumulated enough experiences between updates, update now
      if (_experienceCount > curThreshold)
      {
        // Calling the algorithm specific policy training algorithm
        trainPolicy();

        // Gathering their training hyperparameters (for checkpoint/resume purposes)
        _trainingState = getTrainingState();

        // Increasing policy update counter
        _policyUpdateCount++;
      }
    }
  }

  // Measuring generation time
  auto endTime = std::chrono::steady_clock::now();
  _generationTime = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();

  // Updating experience probabilities
  if (_miniBatchStrategy == "Prioritized") updateExperienceReplayProbabilities();

  // Now serializing experience replay database
  if (_experienceReplaySerializationFrequency > 0)
    if (_k->_currentGeneration % _experienceReplaySerializationFrequency == 0)
    {
      _k->_logger->logInfo("Normal", "Serializing Experience Replay database...\n");
      auto beginTime = std::chrono::steady_clock::now();
      serializeExperienceReplay();
      auto endTime = std::chrono::steady_clock::now();
      _serializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();
    }
}

void Agent::attendAgent(size_t agentId)
{
  // Storage for the incoming experience
  knlohmann::json experience;

  // Retrieving the experience, if any has arrived for the current agent.
  if (_agents[agentId].retrievePendingMessage(experience))
  {
    // Storage to determine terminal state
    bool isTerminal = false;

    // Storing experience in the episode
    _episodes[agentId].push_back(experience);

    // Increasing total experience counter
    _experienceCount++;

    // Check if experience is terminal
    isTerminal = experience["Termination"] != "Non Terminal";

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true)
      KORALI_SEND_MSG_TO_SAMPLE(_agents[agentId], _hyperparameters);

    if (isTerminal)
    {
      // Now that we have the entire episode, process its experiences (add them to replay memory)
      for (size_t i = 0; i < _episodes[agentId].size(); i++) processExperience(_episodes[agentId][i]);

      // Now clear the episode of all its current experiences
      _episodes[agentId].clear();

      // Waiting for the agent to come back with all the information
      KORALI_WAIT(_agents[agentId]);

      // Getting the training reward of the latest episode
      _lastTrainingReward = _agents[agentId]["Training Reward"].get<float>();

      // Keeping training statistics. Updating if exceeded best training policy so far.
      if (_lastTrainingReward > _bestTrainingReward)
      {
        _bestTrainingReward = _lastTrainingReward;
        _bestTrainingEpisode = _currentEpisode;
      }

      // If the policy has exceeded the threshold during training, we gather its statistics
      if (_agents[agentId]["Tested Policy"] == true)
      {
        _candidatePoliciesTested++;

        _averageTestingReward = _agents[agentId]["Average Testing Reward"].get<float>();
        _bestTestingReward = _agents[agentId]["Best Testing Reward"].get<float>();
        _worstTestingReward = _agents[agentId]["Worst Testing Reward"].get<float>();

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_averageTestingReward > _bestAverageTestingReward)
        {
          _bestAverageTestingReward = _averageTestingReward;
          _bestHyperparameters = _agents[agentId]["Hyperparameters"];
        }
      }

      // Set agent as finished
      _isAgentRunning[agentId] = false;

      // Increasing number of episodes
      _currentEpisode++;
    }
  }
}

void Agent::processExperience(knlohmann::json &experience)
{
  experience_t e;

  e.cache.setMaxAge(_cachePersistence);
  e.cache.setTimer(&_policyUpdateCount);

  // Getting state, action, reward, and whether it's terminal
  e.state = experience["State"].get<std::vector<float>>();
  e.action = experience["Action"].get<std::vector<float>>();
  e.reward = experience["Reward"].get<float>();

  if (experience["Termination"] == "Non Terminal") e.termination = e_nonTerminal;
  if (experience["Termination"] == "Terminal") e.termination = e_terminal;
  if (experience["Termination"] == "Truncated")
  {
    e.termination = e_truncated;
    e.truncatedState = experience["Truncated State"].get<std::vector<float>>();
  }

  e.priority = _experienceReplayMaxPriority;
  e.probability = 0.0;
  e.policy = experience["Policy"];
  _experienceReplay.add(e);
}

void Agent::updateExperienceReplayProbabilities()
{
  // Rank priorities
  auto experienceRanking = std::vector<std::pair<float, size_t>>(_experienceReplay.size());
  for (size_t i = 0; i < _experienceReplay.size(); ++i) experienceRanking[i] = std::make_pair(_experienceReplay[i].priority, i + 1);
  sort(experienceRanking.begin(), experienceRanking.end());

  // Calculate rank based probabilities
  float prioritySum = 0.0;
  for (size_t i = 1; i < _experienceReplay.size() + 1; ++i)
    prioritySum += std::pow((float)i, _priorityAnnealingRate);

  // Update probabilities
  for (size_t i = 0; i < _experienceReplay.size(); ++i)
    _experienceReplay[i].probability = std::pow((float)experienceRanking[i].second, _priorityAnnealingRate) / (prioritySum);
}

void Agent::updateExperienceReplayPriority(size_t expId, float priority)
{
  // Insert priority calculated in learner
  if (priority > _experienceReplayMaxPriority) _experienceReplayMaxPriority = priority;
  _experienceReplay[expId].priority = priority;
}

std::vector<float> Agent::calculateImportanceWeights(std::vector<size_t> miniBatch)
{
  auto importanceWeights = std::vector<float>(miniBatch.size(), 1.0);

  if (_miniBatchStrategy == "Prioritized")
  {
    for (size_t i = 0; i < miniBatch.size(); ++i)
      importanceWeights[i] = std::pow((float)_experienceReplay.size() * _experienceReplay[miniBatch[i]].probability, -_importanceWeightAnnealingRate);
  }

  return importanceWeights;
}

std::vector<size_t> Agent::generateMiniBatch(size_t size)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(size);

  if (_miniBatchStrategy == "Uniform")
  {
    for (size_t i = 0; i < size; i++)
    {
      // Producing random (uniform) number for the selection of the experience
      float x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      size_t expId = std::floor(x * (float)(_experienceReplay.size() - 1));

      // Setting experience
      miniBatch[i] = expId;
    }
  }

  if (_miniBatchStrategy == "Prioritized")
  {
    // Allocating storage for uniform random numbers
    auto pvalues = std::vector<float>(size);

    for (size_t i = 0; i < size; i++)
      pvalues[i] = _uniformGenerator->getRandomNumber();

    // Sort selections ascending for traversal
    std::sort(pvalues.begin(), pvalues.end());

    size_t expId = 0;
    size_t batchIdx = 0;
    float cumulativeProbability = 0;

    // Sampling from multinomial distribution using inverse transform
    cumulativeProbability += _experienceReplay[0].probability;
    while (batchIdx < size && expId < _experienceReplay.size())
    {
      if (cumulativeProbability > pvalues[batchIdx])
        miniBatch[batchIdx++] = expId;
      else
        cumulativeProbability += _experienceReplay[++expId].probability;
    }

    // Uniform fill if miniBatch not full due to numerical precision during probability accumulation (cumulative probability < 1)
    while (batchIdx < size)
      miniBatch[batchIdx++] = std::floor(_uniformGenerator->getRandomNumber() * (float)(_experienceReplay.size() - 1));
  }

  return miniBatch;
}

void Agent::finalize()
{
  if (_experienceReplaySerializationFrequency > 0)
  {
    _k->_logger->logInfo("Normal", "Serializing Experience Replay database...\n");
    serializeExperienceReplay();
  }

  _k->_logger->logInfo("Normal", "Waiting for pending agents to finish...\n");

  // Waiting for pending agents to finish
  bool agentsRemain = true;
  while (agentsRemain == true)
  {
    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    agentsRemain = false;
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }
  }

  // Updating policy with the best parameters produced during training
  updateAgentPolicy(_bestHyperparameters);
}

void Agent::serializeExperienceReplay()
{
  _experienceReplayDatabase.resize(_experienceReplay.size());

  for (size_t i = 0; i < _experienceReplay.size(); i++)
  {
    _experienceReplayDatabase[i]["State"] = _experienceReplay[i].state;
    _experienceReplayDatabase[i]["Action"] = _experienceReplay[i].action;
    _experienceReplayDatabase[i]["Reward"] = _experienceReplay[i].reward;
    _experienceReplayDatabase[i]["Priority"] = _experienceReplay[i].priority;
    _experienceReplayDatabase[i]["Probability"] = _experienceReplay[i].probability;
    _experienceReplayDatabase[i]["Termination"] = _experienceReplay[i].termination;
    _experienceReplayDatabase[i]["Truncated State"] = _experienceReplay[i].truncatedState;
    _experienceReplayDatabase[i]["Policy"] = _experienceReplay[i].policy;
    _experienceReplayDatabase[i]["Metadata"] = _experienceReplay[i].metadata;
  }
}

void Agent::deserializeExperienceReplay()
{
  for (size_t i = 0; i < _experienceReplayDatabase.size(); i++)
  {
    experience_t e;
    e.state = _experienceReplayDatabase[i]["State"].get<std::vector<float>>();
    e.action = _experienceReplayDatabase[i]["Action"].get<std::vector<float>>();
    e.reward = _experienceReplayDatabase[i]["Reward"].get<float>();
    e.priority = _experienceReplayDatabase[i]["Priority"].get<float>();
    e.probability = _experienceReplayDatabase[i]["Probability"].get<float>();
    e.termination = _experienceReplayDatabase[i]["Termination"].get<termination_t>();
    e.truncatedState = _experienceReplayDatabase[i]["Truncated State"].get<std::vector<float>>();
    e.policy = _experienceReplayDatabase[i]["Policy"];
    e.metadata = _experienceReplayDatabase[i]["Metadata"];
    _experienceReplay.add(e);
  }
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplay.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxPolicyUpdates > 0)
    _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
  else
    _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

  _k->_logger->logInfo("Normal", " + Latest Reward:              %f/%f\n", _lastTrainingReward, _problem->_trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _bestTrainingReward, _bestTrainingEpisode);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average (Worst / Best) Reward: %f (%f / %f)\n", _averageTestingReward, _worstTestingReward, _bestTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward: %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward: %f\n", _bestAverageTestingReward);

  printAgentInformation();

  _k->_logger->logInfo("Normal", "Profiling Information:\n");
  _k->_logger->logInfo("Normal", " + Generation Running Time:   %fs\n", _generationTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + Cumulative Experience Replay Serialization Time:   %fs\n", _serializationTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + Cumulative Result File Saving I/O Time:            %fs\n", _k->_resultSavingTime / 1.0e+9f);
}

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

//printf("Input Write Time: %fs\n", _inputWriteTime/1000000000);
//printf("Output Read Time: %fs\n", _outputReadTime/1000000000);
//printf("Normalization Time: %fs\n", _outputReadTime/1000000000);
//printf("Forward Propagation Time: %fs\n", _forwardStreamTime/1000000000);
//printf("Backward Propagation Time: %fs\n", _backwardStreamTime/1000000000);

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // Pre-allocating space for the experience replay memory
  _experienceReplayStates.resize(_experienceReplayMaximumSize);
  _experienceReplayActions.resize(_experienceReplayMaximumSize);
  _experienceReplayRewards.resize(_experienceReplayMaximumSize);
  _experienceReplayTerminal.resize(_experienceReplayMaximumSize);
  _experienceReplayPriorities.resize(_experienceReplayMaximumSize);
  _experienceReplayProbabilities.resize(_experienceReplayMaximumSize);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _randomActionProbabilityCurrentValue = _randomActionProbabilityInitialValue;

    _currentEpisode = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;
    _cumulativeQStar = 0;
    _experienceCount = 0;
    _experienceReplayMaxPriority = 0.0;
    _randomActionProbabilityEffectiveValue = 1.0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing critic hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting initial hyperparameters for all workers to use
  if (_k->_currentGeneration == 1) KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

  // Configuring current agent
  Sample agent;
  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";
  agent["Random Action Probability"] = _randomActionProbabilityEffectiveValue;

  // Initializing statistical information
  _currentReward = 0.0;

  // Storage to keep experience information
  knlohmann::json experience;

  // Storage to determine terminal state
  bool isTerminal = false;

  // Launching agent
  KORALI_START(agent);

  // Creating storage for the entire episode before adding it to the replay memory
  std::vector<knlohmann::json> episode;

  // Keep listening to incoming experiences until we reached a terminal state
  while (isTerminal == false)
  {
    // Listening for incoming experiences
    experience = KORALI_RECV_MSG_FROM_SAMPLE(agent);

    // Storing experience in the episode
    episode.push_back(experience);

    // Increasing total experience counter
    _experienceCount++;

    // Check if experience is terminal
    isTerminal = experience["Is Terminal"];

    // Running Agent training if the following conditions are met
    if (_experienceCount % _experiencesBetweenAgentTrainings == 0)       // If we accumulated enough experiences between updates, update now
      if (isTerminal == false)                                           // Do not update policy if this is terminal
        if (_experienceReplayStates.size() > _experienceReplayStartSize) // Perform optimization steps on the critic/policy, if reached the minimum replay memory size
        {
          for (size_t i = 0; i < _optimizationStepsPerUpdate; i++) trainAgent();
          _optimizationStepCount += _optimizationStepsPerUpdate;
        }

    // Delay using latest critic hyperparameters for training until reaching a number of experiences
    if (_experienceCount % _experiencesBetweenTargetNetworkUpdates == 0)
      updateHyperparameters(_hyperparameters);

    // If agent requested new policy, send the new hyperparameters
    if (experience["Request New Policy"] == true) KORALI_SEND_MSG_TO_SAMPLE(agent, _hyperparameters);
  }

  // Now that we have the entire episode, process its experiences (add them to replay memory)
  for (size_t i = 0; i < episode.size(); i++) processExperience(episode[i]);

  // Waiting for agent to finish
  KORALI_WAIT(agent);

  // Adding to the number of episodes
  _currentEpisode++;

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward) _bestTrainingReward = _currentReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward >= _trainingRewardThreshold)
  {
    // Broadcasting hyperparameters testing
    KORALI_UPDATE_GLOBALS("Hyperparameters", _hyperparameters);

    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> agents(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      agents[i]["Sample Id"] = _currentSampleID++;
      agents[i]["Module"] = "Problem";
      agents[i]["Action Source"] = "Policy";
      agents[i]["Operation"] = "Run Episode";
      agents[i]["Mode"] = "Testing";
      agents[i]["Random Action Probability"] = 0.0;

      // Launching agent initialization
      KORALI_START(agents[i]);
    }

    KORALI_WAITALL(agents);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += agents[i]["Cumulative Reward"].get<float>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  /////////////// Updating experience probabilities

  updateExperienceReplayProbabilities();

  /////////////// Updating the probability of a random action (epsilon)

  updateRandomActionProbability();
}

void Agent::processExperience(knlohmann::json &experience)
{
  // Getting state, action, reward, and whether it's terminal
  std::vector<float> state = experience["State"];
  std::vector<float> action = experience["Action"];
  float reward = experience["Reward"];
  bool isTerminal = experience["Is Terminal"];

  // Adding common information to experience replay
  _experienceReplayStates.add(state);
  _experienceReplayActions.add(action);
  _experienceReplayRewards.add(reward);
  _experienceReplayTerminal.add(isTerminal);
  _experienceReplayPriorities.add(_experienceReplayMaxPriority);
  _experienceReplayProbabilities.add(0.0);

  // Adding experience's reward to episode's reward
  _currentReward += reward;
}

void Agent::updateExperienceReplayProbabilities()
{
  // Rank priorities
  auto experienceRanking = std::vector<std::pair<float, size_t>>(_experienceReplayStates.size());
  for (size_t i = 0; i < _experienceReplayStates.size(); ++i) experienceRanking[i] = std::make_pair(_experienceReplayPriorities[i], i + 1);
  sort(experienceRanking.begin(), experienceRanking.end());

  // Calculate rank based probabilities
  float prioritySum = 0.0;
  for (size_t i = 1; i < _experienceReplayStates.size()+1; ++i)
    prioritySum += std::pow((float)i, _priorityAnnealingRate);
  
  // Update probabilities
  for (size_t i = 0; i < _experienceReplayStates.size(); ++i)
    _experienceReplayProbabilities[i] = std::pow((float)experienceRanking[i].second, _priorityAnnealingRate) / (prioritySum);
}

void Agent::updateExperienceReplayPriority(size_t expId, float priority)
{
  // Insert priority calculated in learner
  if (priority > _experienceReplayMaxPriority) _experienceReplayMaxPriority = priority;
  _experienceReplayPriorities[expId] = priority;
}

std::vector<float> Agent::calculateImportanceWeights(std::vector<size_t> miniBatch)
{
  auto importanceWeights = std::vector<float>(miniBatch.size(), 1.0);

  if (_miniBatchStrategy == "Prioritized")
  {
    for (size_t i = 0; i < miniBatch.size(); ++i)
      importanceWeights[i] = std::pow((float)_experienceReplayStates.size() * _experienceReplayProbabilities[miniBatch[i]], -_importanceWeightAnnealingRate);
  }

  return importanceWeights;
}

std::vector<size_t> Agent::generateMiniBatch(size_t size)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(size);

  if (_miniBatchStrategy == "Uniform")
  {
    for (size_t i = 0; i < size; i++)
    {
      // Producing random (uniform) number for the selection of the experience
      float x = _uniformGenerator->getRandomNumber();

      // Selecting experience
      size_t expId = std::floor(x * (float)(_experienceReplayStates.size() - 1));

      // Setting experience
      miniBatch[i] = expId;
    }
  }
  else if (_miniBatchStrategy == "Prioritized")
  {
    // Allocating storage for uniform random numbers
    auto pvalues = std::vector<float>(size);

    for (size_t i = 0; i < size; i++)
      pvalues[i] = _uniformGenerator->getRandomNumber();

    // Sort selections ascending for traversal
    std::sort(pvalues.begin(), pvalues.end());

    size_t expId = 0;
    size_t batchIdx = 0;
    float cumulativeProbability = 0;
    
    // Sampling from multinomial distribution using inverse transform
    cumulativeProbability += _experienceReplayProbabilities[0];
    while(batchIdx < size && expId <_experienceReplayStates.size())
    {
      if(cumulativeProbability > pvalues[batchIdx]) 
        miniBatch[batchIdx++] = expId;
      else
        cumulativeProbability += _experienceReplayProbabilities[++expId];
    }
  
    // Uniform fill if miniBatch not full due to numerical precision during probability accumulation (cumulative probability < 1)
    while(batchIdx < size) miniBatch[batchIdx++] = std::floor(_uniformGenerator->getRandomNumber() * (float)(_experienceReplayStates.size() - 1));
  }
  else
  {
    KORALI_LOG_ERROR("Mini Batch Strategy not recognized.");
  }

  return miniBatch;
}

void Agent::updateRandomActionProbability()
{
  // If we reached the starting size
  if (_experienceReplayStates.size() > _experienceReplayStartSize)
  {
    // Decreasing the value of epsilon
    _randomActionProbabilityCurrentValue = _randomActionProbabilityCurrentValue - _randomActionProbabilityDecreaseRate;

    // Without exceeding the lower bound
    if (_randomActionProbabilityCurrentValue < _randomActionProbabilityTargetValue) _randomActionProbabilityCurrentValue = _randomActionProbabilityTargetValue;
  }

  // Calculating effective value for random action probability
  _randomActionProbabilityEffectiveValue = _experienceReplayStates.size() > _experienceReplayStartSize ? _randomActionProbabilityCurrentValue : 1.0;

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;
}

void Agent::normalizeStateActionNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
  // Allocating memory for the mini batch set
  std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

  for (size_t i = 0; i < normalizationSteps; i++)
    miniBatches[i].resize(miniBatchSize);

  for (size_t i = 0; i < normalizationSteps; i++)
    for (size_t j = 0; j < miniBatchSize; j++)
      miniBatches[i][j].resize(_problem->_stateVectorSize + _problem->_actionVectorSize);

  // Creating minibatch to normalize from
  auto miniBatchIndexes = generateMiniBatch(miniBatchSize);

  // Filling the minibatches
  for (size_t step = 0; step < normalizationSteps; step++)
    for (size_t i = 0; i < miniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = miniBatchIndexes[i];

      for (size_t j = 0; j < _problem->_stateVectorSize; j++) miniBatches[step][i][j] = _experienceReplayStates[expId][j];
      for (size_t j = 0; j < _problem->_actionVectorSize; j++) miniBatches[step][i][_problem->_stateVectorSize + j] = _experienceReplayActions[expId][j];
    }

  neuralNetwork->normalize(miniBatches);
}

void Agent::normalizeStateNeuralNetwork(NeuralNetwork *neuralNetwork, size_t miniBatchSize, size_t normalizationSteps)
{
  // Allocating memory for the mini batch set
  std::vector<std::vector<std::vector<float>>> miniBatches(normalizationSteps);

  for (size_t i = 0; i < normalizationSteps; i++)
    miniBatches[i].resize(miniBatchSize);

  for (size_t i = 0; i < normalizationSteps; i++)
    for (size_t j = 0; j < miniBatchSize; j++)
      miniBatches[i][j].resize(_problem->_stateVectorSize);

  // Creating storage for state history indexes to choose from
  std::vector<size_t> experienceReplayIndexes(_experienceReplayStates.size());
  for (size_t i = 0; i < _experienceReplayStates.size() - 1; i++) experienceReplayIndexes[i] = i;

  // Filling the minibatches
  for (size_t step = 0; step < normalizationSteps; step++)
  {
    // Shuffling indexes to choose the mini batch from
    std::shuffle(experienceReplayIndexes.begin(), experienceReplayIndexes.end(), *mt);

    for (size_t i = 0; i < miniBatchSize; i++)
    {
      // Selecting a uniformly random selected, yet not repeated experience
      size_t expId = experienceReplayIndexes[i];
      miniBatches[step][i] = _experienceReplayStates[expId];
    }
  }

  neuralNetwork->normalize(miniBatches);
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplayStates.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
    _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  if (_maxOptimizationSteps > 0)
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu/%lu\n", _optimizationStepCount, _maxOptimizationSteps);
  else
    _k->_logger->logInfo("Normal", " + Optimization Step Count:         %lu\n", _optimizationStepCount);

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f\n", _bestTrainingReward);
  _k->_logger->logInfo("Normal", " + Current p(Random Action):       %f\n", _randomActionProbabilityEffectiveValue);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
    _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

  printAgentInformation();

  _k->_logger->logInfo("Normal", "Profiling Information:\n");
  _k->_logger->logInfo("Normal", " + NN Input Write Time:         %fs\n", _inputWriteTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Output Read Time:         %fs\n", _outputReadTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Normalization Time:       %fs\n", _normalizationTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Forward Propagation Time: %fs\n", _forwardStreamTime / 1.0e+9f);
  _k->_logger->logInfo("Normal", " + NN Back Propagation Time:    %fs\n", _backwardStreamTime / 1.0e+9f);
}

} // namespace solver

} // namespace korali

#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
void Agent::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  // If initial generation, set initial agent configuration
  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _totalExperienceCount = 0;
    _currentExperienceCount = 0;
    _optimizationStepCount = 0;
    _candidatePoliciesTested = 0;
    _currentSampleID = 0;

    // Initialize best rewards
    _averageTestingReward = -korali::Inf;
    _bestTrainingReward = -korali::Inf;
    _bestAverageTestingReward = -korali::Inf;
  }

  // Initializing policy hyperparameters
  updateHyperparameters(_hyperparameters);
}

void Agent::runGeneration()
{
  // Broadcasting hyperparameters for all workers to use
  KORALI_BROADCAST("Hyperparameters", _hyperparameters);

  // Configuring current agent
  Sample agent;

  agent["Sample Id"] = _currentSampleID++;
  agent["Module"] = "Problem";
  agent["Operation"] = "Run Episode";
  agent["Mode"] = "Training";

  // Initializing statistical information
  _currentReward = 0.0;
  _currentExperienceCount = 0;

  // Launching agent
  KORALI_START(agent);

  // Storage to keep experience information
  std::vector<knlohmann::json> episode;
  knlohmann::json experience;

  // Keep listening to incoming experiences until we reached a terminal state
  do
  {
   // Listening for incoming experiences
   experience = KORALI_LISTEN(agent);

   // Storing new experience into the episode history
   episode.push_back(experience);
  }
  while(experience["Is Terminal"] == false);

  // Waiting for agent to finish
  KORALI_WAIT(agent);

  // Setting current cumulative reward
  _currentReward = agent["Cumulative Reward"].get<double>();

  // Pusing the new episode into the replay history
  _experienceReplayHistory.insert(_experienceReplayHistory.end(), episode.begin(), episode.end());

  // Adding to the number of episodes
  _currentEpisode++;

  // Updating average experience count
  _currentExperienceCount = episode.size();
  _totalExperienceCount += _currentExperienceCount;

  /*********************************************************************
   * Updating experience replay memory
   *********************************************************************/

  // If the maximum number of experiences have been reached, start forgetting older excess experiences
  if (_experienceReplayHistory.size() > _experienceReplayMaximumSize)
  {
    size_t excess = _experienceReplayHistory.size() - _experienceReplayMaximumSize;
    _experienceReplayHistory.erase(_experienceReplayHistory.begin(), _experienceReplayHistory.begin() + excess);
  }

  // If the minimum number of experiences has not yet been reached, there's not much else to do
  if (_experienceReplayHistory.size() < _experienceReplayStartSize) return;

  // Updating if exceeded best training policy so far.
  if (_currentReward > _bestTrainingReward) _bestTrainingReward = _currentReward;

  // If the policy has exceeded the threshold during training, it is time to test it properly (without noise)
  // Otherwise, let current testing reward remain as -inf.
  if (_currentReward > _trainingRewardThreshold)
  {
    // Increasing tested policy counter
    _candidatePoliciesTested++;

    // Creating storage for agents
    std::vector<Sample> tests(_policyTestingEpisodes);

    // Initializing the agents and their environments
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      // Configuring Agent
      tests[i]["Sample Id"] = _currentSampleID++;
      tests[i]["Module"] = "Problem";
      tests[i]["Operation"] = "Run Episode";
      tests[i]["Mode"] = "Testing";

      // Launching agent initialization
      KORALI_START(tests[i]);
    }

    KORALI_WAITALL(tests);

    // Calculating average testing reward
    _averageTestingReward = 0.0;
    for (size_t i = 0; i < _policyTestingEpisodes; i++)
      _averageTestingReward += tests[i]["Cumulative Reward"].get<double>();
    _averageTestingReward = _averageTestingReward / _policyTestingEpisodes;
  }

  // If the average testing reward is better than the previous best, replace it
  // and store hyperparameters as best so far.
  if (_averageTestingReward > _bestAverageTestingReward) _bestAverageTestingReward = _averageTestingReward;

  // If we reached our target testing reward, there's nothing else to do
  if (_targetAverageTestingReward > -korali::Inf)
    if (_bestAverageTestingReward >= _targetAverageTestingReward) return;

  // Otherwise, optimize agent's critic/policy learners.
  for (size_t i = 0; i < _agentOptimizationStepsPerUpdate; i++)
   trainAgent();
}

void Agent::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

  _k->_logger->logInfo("Normal", " + Episode's Experience Count:  %lu\n", _currentExperienceCount);

  _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _experienceReplayHistory.size(), _experienceReplayMaximumSize);

  if (_maxEpisodes > 0)
  _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
  else
  _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

  if (_maxExperiences > 0)
  _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _totalExperienceCount, _maxExperiences);
  else
    _k->_logger->logInfo("Normal", " + Total Experience Count:    %lu\n", _totalExperienceCount);

  _k->_logger->logInfo("Normal", "Training Statistics:\n");

  _k->_logger->logInfo("Normal", " + Current Reward:              %f/%f\n", _currentReward, _trainingRewardThreshold);
  _k->_logger->logInfo("Normal", " + Best Reward:                 %f\n", _bestTrainingReward);

  _k->_logger->logInfo("Normal", "Testing Statistics:\n");

  _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _candidatePoliciesTested);

  _k->_logger->logInfo("Normal", " + Latest Average Reward:       %f\n", _averageTestingReward);

  if (_targetAverageTestingReward > -korali::Inf)
  _k->_logger->logInfo("Normal", " + Best Average Reward:         %f/%f\n", _bestAverageTestingReward, _targetAverageTestingReward);
  else
  _k->_logger->logInfo("Normal", " + Best Average Reward:         %f\n", _bestAverageTestingReward);

  printAgentInformation();
}

} // namespace solver

} // namespace korali

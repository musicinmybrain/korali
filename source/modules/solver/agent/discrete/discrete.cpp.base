#include "engine.hpp"
#include "modules/solver/agent/discrete/discrete.hpp"
#include "sample/sample.hpp"

__startNamespace__;

void __className__::initializeAgent()
{
  // Getting discrete problem pointer
  _problem = dynamic_cast<problem::reinforcementLearning::Discrete *>(_k->_problem);
}

void __className__::getAction(korali::Sample &sample)
{
  // Get action for all the agents in the environment
  for (size_t i = 0; i < sample["State"].size(); i++)
  {
    // Getting current state
    auto state = sample["State"][i].get<std::vector<float>>();

    // Adding state to the state time sequence
    _stateTimeSequence.add(state);

    // Getting the probability of the actions given by the agent's policy
    auto policy = runPolicy({_stateTimeSequence.getVector()})[0];
    const auto &pActions = policy.actionProbabilities;

    // Storage for the action index to use
    size_t actionIdx = 0;

    /*****************************************************************************
  * During training, we follow the Epsilon-greedy strategy. Choose, given a
  * probability (pEpsilon), one from the following:
  *  - Uniformly random action among all possible actions
  *  - Sample action guided by the policy's probability distribution
  ****************************************************************************/

    if (sample["Mode"] == "Training")
    {
      // Producing random (uniform) number for the selection of the action
      const float x = _uniformGenerator->getRandomNumber();

      // Categorical action sampled from action probabilites (from ACER paper [Wang2017])
      float curSum = 0.0;
      for (actionIdx = 0; actionIdx < pActions.size() - 1; actionIdx++)
      {
        curSum += pActions[actionIdx];
        if (x < curSum) break;
      }

      // NOTE: In original DQN paper [Minh2015] we choose max
      // actionIdx = std::distance(pActions.begin(), std::max_element(pActions.begin(), pActions.end()));
    }

    /*****************************************************************************
  * During testing, we just select the action with the largest probability
  * given by the policy.
  ****************************************************************************/

    // Finding the best action index from the probabilities
    if (sample["Mode"] == "Testing")
      actionIdx = std::distance(pActions.begin(), std::max_element(pActions.begin(), pActions.end()));

    /*****************************************************************************
  * Storing the action itself
 ****************************************************************************/

    // Storing action itself, its idx, and probabilities
    sample["Policy"][i]["Distribution Parameters"] = pActions;
    sample["Policy"][i]["Action Index"] = actionIdx;
    sample["Policy"][i]["State Value"] = policy.stateValue;
    sample["Action"][i] = _problem->_possibleActions[actionIdx];
  }
}

float __className__::calculateImportanceWeight(const std::vector<float> &action, const policy_t &curPolicy, const policy_t &oldPolicy)
{
  const auto &pVectorCurPolicy = curPolicy.actionProbabilities;
  const auto &pVectorOldPolicy = oldPolicy.actionProbabilities;
  auto actionIdx = oldPolicy.actionIndex;

  // Getting probability density of action for current policy
  float pCurPolicy = pVectorCurPolicy[actionIdx];

  // Getting probability density of action for old policy
  float pOldPolicy = pVectorOldPolicy[actionIdx];

  // Now calculating importance weight for the old s,a experience
  float constexpr epsilon = 0.00000001f;
  float importanceWeight = pCurPolicy / (pOldPolicy + epsilon);

  // Safety checks
  if (importanceWeight > 1024.0f) importanceWeight = 1024.0f;
  if (importanceWeight < -1024.0f) importanceWeight = -1024.0f;

  return importanceWeight;
}

std::vector<float> __className__::calculateImportanceWeightGradient(const policy_t &curPolicy, const policy_t &oldPolicy)
{
  std::vector<float> grad(_problem->_possibleActions.size() + 1, 0.0);

  const float invTemperature = curPolicy.distributionParameters[_problem->_possibleActions.size()];
  const size_t oldActionIdx = oldPolicy.actionIndex;
  float importanceWeight = curPolicy.actionProbabilities[oldActionIdx] / oldPolicy.actionProbabilities[oldActionIdx];

  // Safety checks
  if (importanceWeight > 1024.0f) importanceWeight = 1024.0f;
  if (importanceWeight < -1024.0f) importanceWeight = -1024.0f;

  // calculate gradient of categorical distribution normalized by old pvals
  for (size_t i = 0; i < _problem->_possibleActions.size(); i++)
  {
    if (i == oldActionIdx)
      grad[i] = importanceWeight * (1. - curPolicy.actionProbabilities[i]) * invTemperature;
    else
      grad[i] = -importanceWeight * curPolicy.actionProbabilities[i] * invTemperature;
  }

  // calculate gradient of categorial distribution wrt. inverse temperature
  grad[_problem->_possibleActions.size()] = 0.0; // TODO

  return grad;
}

std::vector<float> __className__::calculateKLDivergenceGradient(const policy_t &oldPolicy, const policy_t &curPolicy)
{
  std::vector<float> klGrad(_problem->_possibleActions.size() + 1, 0.0);

  // Gradient wrt NN output i
  for (size_t i = 0; i < _problem->_possibleActions.size(); i++)
  {
    // Iterate over all pvalues
    for (size_t j = 0; j < _problem->_possibleActions.size(); j++)
    {
      if (i == j)
        klGrad[i] -= oldPolicy.actionProbabilities[j] * (1.0 - curPolicy.actionProbabilities[i]);
      else
        klGrad[i] += oldPolicy.actionProbabilities[j] * curPolicy.actionProbabilities[i];
    }
  }

  klGrad[_problem->_possibleActions.size()] = 0.0; // TODO

  return klGrad;
}

__moduleAutoCode__;

__endNamespace__;

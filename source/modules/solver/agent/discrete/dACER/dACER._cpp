#include "engine.hpp"
#include "modules/solver/agent/discrete/dACER/dACER.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace solver
{
namespace agent
{
namespace discrete
{
void dACER::initializeAgent()
{
  // Initializing common discrete agent configuration
  Discrete::initializeAgent();

  if (_experienceReplayStartSize < _trajectorySize*(_offPolicyUpdates+1.))
      KORALI_LOG_ERROR("Experience Replay Start Size too small, this might cause undefined behaviour. Increase value of Start Size.");
  /*********************************************************************
 * Initializing Critic-Related Structures
 *********************************************************************/

  korali::Engine engine; // Engine to initialize experiments with

  _criticExperiment["Problem"]["Type"] = "Supervised Learning";
  _criticExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _criticExperiment["Solver"]["Optimizer"] = _criticOptimizer;
  _criticExperiment["Solver"]["Learning Rate"] = _criticLearningRate;
  _criticExperiment["Solver"]["Loss Function"] = "Mean Squared Error";
  _criticExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticExperiment["Solver"]["Neural Network"] = _criticNeuralNetwork;

  for (size_t i = 0; i < _problem->_stateVectorSize; i++)
    _criticExperiment["Problem"]["Inputs"][0][i] = 0.0;

  for (size_t i = 0; i < _problem->_possibleActions.size(); i++)
    _criticExperiment["Problem"]["Solution"][0][i] = 0.0;

  // Running initialization to verify that the configuration is correct
  engine.initialize(_criticExperiment);

  // Getting learner pointers
  _criticProblem = dynamic_cast<problem::SupervisedLearning *>(_criticExperiment._problem);
  _criticLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticExperiment._solver);

  /*********************************************************************
  * Initializing Policy-Related Structures
  *********************************************************************/

  // Creating and running Actor Learning Experiments

  _policyExperiment["Problem"]["Type"] = "Supervised Learning";
  _policyExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _policyExperiment["Solver"]["Loss Function"] = "Direct Gradient";
  _policyExperiment["Solver"]["Optimizer"] = _policyOptimizer;
  _policyExperiment["Solver"]["Learning Rate"] = _policyLearningRate;
  _policyExperiment["Solver"]["Steps Per Generation"] = 1;
  _policyExperiment["Solver"]["Neural Network"] = _policyNeuralNetwork;

  // Initializing experiment with an initial zero set
  for (size_t j = 0; j < _problem->_stateVectorSize; j++)
    _policyExperiment["Problem"]["Inputs"][0][j] = 0.0;

  for (size_t j = 0; j < _problem->_possibleActions.size(); j++)
    _policyExperiment["Problem"]["Solution"][0][j] = 0.0;

  // Running initialization to verify that the configuration is correct
  engine.initialize(_policyExperiment);

  // Getting learner pointers
  _policyProblem = dynamic_cast<problem::SupervisedLearning *>(_policyExperiment._problem);
  _policyLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_policyExperiment._solver);

  /*********************************************************************
  * Loading/Setting Hyperparameters
  *********************************************************************/

  // Getting current hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getHyperparameters();

  // Storing initial average policy parameters
  if (_k->_currentGeneration == 0)
    _policyAverageHyperparameters = _policyCurrentHyperparameters;

  // Get the initial set of policy NN hyperparameters
  _hyperparameters["Policy"] = _policyCurrentHyperparameters;
}

void dACER::trainAgent()
{
  // Resetting critic statistics
  _cumulativeQStar = 0;

  // Creating storage for the starting index for all current trajectories
  std::vector<size_t> trajectoryStartIndexes;
  std::vector<size_t> trajectoryEndIndexes;

  // Detecting trajectories
  size_t curTrajectoryPos = 0;
  for (size_t i = 0; i < _experienceReplayStates.size(); i++)
  {
    if (curTrajectoryPos == 0) trajectoryStartIndexes.push_back(i);

    if ((curTrajectoryPos == _trajectorySize - 1) || // Reached specified trajectory size
        (_experienceReplayTerminal[i] == true) ||    // The experience is terminal
        (i == _experienceReplayStates.size() - 1))   // Reached the end of the replay experience
    {
      trajectoryEndIndexes.push_back(i);
      curTrajectoryPos = 0;
    }
    else
    {
      curTrajectoryPos++;
    }
  }

  // Detecting final complete trajectory
  size_t lastIdx = trajectoryStartIndexes.size() - 2;

  // Creating index vector for random off-policy trajectory selection
  std::vector<size_t> trajectoryIndexes(lastIdx);
  for (size_t i = 0; i < lastIdx; i++) trajectoryIndexes[i] = i;

  // Shuffling trajectory
  std::shuffle(trajectoryIndexes.begin(), trajectoryIndexes.end(), *mt);

  // Performing off-policy updates
  for (size_t i = 0; i < _numOffPolicyUpdates; i++)
  {
    size_t tIdx = trajectoryIndexes[i];
    processTrajectory(trajectoryStartIndexes[tIdx], trajectoryEndIndexes[tIdx]);
  }
}

void dACER::processTrajectory(size_t startId, size_t endId)
{
  size_t experienceCount = endId - startId;
  _criticProblem->_inputs.resize(experienceCount);
  _criticProblem->_solution.resize(experienceCount);
  _policyProblem->_inputs.resize(experienceCount);
  _policyProblem->_solution.resize(experienceCount);

  // Going straight to the latest experience in the corresponding trajectory
  size_t curId = endId;

  // First, get the retrace value for the last experience (just the reward)
  float qRet = _experienceReplayRewards[curId];
  if (_experienceReplayTerminal[curId] == false)
  {
    std::vector<float> curState = _experienceReplayStates[curId];
    qRet = stateValueFunction(curState);
  }

  // Keeping track of the current experience
  size_t curExperience = 0;

  while (curId > startId)
  {
    // Decreasing current experience index
    curId--;

    // Getting current reward
    float curReward = _experienceReplayRewards[curId];

    // Re-calculating qRet (equation 5 in ACER paper, also see below)
    qRet = curReward + _criticDiscountFactor * qRet;

    // Getting experience's state
    std::vector<float> curState = _experienceReplayStates[curId];

    // Calculating V(state) with the current policy
    float vCurState = stateValueFunction(curState);

    //// Computing quantities needed for updating weights

    // Getting experience's action
    size_t curActionIdx = _experienceReplayActionIndexes[curId];

    // Getting probability densities for current action given past behaviour
    auto pOldPolicy = _experienceReplayActionProbabilities[curId];

    // Getting probability densities for current action given current policy
    auto pCurPolicy = getActionProbabilities(curState);

    // Getting p(s,a) for selected experience, given the old policy
    float pActionOldPolicy = pOldPolicy[curActionIdx];

    // Getting p(s,a) for the best action, given the current policy
    float pActionCurPolicy = pCurPolicy[curActionIdx];

    // Now calculating importance weight for the old s,a experience
    float importanceWeight = pActionCurPolicy / pActionOldPolicy;

    float qCurState = stateActionValueFunction(curState, curActionIdx);

    /*****************************************
    * Policy Section
    *****************************************/

    // Now calculating truncated importance weight
    float truncatedImportanceWeight = std::min(_importanceWeightTruncation, importanceWeight);

    // Now Calculating Acer gradient, from https://arxiv.org/pdf/1611.01224.pdf, Eq. 9
    std::vector<float> gPolicy(_problem->_possibleActions.size(), 0.0f);

    // First part of the gradient vector
    gPolicy[curActionIdx] = truncatedImportanceWeight * (1.0f / pActionCurPolicy * (qRet - vCurState));

    // Compute the expectation for the second part of the gradient vector
    for (size_t newActionIdx = 0; newActionIdx < _problem->_possibleActions.size(); newActionIdx++)
    {
      // Calculating Qcritic(state,action)
      float qCritic = stateActionValueFunction(curState, newActionIdx);

      // Getting p(s,a) for selected action, given the old policy
      float pNewActionOldPolicy = pOldPolicy[newActionIdx];

      // Getting p(s,a) for selected action, given the current policy
      float pNewActionCurPolicy = pCurPolicy[newActionIdx];

      // Now calculating importance weight for the old s,a experience
      float newImportanceWeight = pNewActionCurPolicy / pNewActionOldPolicy;

      // Now calculating r(a) - c / r(a) ratio
      float importanceRatio = 1.0f - (_importanceWeightTruncation / newImportanceWeight);

      // Now calculating the correction weight
      float correctionWeight = importanceRatio > 0.0f ? importanceRatio : 0.0f;

      // Adding the component of part two to the gradient vector
      gPolicy[newActionIdx] += correctionWeight * (qCritic - vCurState);
    }

    ///////////////// KL Calculation /////////////////////

    // Now calculating trust region, if required
    if (_policyTrustRegionEnabled)
    {
      // Getting probability densities for current action given average policy
      _policyLearner->setHyperparameters(_policyAverageHyperparameters);
      auto pAvgPolicy = getActionProbabilities(curState);

      // Obtaining KL Divergence gradients for the current state. //PW: check this
      std::vector<float> k(_problem->_possibleActions.size());
      for (size_t i = 0; i < k.size(); i++)
        k[i] = -1 / pCurPolicy[i];

      // Getting dot product between the gradient vector and k
      auto gkDotProduct = dotProduct(k, gPolicy);

      // Getting norm(k)^2, simply by dot product of k and itself
      auto kNormSquared = dotProduct(k, k);

      // Getting magnitude of adjustment
      float adjustmentMagnitude = std::max(0.0f, (gkDotProduct - _policyTrustRegionDivergenceConstraint) / kNormSquared);

      // Adjusting gradients to trust region
      for (size_t i = 0; i < _problem->_possibleActions.size(); i++) gPolicy[i] = gPolicy[i] - adjustmentMagnitude * k[i];
    }

    ///////////////// Storing Gradients /////////////////////

    _policyProblem->_inputs[curExperience] = curState;
    _policyProblem->_solution[curExperience] = gPolicy;

    /*****************************************
    * Critic Section
    *****************************************/

    // Creating storage for the gradient vector
    std::vector<float> gCritic(_problem->_possibleActions.size(), 0);

    // Update the gradient vector with the value of the retrace function for g  = Q-Qret
    gCritic[curActionIdx] = qRet;

    // Updating inputs to training learner
    _criticProblem->_inputs[curExperience] = curState;
    _criticProblem->_solution[curExperience] = gCritic;

    // Keeping statistics
    _cumulativeQStar += gCritic[curActionIdx];

    /*****************************************
     * Updating the value of qRet
     *****************************************/

    // Now calculating truncated importance weight with 1.0 as truncation factor
    truncatedImportanceWeight = std::min(1.0f, importanceWeight);

    // Updating qRet (equation 5 in ACER paper)
    qRet = truncatedImportanceWeight * (qRet - qCurState) + vCurState;

    // Increasing current experience count
    curExperience++;
  }

  // Running one generation of the optimization method with the given mini-batch
  _criticLearner->initialize();
  _criticLearner->runGeneration();
  _criticLearner->finalize();

  // Running one generation of the optimization method with the given mini-batch
  _policyLearner->initialize();
  _policyLearner->runGeneration();
  _policyLearner->finalize();

  /****************************************************************************
 * If batch normalization is being used, we need to adjust mean and variances
 * by sampling a few more mini-batches after the optimization steps
 ******************************************************************************/

  normalizeStateNeuralNetwork(_criticLearner->_trainingNeuralNetwork, curExperience, _criticNormalizationSteps);
  normalizeStateNeuralNetwork(_policyLearner->_trainingNeuralNetwork, curExperience, _policyNormalizationSteps);

  // Storing new inference parameters
  auto criticHyperparameters = _criticLearner->getHyperparameters();
  _criticLearner->setHyperparameters(criticHyperparameters);

  /****************************************************************************
 * Updating Policy
 ******************************************************************************/

  // Getting new policy hyperparameters
  _policyCurrentHyperparameters = _policyLearner->getHyperparameters();

  // If using a trust region, softly adopting the new parameters of the average policy, using an adoption rate
  if (_policyTrustRegionEnabled)
    for (size_t i = 0; i < _policyCurrentHyperparameters.size(); i++)
      _policyAverageHyperparameters[i] = _policyTrustRegionAdoptionRate * _policyAverageHyperparameters[i] + (1 - _policyTrustRegionAdoptionRate) * _policyCurrentHyperparameters[i];

  // Updating policy with new averaged parameters
  _policyLearner->setHyperparameters(_policyCurrentHyperparameters);

  // Storing average policy hyperparameters
  _hyperparameters["Policy"] = _policyCurrentHyperparameters;
}

std::vector<float> dACER::getActionProbabilities(const std::vector<float> &state)
{
  // Forward policy
  auto pValues = _policyLearner->getEvaluation(state);

  return pValues;
}

float dACER::stateActionValueFunction(const std::vector<float> &state, const size_t &actionIdx)
{
  // Forward propagating state through the critic to get Q(s,a) for all A
  auto evaluation = _criticLearner->getEvaluation(state);

  return evaluation[actionIdx];
}

float dACER::stateValueFunction(const std::vector<float> &state)
{
  // Forward propagating state through the critic to get Q(s,a) for all a
  auto qEval = _criticLearner->getEvaluation(state);

  // Forward propagating state through the policy to get P(s,a) for all a
  auto pEval = getActionProbabilities(state);

  // Calculating the sum of p(s,a)*q(s,a) for all a
  float qSum = 0.0;
  for (size_t i = 0; i < qEval.size(); i++)
    qSum += qEval[i] * pEval[i];

  // Returning the sum
  return qSum;
}

void dACER::updateHyperparameters(const knlohmann::json &hyperparameters)
{
  _policyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<float>>());
}

void dACER::printAgentInformation()
{
  _k->_logger->logInfo("Normal", "Critic Information:\n");

  _k->_logger->logInfo("Normal", " + Cumulative Expected Q-Value:     %f\n", _cumulativeQStar);
  _k->_logger->logInfo("Normal", " + Average Expected Q-Value:        %f\n", _averageQStar);

  _criticExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _criticExperiment._solver->printGenerationAfter();
  _criticExperiment._logger->setVerbosityLevel("Silent");

  _k->_logger->logInfo("Normal", "Policy Information:\n");

  _policyExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
  _policyExperiment._solver->printGenerationAfter();
  _policyExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace discrete
} // namespace agent
} // namespace solver
} // namespace korali

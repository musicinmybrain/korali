{

 "Configuration Settings":
 [
  {
    "Name": [ "Experience Replay", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Experience Replay", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Agent", "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "The number of episodes from which to draw experiences at each generation."
  },
  {
    "Name": [ "Agent", "Experience Limit" ],
    "Type": "size_t",
    "Description": "The number of most recent experiences returned by the actor."
  },
  {
   "Name": [ "Importance Weight", "Lambda" ],
   "Type": "double",
   "Description": "Multiplier of the importance factor."
  },
  {
   "Name": [ "Importance Weight", "Truncation" ],
   "Type": "double",
   "Description": "Maximum value (c) for the importance weight (to prevent it from exploding), before multiplying it by Lambda."
  },
  {
   "Name": [ "Average Training Reward Threshold" ],
   "Type": "double",
   "Description": "Minimum value (r) of the episode's average training reward for a policy to be considered as candidate."
  },
  {
   "Name": [ "Policy Testing Episodes" ],
   "Type": "size_t",
   "Description": "Number of test episodes to run the policy (without noise) for, for which the average reward will serve to evaluate the reward termination criteria."
  },
  {
   "Name": [ "Normalization Steps" ],
   "Type": "size_t",
   "Description": "How many mini-batches will be used to correct mean/variance bias for batch normalized layers."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_maxExperiences > 0) && (_experienceReplayHistory.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Target Average Testing Reward" ],
    "Type": "double",
    "Criteria": "(_targetAverageTestingReward > -korali::Inf) && (_bestAverageTestingReward >= _targetAverageTestingReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  },
  {
    "Name": [ "Max Optimization Steps" ],
    "Type": "size_t",
    "Criteria": "(_maxOptimizationSteps > 0) && (_optimizationStepCount >= _maxOptimizationSteps)",
    "Description": "The solver will stop when the given number of optimizations have been made to the learner."
  }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
    "Name": [ "Experience Replay", "History" ],
    "Type": "std::vector<knlohmann::json>",
    "Description": "Stores experience replay history."
  },
  {
   "Name": [ "Total Experience Count" ],
   "Type": "size_t",
   "Description": "Total number of experiences in the generation."
  },
  {
   "Name": [ "Min Experience Count" ],
   "Type": "size_t",
   "Description": "Minimum number of experiences among the actors."
  },
  {
   "Name": [ "Max Experience Count" ],
   "Type": "size_t",
   "Description": "Maximum number of experiences among the actors."
  },
  {
   "Name": [ "Average Experience Count" ],
   "Type": "double",
   "Description": "Average number of experiences among the actors."
  },
  {
    "Name": [ "Average Training Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found on this generation's training episodes."
  },
  {
    "Name": [ "Average Testing Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found on this generation's testing episodes, if any."
  },
  {
    "Name": [ "Best Average Training Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found so far from training episodes."
  },
  {
    "Name": [ "Candidate Policies Tested" ],
    "Type": "size_t",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
    "Name": [ "Best Average Testing Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the current policy configuration ."
  },
  {
   "Name": [ "Cumulative Q Star" ],
   "Type": "double",
   "Description": "Sum of E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Average Q Star" ],
   "Type": "double",
   "Description": "Average E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Optimization Step Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of optimization steps performed by the learner."
  },
  {
    "Name": [ "Random Action Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for selecting random actions during the startup phase."
  },
  {
   "Name": [ "Current Sample ID" ],
   "Type": "size_t",
   "Description": "Keeps track of the current Sample ID, to make sure no two equal sample IDs are produced and that this value can be used as random seed."
  }
 ],

 "Module Defaults":
 {
   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Optimization Steps": 0,
    "Target Average Testing Reward": -Infinity 
   },
   
  "Experience Replay":
   {
    "Maximum Size": 10000,
    "Start Size": 1000
   },
   
   "Agent":
   {
     "Episodes Per Generation": 32,
     "Experience Limit": 4
   },
 
   "Importance Weight":
   {
     "Lambda": 1.0,
     "Truncation": 1.0
   },
   
   "Random Action Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   },
   
   "Policy Testing Episodes": 10,
   "Normalization Steps": 32
 },
 
 "Variable Defaults":
 {
 
 }
 
}

{

  "Module Data":
  {
    "Class Name": "Agent",
    "Namespace": ["korali", "solver"],
    "Parent Class Name": "Solver"
  },

 "Configuration Settings":
 [
   {
   "Name": [ "Mode" ],
   "Type": "std::string",
   "Options": [
               { "Value": "Training", "Description": "Learns a policy for the reinforcement learning problem." },
               { "Value": "Testing", "Description": "Tests the policy with a learned policy." }
              ],
   "Description": "Specifies the operation mode for the agent."
  },
  {
    "Name": [ "Testing", "Sample Ids" ],
    "Type": "std::vector<size_t>",
    "Description": "A vector with the identifiers for the samples to test the hyperparameters with."
  },
  {
    "Name": [ "Testing", "Current Policies" ],
    "Type": "knlohmann::json",
    "Description": "The current hyperparameters of the policies to test."
  },
  {
    "Name": [ "Training", "Average Depth" ],
    "Type": "size_t",
    "Description": "Specifies the depth of the running training average to report."
  },
  {
    "Name": [ "Concurrent Workers" ],
    "Type": "size_t",
    "Description": "Indicates the number of concurrent environments to use to collect experiences."
  },
  {
    "Name": [ "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "Number of reinforcement learning episodes per Korali generation (checkpoints are generated between generations)."
  },
  {
    "Name": [ "Mini Batch", "Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network(s) with."
  },
  {
    "Name": [ "Time Sequence Length" ],
    "Type": "size_t",
    "Description": "Indicates the number of contiguous experiences to pass to the NN for learning. This is only useful when using recurrent NNs."
  },
  {
    "Name": [ "Learning Rate" ],
    "Type": "float",
    "Description": "The initial learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "L2 Regularization", "Enabled" ],
   "Type": "bool",
   "Description": "Boolean to determine if l2 regularization will be applied to the neural networks."
  },
  {
   "Name": [ "L2 Regularization", "Importance" ],
   "Type": "float",
   "Description": "Coefficient for l2 regularization."
  },
  {
    "Name": [ "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the hidden neural network layers."
  },
  {
    "Name": [ "Neural Network", "Optimizer" ],
    "Type": "std::string",
    "Description": "Indicates the optimizer algorithm to update the NN hyperparameters."
  },
  {
   "Name": [ "Neural Network", "Engine" ],
   "Type": "std::string",
   "Description": "Specifies which Neural Network backend to use."
  },
  {
   "Name": [ "Discount Factor" ],
   "Type": "float",
   "Description": "Represents the discount factor to weight future experiences."
  },
  {
   "Name": [ "Importance Weight Truncation Level" ],
   "Type": "float",
   "Description": "Represents the discount factor to weight future experiences."
  },
  {
    "Name": [ "State Rescaling", "Enabled" ],
    "Type": "bool",
    "Description": "Determines whether to normalize the states, such that they have mean 0 and standard deviation 1 (done only once after the initial exploration phase)."
  },
  {
    "Name": [ "Reward", "Rescaling", "Enabled" ],
    "Type": "bool",
    "Description": "Determines whether to normalize the rewards, such that they have mean 0 and standard deviation 1"
  },
  {
    "Name": [ "Feature Rescaling", "Enabled" ],
    "Type": "bool",
    "Description": "Determines whether to normalize the features such that they have mean 0 and standard deviation 1 (done only once after the initial exploration phase)."
  },
  {
    "Name": [ "Experience Replay", "Serialize" ],
    "Type": "bool",
    "Description": "Indicates whether to serialize and store the experience replay after each generation. Disabling will reduce I/O overheads but will disable the checkpoint/resume function."
  },
  {
    "Name": [ "Experience Replay", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences before learning starts."
  },
  {
    "Name": [ "Experience Replay", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The size of the replay memory. If this number is exceeded, experiences are deleted."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Cutoff Scale" ],
    "Type": "float",
    "Description": "Initial Cut-Off to classify experiences as on- or off-policy. (c_max in https://arxiv.org/abs/1807.05827)"
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Target" ],
    "Type": "float",
    "Description": "Target fraction of off-policy experiences in the replay memory. (D in https://arxiv.org/abs/1807.05827)"
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Annealing Rate" ],
    "Type": "float",
    "Description": "Annealing rate for Off Policy Cutoff Scale and Learning Rate. (A in https://arxiv.org/abs/1807.05827)"
  },
  {
   "Name": [ "Experience Replay", "Off Policy", "REFER Beta" ],
   "Type": "float",
   "Description": "Initial value for the penalisation coefficient for off-policiness. (beta in https://arxiv.org/abs/1807.05827)"
  },
  {
    "Name": [ "Experiences Between Policy Updates" ],
    "Type": "float",
    "Description": "The number of experiences to receive before training/updating (real number, may be less than < 1.0, for more than one update per experience)."
  },
  {
    "Name": [ "Experiences Between Reward Updates" ],
    "Type": "float",
    "Description": "The number of experiences to receive before updating parameter of reward function."
  },
  {
    "Name": [ "Optimize Max Entropy Objective" ],
    "Type": "bool",
    "Description": "If true, optimize max entropy objective, else maximize feature reward on observed trajectories."
  },
  {
    "Name": [ "Use Fusion Distribution" ],
    "Type": "bool",
    "Description": "Evaluate policy probabilities with fusion distribution."
  },
  {
    "Name": [ "Demonstration Batch Size" ],
    "Type": "size_t",
    "Description": "The number of sampled observed trajectories to update the weights of the reward function."
  },
  {
    "Name": [ "Background Batch Size" ],
    "Type": "size_t",
    "Description": "The number of sampled trajectories from the experience replay to update the weights of the reward function."
  },
  {
    "Name": ["Background Sample Size" ],
    "Type": "size_t",
    "Description": "Maximal number of stored background trajectories. If this number is exceeded, background trajectories are replaced."
  },
  {
    "Name": [ "Reward Function", "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "TODO"
  },
  {
    "Name": [ "Reward Function","Learning Rate" ],
    "Type": "float",
    "Description": "The learning rate to update the neural network of the reward function."
  },
  {
    "Name": [ "Reward Function","Batch Size" ],
    "Type": "size_t",
    "Description": "The batch size used during reward function updates, ideally the length of an episode."
  },
  {
    "Name": [ "Multi Agent Relationship" ],
    "Type": "std::string",
    "Options": [
      { "Value": "Individual", "Description": "Each Agent learns solely on his own." }
    ],
    "Description": "Specifies whether we are in an individual setting or collaborator setting."
  },
  {
    "Name": [ "Reward Function","L2 Regularization", "Enabled" ],
    "Type": "float",
    "Description": "TODO"
  },
  {
    "Name": [ "Multi Agent Correlation" ],
    "Type": "bool",
    "Description": "Specifies whether we take into account the dependencies of the agents or not."
  },
  {
    "Name": [ "Reward Function","L2 Regularization", "Importance" ],
    "Type": "float",
    "Description": "TODO"
  },
  {
    "Name": [ "Multi Agent Sampling" ],
    "Type": "std::string",
    "Options": [
        { "Value": "Tuple", "Description": "Sample expId and use same value for all agents." }
    ],
    "Description": "Specifies how to sample the minibatch."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of episodes have been run."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxExperiences > 0) && (_experienceCount >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Max Policy Updates" ],
    "Type": "size_t",
    "Criteria": "(_mode == \"Training\") && (_maxPolicyUpdates > 0) && (_policyUpdateCount >= _maxPolicyUpdates)",
    "Description": "The solver will stop when the given number of optimization steps have been performed."
  }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
   "Name": [ "Policy", "Parameter Count" ],
   "Type": "size_t",
   "Description": "Stores the number of parameters that determine the probability distribution for the current state sequence."
  }, 
  {
    "Name": [ "Action Lower Bounds" ],
    "Type": "std::vector<float>",
    "Description": "Lower bounds for actions."
  },
  {
    "Name": [ "Action Upper Bounds" ],
    "Type": "std::vector<float>",
    "Description": "Upper bounds for actions."
  },
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
    "Name": [ "Training", "Reward History" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Keeps a history of all training episode rewards."
  },
  {
    "Name": [ "Training", "Feature Reward History" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Keeps a history of all training episode feature rewards."
  },
  {
    "Name": [ "Training", "Experience History" ],
    "Type": "std::vector<size_t>",
    "Description": "Keeps a history of all training episode experience counts."
  },
  {
    "Name": [ "Testing", "Average Reward History" ],
    "Type": "std::vector<float>",
    "Description": "Keeps a history of all training episode rewards."
  },
  {
    "Name": [ "Training", "Average Feature Reward" ],
    "Type": "std::vector<float>",
    "Description": "Contains a running average of the training episode feature rewards."
  },
  {
    "Name": [ "Training", "Average Reward" ],
    "Type": "std::vector<float>",
    "Description": "Contains a running average of the training episode rewards."
  },
  {
    "Name": [ "Training", "Last Reward" ],
    "Type": "std::vector<float>",
    "Description": "Remembers the cumulative sum of rewards for the last training episode."
  },
  {
    "Name": [ "Training", "Best Reward" ],
    "Type": "std::vector<float>",
    "Description": "Remembers the best cumulative sum of rewards found so far in any episodes."
  },
  {
    "Name": [ "Training", "Best Episode Id" ],
    "Type": "std::vector<size_t>",
    "Description": "Remembers the episode that obtained the maximum cumulative sum of rewards found so far."
  },
  {
    "Name": [ "Training", "Best Reward Params" ],
    "Type": "knlohmann::json",
    "Description": "Stores the best training policies configuration found so far."
  },
  {
    "Name": [ "Training", "Current Policies" ],
    "Type": "knlohmann::json",
    "Description": "Stores the current training policies configuration."
  },
  {
    "Name": [ "Training", "Best Policies" ],
    "Type": "knlohmann::json",
    "Description": "Stores the best training policies configuration found so far."
  },
  {
    "Name": [ "Testing", "Reward" ],
    "Type": "std::vector<float>",
    "Description": "The cumulative sum of rewards obtained when evaluating the testing samples."
  },
  {
    "Name": [ "Testing", "Best Reward" ],
    "Type": "float",
    "Description": "Remembers the best cumulative sum of rewards from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Worst Reward" ],
    "Type": "float",
    "Description": "Remembers the worst cumulative sum of rewards from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Episode Id" ],
    "Type": "size_t",
    "Description": "Remembers the episode Id that obtained the maximum cumulative sum of rewards found so far during testing."
  },
  {
    "Name": [ "Testing", "Candidate Count" ],
    "Type": "size_t",
    "Description": "Remembers the number of candidate policies tested so far."
  },
  {
    "Name": [ "Testing", "Average Reward" ],
    "Type": "float",
    "Description": "Remembers the average cumulative sum of rewards from latest testing episodes, if any."
  },
  {
    "Name": [ "Testing", "Best Average Reward" ],
    "Type": "float",
    "Description": "Remembers the best cumulative sum of rewards found so far from testing episodes."
  },
  {
    "Name": [ "Testing", "Best Policies" ],
    "Type": "knlohmann::json",
    "Description": "Stores the best testing policies configuration found so far."
  },
  {
    "Name": [ "Testing", "Best Reward Params" ],
    "Type": "knlohmann::json",
    "Description": "Stores the best testing policies configuration found so far."
  },
  {
    "Name": [ "Experience Replay", "Off Policy", "Count" ],
    "Type": "std::vector<size_t>",
    "Description": "Number of off-policy experiences in the experience replay."
  }, 
  {
    "Name": [ "Experience Replay", "Off Policy", "Ratio" ],
    "Type": "std::vector<float>",
    "Description": "Current off policy ratio in the experience replay."
  }, 
  {
    "Name": [ "Experience Replay", "Off Policy", "Current Cutoff" ],
    "Type": "float",
    "Description": "Indicates the current cutoff to classify experiences as on- or off-policy."
  },
  {
   "Name": [ "Experience Replay", "Off Policy", "REFER Current Beta" ],
   "Type": "std::vector<float>",
   "Description": "Vector of the current penalisation coefficient for off-policiness for each agent."
  },
  {
   "Name": [ "Experience Replay", "Off Policy", "History" ],
   "Type": "std::vector<std::vector<float>>",
   "Description": "Keeps a history of all off policy ratios."
  },
  {
    "Name": [ "Current Learning Rate" ],
    "Type": "float",
    "Description": "The current learning rate to use for the NN hyperparameter optimization."
  },
  {
   "Name": [ "Policy Update Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of policy updates that have been performed."
  },
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator."
  },
  {
    "Name": [ "Experience Count" ],
    "Type": "size_t",
    "Description": "Count of the number of experiences produced so far."
  },
  {
    "Name": [ "Reward", "Rescaling", "Sigma" ],
    "Type": "float",
    "Description": "Contains the standard deviation of the rewards. They will be scaled by this value in order to normalize the reward distribution in the RM."
  },
  {
    "Name": [ "Reward", "Rescaling", "Sum Squared Rewards" ],
    "Type": "float",
    "Description": "Sum of squared rewards in experience replay."
  },
  {
    "Name": [ "Log Partition Function" ],
    "Type": "float",
    "Description": "The logarithm of the estimated partition function"
  }, 
  {
    "Name": [ "Log Sdev Partition Function" ],
    "Type": "float",
    "Description": "The logarithm of the standard deviation of the partition function estimate"
  },
  {
    "Name": [ "State Rescaling", "Means" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Contains the mean of the states. They will be shifted by this value in order to normalize the state distribution in the RM."
  },
  {
    "Name": [ "State Rescaling", "Sigmas" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Standard deviation of states during exploration phase."
  },
  {
    "Name": [ "Demonstration Policy" ],
    "Type": "std::string",
    "Description": "Choose between 'Constant', 'Linear' or 'Quadratic'."
  },
  {
    "Name": [ "Feature Rescaling", "Means" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Contains the mean of the features during exploration phase. They will be shifted by this value in order to normalize the state distribution in the RM."
  },
  {
    "Name": [ "Feature Rescaling", "Sigmas" ],
    "Type": "std::vector<std::vector<float>>",
    "Description": "Standard deviation of features during exploration phase."
  },
  {
   "Name": [ "Reward Update Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of reward function updates that have been performed."
  },
  {
    "Name": ["Background Trajectory Count" ],
    "Type": "size_t",
    "Description": "Number of samples collected from background distribution."
  },
  {
    "Name": [ "Effective Minibatch Size" ],
    "Type": "size_t",
    "Description": "Effective Minibatch Size in the context of MARL."
  },
  {
    "Name": [ "Demonstration Log Probability" ],
    "Type": "std::vector<float>",
    "Description": "The log probability of the demonstration batch using the latest policy."
  },
  {
    "Name": [ "Background Log Probability" ],
    "Type": "std::vector<float>",
    "Description": "The log probability of the background batch using the latest policy."
  },
  {
    "Name": [ "Demonstration Feature Reward" ],
    "Type": "std::vector<float>",
    "Description": "The cumulative feature rewards of the demonstration batch."
  },
  {
    "Name": [ "Background Feature Reward" ],
    "Type": "std::vector<float>",
    "Description": "The cumulative feature rewards of the demonstration batch."
  },
  {
    "Name": [ "Max Entropy Objective" ],
    "Type": "std::vector<float>",
    "Description": "The history of the max entropy objective."
  },
  {
    "Name": [ "Log Partition Function History" ],
    "Type": "std::vector<float>",
    "Description": "The history of the log partition function."
  },
  {
    "Name": [ "Demonstration Batch Importance Weight" ],
    "Type": "std::vector<float>",
    "Description": "History of avg demonstration batch importance weights."
  },
  {
    "Name": [ "Background Batch Importance Weight" ],
    "Type": "std::vector<float>",
    "Description": "History of avg background batch importance weights."
  },
  {
    "Name": [ "Effective Sample Size" ],
    "Type": "std::vector<float>",
    "Description": "History of effective sample size."
  }
 ],

 "Module Defaults":
 {
   "Episodes Per Generation": 1,
   "Concurrent Workers": 1,
   "Discount Factor": 0.995,
   "Time Sequence Length": 1,
   "Importance Weight Truncation Level": 1.0,
   "Multi Agent Relationship": "Individual",
   "Multi Agent Correlation": false,
   "Multi Agent Sampling": "Tuple",
   "State Rescaling": 
   {
    "Enabled": false
   },
    
   "Feature Rescaling": 
   {
    "Enabled": false
   },
   
   "Reward":
   {
    "Rescaling": 
    {
     "Enabled": false
    }
   },

   "Mini Batch":
    {
     "Size": 256
    },
       
   "L2 Regularization": 
   {
     "Enabled": false,
     "Importance": 1e-4
   },
   
   "Training":
   {
    "Average Depth": 100,
    "Current Policies": { },
    "Best Policies": { }
   },

   "Testing":
   {
    "Sample Ids": [ ],
    "Current Policies": { },
    "Best Policies": { }
   },

   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Policy Updates": 0
   },

  "Experience Replay":
   {
    "Serialize": true,
    "Off Policy":
    {
     "Cutoff Scale": 4.0,
     "Target": 0.1,
     "REFER Beta": 0.3,
     "Annealing Rate": 0.0
    }
   },

   "Uniform Generator":
   {
    "Name": "Agent / Uniform Generator",
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   }
 },

 "Variable Defaults":
 {

 }

}

{

 "Configuration Settings":
 [
   {
    "Name": [ "Replay Memory", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Replay Memory", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Replay Memory", "Replacement Policy" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Least Recently Added", "Description": "New experiences will replace those added least recently." },
                { "Value": "Uniform", "Description": "After adding the new experiences to the replay memory, randomly select experiences to evict with a uniform distribution." }
               ],
    "Description": "Specified the replacement policy to be used when receiving new experiences."
  },
  {
    "Name": [ "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "The number of episodes from which to draw experiences at each generation."
  },
  {
    "Name": [ "Agent History Size" ],
    "Type": "size_t",
    "Description": "The number of most recent experiences returned by the actor."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Suboptimal Steps" ],
    "Type": "size_t",
    "Criteria": "(_maxSuboptimalSteps > 0) && (_suboptimalStepCounter >= _maxSuboptimalSteps)",
    "Description": "The solver will stop when the given number of optimization steps have been applied without an improvement between learner updates."
  },
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_maxExperiences > 0) && (_stateHistory.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Target Average Reward" ],
    "Type": "double",
    "Criteria": "(_targetAverageReward > -korali::Inf) && (_bestAverageReward >= _targetAverageReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  }
 ],

 "Variables Configuration":
 [
  {
   "Name": [ "Lower Bound" ],
   "Type": "double",
   "Description": "Lower bound for the variable's value."
  },
  {
   "Name": [ "Upper Bound" ],
   "Type": "double",
   "Description": "Upper bound for the variable's value."
  },
  {
    "Name": [ "Values" ],
    "Type": "std::vector<double>",
    "Description": "[Hint] Locations to evaluate the Objective Function."
  }
 ],

 "Internal Settings":
 [
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for the epsilon-greedy strategy."
  },
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
    "Name": [ "State History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of states."
  },
  {
    "Name": [ "Action History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of actions."
  },
  {
    "Name": [ "Reward History" ],
    "Type": "std::vector<double>",
    "Description": "Stores the whole history of rewards."
  },
  {
    "Name": [ "Next State History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of next states (the ones that came after the action from the previous state)."
  },
    {
   "Name": [ "Total Experience Count" ],
   "Type": "size_t",
   "Description": "Total number of experiences in the generation."
  },
  {
   "Name": [ "Min Experience Count" ],
   "Type": "size_t",
   "Description": "Minimum number of experiences among the actors."
  },
  {
   "Name": [ "Max Experience Count" ],
   "Type": "size_t",
   "Description": "Maximum number of experiences among the actors."
  },
  {
   "Name": [ "Average Experience Count" ],
   "Type": "double",
   "Description": "Average number of experiences among the actors."
  },
    {
    "Name": [ "Suboptimal Step Counter" ],
    "Type": "size_t",
    "Description": "Remembers the number of minibatch optimization steps have been performed without a perceived improvement in rewards."
  },
  {
    "Name": [ "Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found on this round of experiences."
  },
  {
    "Name": [ "Best Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found so far from the experiences in between learner updates."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the current policy configuration ."
  },
  {
   "Name": [ "Best Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the best policy configuration so far (the one that got the best reward)."
  }
 ],

 "Module Defaults":
 {
   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Suboptimal Steps": 0,
    "Max Optimization Steps": 0,
    "Target Average Reward": -Infinity 
   },
   
  "Replay Memory":
   {
    "Maximum Size": 10000,
    "Start Size": 1000,
    "Replacement Policy": "Least Recently Added"
   },
   
   "Uniform Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   },
   
   "Episodes Per Generation": 32,
   "Agent History Size": 4
 },
 
 "Variable Defaults":
 {
   "Lower Bound": -Infinity,
   "Upper Bound": Infinity,
   "Values": [ ]
  }
 
}

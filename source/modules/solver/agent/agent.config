{

 "Configuration Settings":
 [
  {
    "Name": [ "Replay Memory", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Replay Memory", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Critic", "Neural Network" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the underlying neural network to use for the critic."
  },
  {
    "Name": [ "Critic", "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Critic", "Optimizer" ],
    "Type": "knlohmann::json",
    "Description": "Represents the state and configuration of the solver algorithm for the weights and biases of the critic NN."
  },
  {
    "Name": [ "Critic", "Optimization Steps" ],
    "Type": "size_t",
    "Description": "The number of optimization steps on the training Q estimator to perform between episode executions, before updating the actual Q estimator."
  },
  {
    "Name": [ "Critic", "Update Algorithm" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Q-Learning", "Description": "The critic is updated following the standard Q learning formula: Q* = r + discount*V(s)." },
                { "Value": "Retrace", "Description": "Enables the use of the Retrace algorithm from Munos et al. (2016 - https://arxiv.org/pdf/1606.02647.pdf) for the estimation of Qpi." }
               ],
    "Description": "Specifies the algorithm to use for updates to the the critic."
  },
  {
   "Name": [ "Critic", "Discount Factor" ],
   "Type": "double",
   "Description": "Discount Factor for future states."
  },
  {
   "Name": [ "Critic", "Normalization Steps" ],
   "Type": "size_t",
   "Description": "How many mini-batches will be used to correct mean/variance bias for batch normalized layers."
  },
  {
    "Name": [ "Agent", "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "The number of episodes from which to draw experiences at each generation."
  },
  {
    "Name": [ "Agent", "Experience Limit" ],
    "Type": "size_t",
    "Description": "The number of most recent experiences returned by the actor."
  },
  {
   "Name": [ "Policy", "Epsilon", "Initial Value" ],
   "Type": "double",
   "Description": "Specifies the initial value for epsilon, the probability of not choosing the best possible action (exploitation) and using a random selection instead (exploration). e = 0 represents a pure greedy strategy, 0 < e < 1 represents the epsilon-greedy strategy, and e = 1 represents a purely random strategy."
  },
  {
   "Name": [ "Policy", "Epsilon", "Decrease Rate" ],
   "Type": "double",
   "Description": "Specifies the how much the value of epsilon should be decreased as generations progress. A rate d(e) > 0.0 represents the Epsilon-decreasing strategy."
  },
  {
   "Name": [ "Policy", "Epsilon", "Target Value" ],
   "Type": "double",
   "Description": "Specifies the last value of epsilon after which it will not be reduced any further."
  },
  {
   "Name": [ "Importance Weight", "Lambda" ],
   "Type": "double",
   "Description": "Multiplier of the importance factor."
  },
  {
   "Name": [ "Importance Weight", "Truncation" ],
   "Type": "double",
   "Description": "Maximum value (c) for the importance weight (to prevent it from exploding), before multiplying it by Lambda."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Suboptimal Steps" ],
    "Type": "size_t",
    "Criteria": "(_maxSuboptimalSteps > 0) && (_suboptimalStepCounter >= _maxSuboptimalSteps)",
    "Description": "The solver will stop when the given number of optimization steps have been applied without an improvement between learner updates."
  },
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_maxExperiences > 0) && (_stateHistory.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Target Average Reward" ],
    "Type": "double",
    "Criteria": "(_targetAverageReward > -korali::Inf) && (_bestAverageReward >= _targetAverageReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  },
  {
    "Name": [ "Max Optimization Steps" ],
    "Type": "size_t",
    "Criteria": "(_maxOptimizationSteps > 0) && (_optimizationStepCount >= _maxOptimizationSteps)",
    "Description": "The solver will stop when the given number of optimizations have been made to the learner."
  }
 ],

 "Variables Configuration":
 [
  {
   "Name": [ "Lower Bound" ],
   "Type": "double",
   "Description": "Lower bound for the variable's value."
  },
  {
   "Name": [ "Upper Bound" ],
   "Type": "double",
   "Description": "Upper bound for the variable's value."
  },
  {
    "Name": [ "Values" ],
    "Type": "std::vector<double>",
    "Description": "[Hint] Locations to evaluate the Objective Function."
  }
 ],

 "Internal Settings":
 [
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for the epsilon-greedy strategy."
  },
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
    "Name": [ "State History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of states."
  },
  {
    "Name": [ "Action History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of actions."
  },
  {
    "Name": [ "Reward History" ],
    "Type": "std::vector<double>",
    "Description": "Stores the whole history of rewards."
  },
  {
    "Name": [ "Terminal History" ],
    "Type": "std::vector<bool>",
    "Description": "Indicates whether the experience represents the terminal state of the episode."
  },
    {
   "Name": [ "Total Experience Count" ],
   "Type": "size_t",
   "Description": "Total number of experiences in the generation."
  },
  {
   "Name": [ "Min Experience Count" ],
   "Type": "size_t",
   "Description": "Minimum number of experiences among the actors."
  },
  {
   "Name": [ "Max Experience Count" ],
   "Type": "size_t",
   "Description": "Maximum number of experiences among the actors."
  },
  {
   "Name": [ "Average Experience Count" ],
   "Type": "double",
   "Description": "Average number of experiences among the actors."
  },
    {
    "Name": [ "Suboptimal Step Counter" ],
    "Type": "size_t",
    "Description": "Remembers the number of minibatch optimization steps have been performed without a perceived improvement in rewards."
  },
  {
    "Name": [ "Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found on this round of experiences."
  },
  {
    "Name": [ "Best Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found so far from the experiences in between learner updates."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the current policy configuration ."
  },
  {
   "Name": [ "Best Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the best policy configuration so far (the one that got the best reward)."
  },
  {
   "Name": [ "Policy", "Epsilon", "Current Value" ],
   "Type": "double",
   "Description": "Specifies the current value of epsilon."
  },
  {
   "Name": [ "Policy", "Epsilon", "Initialized" ],
   "Type": "bool",
   "Description": "Flag to check whether the initial value of epsilon has already been set."
  },
  {
   "Name": [ "Cumulative Q Star" ],
   "Type": "double",
   "Description": "Sum of E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Average Q Star" ],
   "Type": "double",
   "Description": "Average E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Optimization Step Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of optimization steps performed by the learner."
  }
 ],

 "Module Defaults":
 {
   "Critic": 
   {
     "Mini Batch Size": 32,
     "Optimization Steps": 10,
     "Normalization Steps": 32,
     "Neural Network":   { "Type": "Neural Network" },
     "Update Algorithm": "Q-Learning"
   },
   
   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Suboptimal Steps": 0,
    "Max Optimization Steps": 0,
    "Target Average Reward": -Infinity 
   },
   
  "Replay Memory":
   {
    "Maximum Size": 10000,
    "Start Size": 1000
   },
   
   "Uniform Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   },
   
   "Agent":
   {
     "Episodes Per Generation": 32,
     "Experience Limit": 4
   },
 
   "Policy":
   {
     "Epsilon":
     {
      "Initial Value": 0.0,
      "Decrease Rate": 0.000,
      "Target Value": 0.0
     }
   },
   
   "Importance Weight":
   {
     "Lambda": 1.0,
     "Truncation": 1.0
   }
 },
 
 "Variable Defaults":
 {
   "Lower Bound": -Infinity,
   "Upper Bound": Infinity,
   "Values": [ ]
 }
 
}

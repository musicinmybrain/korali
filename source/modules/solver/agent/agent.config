{

 "Configuration Settings":
 [
  {
    "Name": [ "Agent Count" ],
    "Type": "size_t", 
    "Description": "Indicates the number of concurrent agents collecting experiences."
  },
  {
    "Name": [ "Experiences Per Generation" ],
    "Type": "size_t", 
    "Description": "Indicates the how many policy updates to perform in a generation (checkpoints are generated between generations)."
  },
  {
    "Name": [ "Critic", "Neural Network" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the underlying neural network to use for the critic."
  },
  {
    "Name": [ "Critic", "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
   "Name": [ "Critic", "Optimizer" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Adam", "Description": "Uses the Adam optimizer." },
      { "Value": "AdaBelief", "Description": "Uses the AdaBelief optimizer." }
     ],
   "Description": "Determines which optimizer algorithm to use to update the critic."
  },
  {
    "Name": [ "Critic", "Learning Rate" ],
    "Type": "float",
    "Description": "The learning rate for the critic NN."
  },
  {
   "Name": [ "Critic", "Discount Factor" ],
   "Type": "float",
   "Description": "Discount Factor for future states."
  },
  {
    "Name": [ "Policy", "Neural Network" ],
    "Type": "knlohmann::json",
    "Description": "Indicates the configuration of the underlying neural network to use for the policy."
  },
  {
    "Name": [ "Policy", "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Policy", "Learning Rate" ],
    "Type": "float",
    "Description": "The learning rate for the policy NN."
  },
  {
   "Name": [ "Policy", "Optimizer" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Adam", "Description": "Uses the Adam optimizer." },
      { "Value": "AdaBelief", "Description": "Uses the AdaBelief optimizer." }
     ],
   "Description": "Determines which optimizer algorithm to use to update the policy."
  },
  {
    "Name": [ "Experience Replay", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Experience Replay", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Experience Replay", "Importance Weight Annealing Rate" ],
    "Type": "float",
    "Description": "Annealing factor for prioritized experience replay (1.0 full compensation, 0.0 uniform treatment)."
  },
  {
    "Name": [ "Experience Replay", "Priority Annealing Rate" ],
    "Type": "float",
    "Description": "Annealing rate for experience probability calculation (1.0 full rank based probabilities, 0.0 uniform probabilities)."
  },
  {
    "Name": [ "Experience Replay", "Serialization Frequency" ],
    "Type": "size_t",
    "Description": "The number of generations that pass between serializations of the experience replay (for checkpoint-resume purposes)."
  },
  {
    "Name": [ "Experiences Between Policy Updates" ],
    "Type": "size_t",
    "Description": "The number of experiences to receive before training/updating."
  },
  {
   "Name": [ "Random Action Probability", "Initial Value" ],
   "Type": "float",
   "Description": "Specifies the initial value for epsilon, the probability of not choosing the best possible action (exploitation) and using a random selection instead (exploration). e = 0 represents a pure greedy strategy, 0 < e < 1 represents the epsilon-greedy strategy, and e = 1 represents a purely random strategy."
  },
  {
   "Name": [ "Random Action Probability", "Decrease Rate" ],
   "Type": "float",
   "Description": "Specifies the how much the value of epsilon should be decreased as generations progress. A rate d(e) > 0.0 represents the Epsilon-decreasing strategy."
  },
  {
   "Name": [ "Random Action Probability", "Target Value" ],
   "Type": "float",
   "Description": "Specifies the last value of epsilon after which it will not be reduced any further."
  },
  {
   "Name": [ "Mini Batch Strategy" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Uniform", "Description": "Selects experiences from the replay memory with a randomly." },
      { "Value": "Prioritized", "Description": "Prioritizes experiences according to their rank. See  Prioritized Experience Replay by Schaul et al. (2015)." },
      { "Value": "Remember-and-Forget", "Description": "Selects experiences randomly, so long as their importance weight is above a given tolerance." }
     ],
   "Description": "Determines how to select experiences from the replay memory for mini batch creation."
  },  
  {
   "Name": [ "Cache Persistence" ],
   "Type": "size_t",
   "Description": "Indicates for how many policy updates will pre-calculated values be persist in the experience cache."
  }
 ],

 "Termination Criteria":
 [
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_maxExperiences > 0) && (_experienceReplay.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Target Average Testing Reward" ],
    "Type": "float",
    "Criteria": "(_targetAverageTestingReward > -korali::Inf) && (_bestAverageTestingReward >= _targetAverageTestingReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  },
  {
    "Name": [ "Max Policy Updates" ],
    "Type": "size_t",
    "Criteria": "(_maxPolicyUpdates > 0) && (_policyUpdateCount >= _maxPolicyUpdates)",
    "Description": "The solver will stop when the given number of optimizations have been made to the learner."
  }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
   "Name": [ "Last Training Reward" ],
   "Type": "float",
   "Description": "The cumulative training reward for the last episode received."
  },
  {
    "Name": [ "Average Testing Reward" ],
    "Type": "float",
    "Description": "Remembers the average reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Best Testing Reward" ],
    "Type": "float",
    "Description": "Remembers the best reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Worst Testing Reward" ],
    "Type": "float",
    "Description": "Remembers the worst reward from latest testing episodes, if any."
  },
  {
    "Name": [ "Best Training Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from training episodes."
  },
  {
    "Name": [ "Best Training Episode" ],
    "Type": "size_t",
    "Description": "Remembers the episode that obtained the maximum reward found so far during training."
  },
  {
    "Name": [ "Candidate Policies Tested" ],
    "Type": "size_t",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
    "Name": [ "Best Average Testing Reward" ],
    "Type": "float",
    "Description": "Remembers the cumulative average episode reward found so far from testing episodes."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the current policy configuration."
  },
  {
   "Name": [ "Best Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the candidate policy configuration that has produced the best results."
  },
  {
   "Name": [ "Training State" ],
   "Type": "knlohmann::json",
   "Description": "Stores the state of training networks, necessary to continue learning at a later point."
  },
  {
   "Name": [ "Cumulative Q Star" ],
   "Type": "float",
   "Description": "Sum of Q among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Cumulative Q Star Squared" ],
   "Type": "float",
   "Description": "Sum of Q square among the experiences from a minibatch."
  },
  {
   "Name": [ "Average Q Star" ],
   "Type": "float",
   "Description": "Average Q among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Stdev Q Star" ],
   "Type": "float",
   "Description": "Standard deviation Q among the experiences in a minibatch in this generation."
  },
  {
   "Name": [ "Average TD Error" ],
   "Type": "float",
   "Description": "Average TD error in minibatch."
  },
  {
   "Name": [ "Stdev TD Error" ],
   "Type": "float",
   "Description": "Standard deviation TD error in minibatch."
  },
  {
   "Name": [ "Policy Update Count" ],
   "Type": "size_t",
   "Description": "Keeps track of the number of policy updates performed."
  },
  {
   "Name": [ "Current Sample ID" ],
   "Type": "size_t",
   "Description": "Keeps track of the current Sample ID, to make sure no two equal sample IDs are produced and that this value can be used as random seed."
  },
  {
   "Name": [ "Random Action Probability", "Current Value" ],
   "Type": "float",
   "Description": "Specifies the current value of epsilon."
  },
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for epsilon-greedy strategy."
  },
  {
    "Name": [ "Experience Count" ],
    "Type": "size_t",
    "Description": "Takes count of the number of experiences received so far."
  },
  {
    "Name": [ "Experience Replay", "Max Priority" ],
    "Type": "size_t",
    "Description": "Indicates the maximum priority of any experience in the experience replay."
  },
  {
    "Name": [ "Experience Replay", "Database" ],
    "Type": "std::vector<knlohmann::json>",
    "Description": "Contains the serialized database for the entire experience replay."
  }
 ],

 "Module Defaults":
 {
   "Experiences Per Generation": 100,
   "Agent Count": 1, 
   
   "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Policy Updates": 0,
    "Target Average Testing Reward": -Infinity 
   },
   
  "Experience Replay":
   {
    "Maximum Size": 10000,
    "Start Size": 1000,
    "Priority Annealing Rate" : 1.0,
    "Importance Weight Annealing Rate" : 1.0,
    "Serialization Frequency": 100,
    "Database": [ ]
   },
   
   "Random Action Probability": 
   {
      "Initial Value": 0.0,
      "Decrease Rate": 0.0,
      "Target Value": 0.0
   },
   
   "Uniform Generator":
   {
    "Type": "Univariate/Uniform",
    "Minimum": 0.0,
    "Maximum": 1.0
   },
   
   "Critic": 
   {
     "Mini Batch Size": 32,
     "Neural Network":   { "Type": "Neural Network" },
     "Optimizer": "AdaBelief"
   },
   
   "Policy": 
   {
     "Learning Rate": 0.0001,
     "Mini Batch Size": 32,
     "Neural Network":   { "Type": "Neural Network" },
     "Optimizer": "AdaBelief"
   },
   
   "Mini Batch Strategy": "Uniform",
   "Cache Persistence": 0
 },
 
 "Variable Defaults":
 {
 
 }
 
}

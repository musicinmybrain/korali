#include "modules/solver/deepSupervisor/deepSupervisor.hpp"
#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"

void korali::solver::DeepSupervisor::runNeuralNetwork(korali::Sample& sample, korali::NeuralNetwork* nn, std::vector<std::vector<double>>* solution)
{
 // Setting weights and biases
 size_t currentVariable = 0;
 for (size_t i = 1; i < nn->_layers.size(); i++)
 {
   nn->_layers[i]->_weights.resize(nn->_layers[i]->_nodeCount);
   nn->_layers[i]->_bias.resize(nn->_layers[i]->_nodeCount);

   // Adding layer's weights
   for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
   {
    nn->_layers[i]->_weights[j].resize(nn->_layers[i-1]->_nodeCount);

    for (size_t k = 0; k < nn->_layers[i-1]->_nodeCount; k++)
     nn->_layers[i]->_weights[j][k] = sample["Parameters"][currentVariable++];
   }

   // Adding layer's biases
   for (size_t j = 0; j < nn->_layers[i]->_nodeCount; j++)
    nn->_layers[i]->_bias[j] = sample["Parameters"][currentVariable++];
 }

 // Updating the network's weights and biases
 nn->update();

 // Running the input values through the neural network
 nn->forward();

 // Printing Layer values
 size_t outputLayerId = nn->_layers.size()-1;
 size_t batchSize = nn->_layers[outputLayerId]->_nodeValues.size();
 size_t outputSize = nn->_layers[outputLayerId]->_nodeValues[0].size();

 // Calculating mean square error
 double meanSquaredError = 0.0;

 for (size_t i = 0; i < batchSize; i++)
  for (size_t j = 0; j < outputSize; j++)
  {
   double diff = nn->_layers[outputLayerId]->_nodeValues[i][j] - (*solution)[i][j];
   meanSquaredError += diff*diff;
  }

 meanSquaredError = meanSquaredError / (double) batchSize;

 // Saving the negative of the error because we want to minimize it
 sample["F(x)"] = -meanSquaredError;
}


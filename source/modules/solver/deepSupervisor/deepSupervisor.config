{

 "Module Data":
 {
   "Class Name": "DeepSupervisor",
   "Namespace": ["korali", "solver"],
   "Parent Class Name": "Solver"
 },

 "Available Operations":
 [
  {
    "Name": "Run Training On Worker",
    "Description": "Asks a Korali worker to run the forward and backward pipeline of the network given an input and return the hyperparameter gradients and their loss.",
    "Function": "runTrainingOnWorker"
  },
  {
    "Name": "Run Evaluation On Worker",
    "Description": "Asks a Korali worker to run the forward pipeline of the network given an input and return the output.",
    "Function": "runEvaluationOnWorker"
  }
 ],
 
 "Configuration Settings":
 [
  {
   "Name": [ "Mode" ],
   "Type": "std::string",
   "Options": [
               { "Value": "Training", "Description": "Trains a neural network for the supervised learning problem." },
               { "Value": "Testing", "Description": "Forwards the neural network given an input batch." }
              ],
   "Description": "Specifies the operation mode for the learner."
  },
  {
    "Name": [ "Neural Network", "Hidden Layers" ],
    "Type": "knlohmann::json",
    "Description": "Sets the configuration of the hidden layers for the neural network."
  },
  {
    "Name": [ "Neural Network", "Output Activation" ],
    "Type": "knlohmann::json",
    "Description": "Allows setting an aditional activation for the output layer."
  },
  {
    "Name": [ "Neural Network", "Output Layer" ],
    "Type": "knlohmann::json",
    "Description": "Sets any additional configuration (e.g., masks) for the output NN layer."
  },
  {
   "Name": [ "Neural Network", "Engine" ],
   "Type": "std::string",
   "Description": "Specifies which Neural Network backend engine to use."
  },
  {
   "Name": [ "Neural Network", "Optimizer" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Adam", "Description": "Uses the Adam algorithm." },
      { "Value": "AdaBelief", "Description": "Uses the AdaBelief algorithm." },
      { "Value": "MadGrad", "Description": "Uses the MadGrad algorithm." },
      { "Value": "AdaGrad", "Description": "Uses the AdaGrad algorithm." }
     ],
   "Description": "Determines which optimizer algorithm to use to apply the gradients on the neural network's hyperparameters."
  },
  {
   "Name": [ "Loss Function" ],
   "Type": "std::string",
   "Options": [
      { "Value": "Direct Gradient", "Description": "The given solution represents the gradients of the loss with respect to the network-output. Note that Korali uses the gradients to maximize the objective." },
      { "Value": "Mean Squared Error", "Description": "The loss is calculated as the negative mean of square errors, one per input in the batch. Note that Korali maximizes the negative MSE." }
     ],
   "Description": "Function to calculate the difference (loss) between the NN inference and the exact solution and its gradients for optimization."
  },
  {
   "Name": [ "Learning Rate" ],
   "Type": "float",
   "Description": "Learning rate for the underlying ADAM optimizer."
  },
  {
   "Name": [ "L2 Regularization", "Enabled" ],
   "Type": "bool",
   "Description": "Regulates if l2 regularization will be applied to the neural network."
  },
  {
   "Name": [ "L2 Regularization", "Importance" ],
   "Type": "bool",
   "Description": "Importance weight of l2 regularization."
  },
  {
   "Name": [ "Output Weights Scaling" ],
   "Type": "float",
   "Description": "Specified by how much will the weights of the last linear transformation of the NN be scaled. A value of < 1.0 is useful for a more deterministic start."
  },
  {
   "Name": [ "Batch Concurrency" ],
   "Type": "size_t",
   "Description": "Specifies in how many parts will the mini batch be split for concurrent processing. It must divide the training mini batch size perfectly."
  }
 ],

 "Results":
 [

 ],

 "Termination Criteria":
 [
   {
    "Name": [ "Target Loss" ],
    "Type": "float",
    "Criteria": "(_k->_currentGeneration > 1) && (_targetLoss > 0.0) && (_currentLoss <= _targetLoss)",
    "Description": "Specifies the maximum number of suboptimal generations."
   }
 ],

 "Variables Configuration":
 [

 ],

 "Internal Settings":
 [
  {
   "Name": [ "Evaluation" ],
   "Type": "std::vector<std::vector<float>>",
   "Description": "The output of the neural network if running on testing mode."
  },
  {
   "Name": [ "Current Loss" ],
   "Type": "float",
   "Description": "Current value of the loss function."
  },
  {
   "Name": [ "Loss History" ],
   "Type": "std::vector<float>",
   "Description": "Current value of the loss function."
  },
  {
   "Name": [ "Normalization Means" ],
   "Type": "std::vector<float>",
   "Description": "Stores the current neural network normalization mean parameters."
  },
  {
   "Name": [ "Normalization Variances" ],
   "Type": "std::vector<float>",
   "Description": "Stores the current neural network normalization variance parameters."
  },
  {
   "Name": [ "Optimizer" ],
   "Type": "korali::fGradientBasedOptimizer*",
   "Description": "Stores a pointer to the optimizer."
  }
 ],

 "Module Defaults":
 {
  "L2 Regularization": 
   {
     "Enabled": false,
     "Importance": 1e-4
   },
  
  "Neural Network": 
  {
   "Output Activation": "Identity",
   "Output Layer": { }
  },
  "Termination Criteria":
   { 
     "Target Loss": -1.0
   },
  "Hyperparameters": [],
  "Output Weights Scaling": 1.0,
  "Batch Concurrency": 1
 },

 "Variable Defaults":
 {

 }
}

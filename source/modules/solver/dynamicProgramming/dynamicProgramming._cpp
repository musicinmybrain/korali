#include "modules/solver/dynamicProgramming/dynamicProgramming.hpp"
#include "modules/conduit/conduit.hpp"

void korali::solver::DynamicProgramming::initialize()
{
 // Get problem pointer
 _problem = dynamic_cast<korali::problem::ReinforcementLearning*>(_k->_problem);

 // Calculating number of table entries
 _tableEntryCount = _problem->_actionCount * _problem->_stateCount;

 for (size_t i = 0; i < _k->_variables.size(); i++)
  if (_k->_variables[i]->_parameterSpace == "Continuous")
   _k->_logger->logError("Variable %lu (%s) is continuous, but dynamic programming can only process discrete or custom defined parameter spaces.\n", i, _k->_variables[i]->_name.c_str());

 // For training, We run as many evaluations as needed if no other termination criteria has been defined
 if (_maxEvaluations == 0) _maxEvaluations = _tableEntryCount;

 // For Running, We run as many stages as necessary
 if (_maxStages == 0) _maxStages = _recursionDepth;
}

void korali::solver::DynamicProgramming::runGeneration()
{
 if (_problem->_operation == "Training") doTraining();
 if (_problem->_operation == "Running") doRunning();
}

void korali::solver::DynamicProgramming::doTraining()
{
 // If initial reserve storage and initialize state/action tables
 if (_k->_currentGeneration == 1)
 {
  // Reserving storage and initializing state-action -> reward, state tables
  _rewardTable.resize(_tableEntryCount);
  _stateTable.resize(_tableEntryCount);

  for (size_t i = 0; i < _rewardTable.size(); i++) _rewardTable[i] = -korali::Inf;
  for (size_t i = 0; i < _stateTable.size(); i++) _stateTable[i] = 0;

  // Initalizing evaluation and stage counter
  _evaluationCount = 0;
 }

 // Creating korali sample to run agent
 korali::Sample agent;
 agent["Module"] = "Problem";
 agent["Operation"] = "Run Environment";

 std::vector<double> state(_problem->_stateVectorSize);
 std::vector<double> action(_problem->_actionVectorSize);

 // Computing State/Action Table
 for (size_t i = 0; i < _problem->_stateCount; i++)
 for (size_t j = 0; j < _problem->_actionCount; j++)
 {
  agent["Sample Id"] = _evaluationCount;
  agent["State"]  = _problem->getStateFromIndex(i);
  agent["Action"] = _problem->getActionFromIndex(j);

  _conduit->start(agent);
  _conduit->wait(agent);

  double reward = agent["Reward"];
  std::vector<double> newState = agent["State"];
  size_t newStateIdx = _problem->getIndexFromState(newState);

  _rewardTable[_evaluationCount] = reward;
  _stateTable[_evaluationCount] = newStateIdx;
  _evaluationCount++;
 }
}

void korali::solver::DynamicProgramming::doRunning()
{
 if (_k->_currentGeneration == 1)
 {
  // If first generation, then update the reward table
  _recursiveRewardTable = _rewardTable;

  for (size_t depth = 1; depth < _recursionDepth; depth++)
  {
   auto tmpRewardTable = _recursiveRewardTable;

   for (size_t i = 0; i < _problem->_stateCount; i++)
   for (size_t j = 0; j < _problem->_actionCount; j++)
   {
    size_t currentTableIdx = i*_problem->_actionCount + j;
    size_t newState = _stateTable[currentTableIdx];

    double bestReward = -korali::Inf;
    for (size_t k = 0; k < _problem->_actionCount; k++)
    {
     size_t candidateTableIdx = newState*_problem->_actionCount + k;
     double candidateReward = _recursiveRewardTable[candidateTableIdx];
     if (candidateReward > bestReward) bestReward = candidateReward;
    }

    tmpRewardTable[currentTableIdx] = _rewardTable[currentTableIdx] + bestReward;
   }

   _recursiveRewardTable = tmpRewardTable;
  }

  // Setting Initial State
  _currentState = _problem->_initialState;
  _bestPolicyStates.push_back(_currentState);
  _bestPolicyReward = 0;
 }

 size_t currentStage = _k->_currentGeneration -1;

 korali::Sample agent;
 agent["Module"] = "Problem";
 agent["Operation"] = "Run Environment";
 agent["Sample Id"] = currentStage;
 agent["State"]  = _currentState;

 // Looking for ideal action given the current state
 size_t stateIdx = _problem->getIndexFromState(_currentState);
 size_t actionIdx = 0;
 double bestReward = -korali::Inf;

 for (size_t i = 0; i < _problem->_actionCount; i++)
 {
  size_t tableIdx = stateIdx * _problem->_actionCount + i;
  if (_recursiveRewardTable[tableIdx] > bestReward)
  {
    bestReward = _recursiveRewardTable[tableIdx];
    actionIdx = i;
  }
 }

 auto action = _problem->getActionFromIndex(actionIdx);

 agent["Action"] = action;

 _conduit->start(agent);
 _conduit->wait(agent);

 _currentState = agent["State"].get<std::vector<double>>();
 double currentReward = agent["Reward"];

 // Adding best rewards so far
 _bestPolicyReward += currentReward;
 _bestPolicyStates.push_back(_currentState);
 _bestPolicyActions.push_back(action);

 _k->_js["Results"]["Optimal Policy Actions"] = _bestPolicyActions;
 _k->_js["Results"]["Optimal Policy States"] = _bestPolicyStates;
 _k->_js["Results"]["Optimal Policy Reward"] = _bestPolicyReward;
}

void korali::solver::DynamicProgramming::printGenerationBefore()
{

}

void korali::solver::DynamicProgramming::printGenerationAfter()
{

}




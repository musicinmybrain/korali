#include "modules/conduit/conduit.hpp"
#include "modules/solver/DQN/DQN.hpp"

namespace korali
{
namespace solver
{

void DQN::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  _QExperiment["Problem"]["Type"] = "Supervised Learning";
  _QExperiment["Solver"]["Type"] = "Deep Learner";
  _neuralNetwork->getConfiguration(_QExperiment["Solver"]["Neural Network"]);

  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
   _QExperiment["Variables"][i]["Name"] = _k->_variables[i]->_name;
   _QExperiment["Variables"][i]["Training Data"] = { 0.0 };
   _QExperiment["Variables"][i]["Validation Data"] = { 0.0 };
   if (_k->_variables[i]->_type == "State") _QExperiment["Variables"][i]["Type"] = "Input";
   if (_k->_variables[i]->_type == "Action") _QExperiment["Variables"][i]["Type"] = "Output";
  }

  _QExperiment.initialize();
}

void DQN::setInitialConfiguration()
{
 // Starting to process initial states, from the first one
 _currentInitialState = 0;
}

void DQN::runGeneration()
{
 // If initial generation, set initial DQN configuration
  if (_k->_currentGeneration == 1)
   setInitialConfiguration();

  // Creating as many agents as initial states defined.
  size_t agentCount = 1;

  // Making sure we don't create more agents than remaining initial states
  if (agentCount + _currentInitialState > _problem->_initialStateCount)
   agentCount = _problem->_initialStateCount - _currentInitialState;

  // Creating storage for agents
  std::vector<Sample> agents(agentCount);

  // Index for current worker Id that helps making a balanced distribution of work
  size_t currentWorker = 0;

  // Storage for agent information
  std::vector<std::vector<double>> actions(agentCount);
  std::vector<std::vector<double>> states(agentCount);
  std::vector<double> rewards(agentCount);
  std::vector<bool> hasFinished(agentCount);

  // Storage for the entire history of the current policy
  std::vector<std::vector<std::vector<double>>> actionHistories(agentCount);
  std::vector<std::vector<std::vector<double>>> stateHistories(agentCount);

  // Initializing Finished count
  size_t finishedCount = 0;

  // Setting initial states
  for (size_t i = 0; i < agentCount; i++)
   states[i] = _problem->_initialStates[_currentInitialState++];

  // Setting initial actions
  actions = _QExperiment._solver->evaluate(states);

  // Initializing the agents and their environments
  for (size_t i = 0; i < agentCount; i++)
  {
    // Configuring Agent
    agents[i]["Worker"] = 0;
    agents[i]["Sample Id"] = i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";
    agents[i]["State"] = states[i];
    agents[i]["Action"] = actions[i];

    // Launching agent initialization
    _conduit->start(agents[i]);

    // Advancing to the next worker (if reached the last, start from beginning)
    currentWorker++;
    if (currentWorker == _conduit->maxConcurrency()) currentWorker = 0;
  }

  // Running main iteration loop until all agents have finished
  while (finishedCount < agentCount)
  {
    // Waiting for any of the pending agents
    size_t i = _conduit->waitAny(agents);

    // Add state to current policy history
    actionHistories[i].push_back(actions[i]);
    stateHistories[i].push_back(states[i]);

    // Updating reward
    rewards[i] = KORALI_GET(double, agents[i], "Reward");

    // Storing agent's new state
    auto newState = KORALI_GET(std::vector<double>, agents[i], "State");

    // The new state becomes the current state
    states[i] = newState;

    hasFinished[i] = KORALI_GET(bool, agents[i], "Finished");

    // If it finished, then add to the count
    if (hasFinished[i])
    {
      // Incrementing finished agent count
      finishedCount++;
    }
    else // Else, get new action and run next episode
    {
      // Getting new action to perform
      actions[i] = _QExperiment._solver->evaluate( { newState } )[0];
      agents[i]["Action"] = actions[i];

      // Run Episode
      _conduit->start(agents[i]);
    }
  }

  // Updating the NN with the gathered results
  for (size_t i = 0; i < agentCount; i++)
  {
   printf("Agent %lu\n", i, stateHistories[i].size(), actionHistories[i].size());
   for (size_t j = 0; j < stateHistories[i].size(); j++)
   {
    printf(" + State  %.3lu: ", j);
    for (size_t k = 0; k < stateHistories[i][j].size(); k++) printf("%f ", stateHistories[i][j][k]);
    printf("\n");

    printf(" + Action %.3lu: ", j);
    for (size_t k = 0; k < stateHistories[i][j].size(); k++) printf("%f ", stateHistories[i][j][k]);
    printf("\n");
   }
  }

  exit(0);
}

} // namespace solver

} // namespace korali

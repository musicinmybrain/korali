#include "modules/conduit/conduit.hpp"
#include "modules/solver/DQN/DQN.hpp"

namespace korali
{
namespace solver
{

void DQN::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Checking inputs
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   if (std::isfinite(_k->_variables[varIdx]->_lowerBound) == false)
    KORALI_LOG_ERROR("Lower bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_lowerBound);

   if (std::isfinite(_k->_variables[varIdx]->_upperBound) == false)
    KORALI_LOG_ERROR("Upper bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_upperBound);

   if (_k->_variables[varIdx]->_lowerBound >= _k->_variables[varIdx]->_upperBound)
    KORALI_LOG_ERROR("Lower Bound (%f) for action variable %lu must be strictly smaller than its upper bound (%f).\n", _k->_variables[varIdx]->_lowerBound, i, _k->_variables[varIdx]->_upperBound);
  }

  /*********************************************************************
  * Creating and running Q-Approximation Experiment
  *********************************************************************/

  _QExperiment["Problem"]["Type"] = "Supervised Learning";
  _QExperiment["Solver"]["Type"] = "Learner/Deep Learner";
  _neuralNetwork->getConfiguration(_QExperiment["Solver"]["Neural Network"]);

  _QExperiment["Solver"]["Optimizer"] = _weightOptimizer;
  _QExperiment["Solver"]["Steps Per Generation"] = 10;
  _QExperiment["Solver"]["Termination Criteria"]["Max Inactive Steps"] = 1000;

  _QExperiment["Console Output"]["Frequency"] = 1000;
  //_QExperiment["Console Output"]["Verbosity"] = "Silent";
  _QExperiment["File Output"]["Enabled"] = false;

  // Initializing experiment with an empty set
  size_t expVar = 0;
  for (size_t i = 0; i < _problem->_stateVectorSize; i++)
  {
   _QExperiment["Problem"]["Training"]["Input"][0][expVar] = 1.0;
   _QExperiment["Problem"]["Validation"]["Input"][0][expVar] = 1.0;
   expVar++;
  }

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   _QExperiment["Problem"]["Training"]["Input"][0][expVar] = 1.0;
   _QExperiment["Problem"]["Validation"]["Input"][0][expVar] = 1.0;
   expVar++;
  }

  _QExperiment["Problem"]["Training"]["Solution"][0][0] = 0.0;
  _QExperiment["Problem"]["Validation"]["Solution"][0][0] = 0.0;

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_QExperiment);

  /*********************************************************************
  * Creating Q-Maximizing argmax(Action) Experiment
  *********************************************************************/

  // Creating evaluation lambda function for optimization
  auto fc = [this](Sample &sample) { this->evaluateAction(sample, true); };

  _AExperiment["Problem"]["Type"] = "Optimization";
  _AExperiment["Problem"]["Objective Function"] = fc;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   _AExperiment["Variables"][i]["Name"] = _k->_variables[varIdx]->_name;
   _AExperiment["Variables"][i]["Lower Bound"] = _k->_variables[varIdx]->_lowerBound;
   _AExperiment["Variables"][i]["Upper Bound"] = _k->_variables[varIdx]->_upperBound;

   double initialGuess = (_k->_variables[varIdx]->_upperBound + _k->_variables[varIdx]->_lowerBound) * 0.5;
   double initialStdDev = abs((_k->_variables[varIdx]->_upperBound - _k->_variables[varIdx]->_lowerBound)) * 0.0001;

   _AExperiment["Variables"][i]["Initial Value"] = initialGuess;
   _AExperiment["Variables"][i]["Initial Mean"] = initialGuess;
   _AExperiment["Variables"][i]["Initial Standard Deviation"] = initialStdDev;
  }

  _AExperiment["Solver"] = _actionOptimizer;

  _AExperiment["Console Output"]["Frequency"] = 0;
  _AExperiment["File Output"]["Enabled"] = false;
  _AExperiment["Console Output"]["Verbosity"] = "Silent";

  _AExperiment["Random Seed"] = _k->_randomSeed++;

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_AExperiment);

  // Setting termination criterion
  if (_maxEpisodes > _problem->_initialStateCount || _maxEpisodes == 0)
   _maxEpisodes = _problem->_initialStateCount;
}

void DQN::setInitialConfiguration()
{
 // Starting to process initial states, from the first one
 _currentEpisode = 0;

 // Keeps track of how many states have been processed so far
 _stateCount = 0;
}

void DQN::runGeneration()
{
 // If initial generation, set initial DQN configuration
  if (_k->_currentGeneration == 1)
   setInitialConfiguration();

  // Making sure we don't create more agents than remaining initial states
  if (_agentCount + _currentEpisode > _maxEpisodes)
   _agentCount = _maxEpisodes - _currentEpisode;

  // Creating storage for agents
  std::vector<Sample> agents(_agentCount);

  // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_QExperiment._solver);

  // Broadcasting learner's hyperparameters
  _conduit->broadcastGlobals(learner->getHyperparameters());

  // Initializing the agents and their environments
  for (size_t i = 0; i < _agentCount; i++)
  {
    // Configuring Agent
    agents[i]["Sample Id"] = i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";
    agents[i]["State"] = _problem->_initialStates[_currentEpisode++];

    // Launching agent initialization
    _conduit->start(agents[i]);
  }

  _conduit->waitAll(agents);

//  /*********************************************************************
//  * Running Q-Approximation Experiment
//  *********************************************************************/
//
//  // Getting number of episodes for training
//  size_t trainingEpisodes = floor((double) _stateHistory.size() * (1 - _validationRatio));
//  size_t validationEpisodes = _stateHistory.size() - trainingEpisodes;
//
//  if (trainingEpisodes == 0)
//   KORALI_LOG_ERROR("Not enough training episodes available for training.\n");
//
//  if (validationEpisodes == 0)
//   KORALI_LOG_ERROR("Not enough validation episodes available for training.\n");
//
//  for (size_t i = 0; i < trainingEpisodes; i++)
//  {
//   size_t expVar = 0;
//   for (size_t j = 0; j < _problem->_stateVectorSize; j++)
//   {
//    _QExperiment["Problem"]["Training"]["Input"][i][expVar] = _stateHistory[i][j];
//    expVar++;
//   }
//
//   for (size_t j = 0; j < _problem->_actionVectorSize; j++)
//   {
//    _QExperiment["Problem"]["Training"]["Input"][i][expVar] = _actionHistory[i][j];
//    expVar++;
//   }
//
//   _QExperiment["Problem"]["Training"]["Solution"][i][0] = _rewardHistory[i];
//  }
//
//  // Setting validation episodes
//  for (size_t i = 0; i < validationEpisodes; i++)
//  {
//   size_t expVar = 0;
//   auto epIdx = trainingEpisodes + i;
//
//   for (size_t j = 0; j < _problem->_stateVectorSize; j++)
//   {
//    _QExperiment["Problem"]["Validation"]["Input"][i][expVar] = _stateHistory[epIdx][j];
//    expVar++;
//   }
//
//   for (size_t j = 0; j < _problem->_actionVectorSize; j++)
//   {
//    _QExperiment["Problem"]["Validation"]["Input"][i][expVar] = _actionHistory[epIdx][j];
//    expVar++;
//   }
//
//   _QExperiment["Problem"]["Validation"]["Solution"][i][0] = _rewardHistory[epIdx];
//  }
//
//  //printf("%s\n", _QExperiment["Problem"].dump(2).c_str());
//
//  _engine.run(_QExperiment);
//
//
//  // Increasing state count
//  _stateCount = _stateHistory.size();
}

void DQN::getAction(Sample &sample)
{
 // Getting p = U[0,1] for the epsilon strategy
  double p = _uniformGenerator->getRandomNumber();

  // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
  // If no states have been registered yet, use the random option in any case
  if (p < _epsilon)
  {
   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    double lowerBound = _k->_variables[varIdx]->_lowerBound;
    double upperBound = _k->_variables[varIdx]->_upperBound;
    double x = _uniformGenerator->getRandomNumber();
    sample["Action"][i] = lowerBound + x*(upperBound - lowerBound);
   }

   return;
  }

  // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_QExperiment._solver);

  // Updating learner with the latest hyperparameters
  learner->setHyperparameters(sample.globals());

  // Updating current state
  _currentState = KORALI_GET(std::vector<double>, sample, "State");

  // Running optimization experiment to get best estimated action
  korali::Engine engine;
  engine.run(_AExperiment);

  // Getting optimal action, based on the NN evaluation
  sample["Action"] = _AExperiment["Results"]["Best Sample"]["Parameters"];
}

void DQN::evaluateAction(Sample &sample, bool getGradients)
{
  // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_QExperiment._solver);

  // Setting weights and biases
  auto action = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Creating input batch
  std::vector<std::vector<double>> inputBatch(1);

  // Putting together state and action
  inputBatch[0].resize( _currentState.size() + action.size() );
  size_t pos = 0;
  for (size_t i = 0; i < _currentState.size(); i++) inputBatch[0][pos++]  = _currentState[i];
  for (size_t i = 0; i < action.size(); i++) inputBatch[0][pos++] = action[i];

  // Getting function valuation
  auto evaluation = learner->evaluate(inputBatch);
  auto gradients = learner->getGradients(evaluation);

  sample["F(x)"] = evaluation[0][0];

  for (size_t i = 0; i < action.size(); i++)
  {
    size_t startIdx = _currentState.size();
    sample["Gradient"][i] = gradients[0][startIdx + i];
  }

//  printf("%s\n", sample._js.getJson().dump(2).c_str());
}

} // namespace solver

} // namespace korali

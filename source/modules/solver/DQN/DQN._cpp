#include "modules/conduit/conduit.hpp"
#include "modules/solver/DQN/DQN.hpp"

/****************************************
 * Things to fix:
 * - All samples use their own pseudorandom generators
 * - All seeds must be replicable when restarting a job
 *****************************************/

namespace korali
{
namespace solver
{

void DQN::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Checking inputs
  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   // If value vector has not been provided, check for bound correctness.
   if (_k->_variables[varIdx]->_values.size() == 0)
   {
    if (std::isfinite(_k->_variables[varIdx]->_lowerBound) == false)
     KORALI_LOG_ERROR("Lower bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_lowerBound);

    if (std::isfinite(_k->_variables[varIdx]->_upperBound) == false)
     KORALI_LOG_ERROR("Upper bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_upperBound);

    if (_k->_variables[varIdx]->_lowerBound >= _k->_variables[varIdx]->_upperBound)
     KORALI_LOG_ERROR("Lower Bound (%f) for action variable %lu must be strictly smaller than its upper bound (%f).\n", _k->_variables[varIdx]->_lowerBound, i, _k->_variables[varIdx]->_upperBound);
   }
  }

  /*********************************************************************
  * Creating and running Q-Approximation Experiment
  *********************************************************************/

  _qTrainingExperiment["Problem"]["Type"] = "Supervised Learning";
  _qTrainingExperiment["Solver"]["Type"] = "Learner/Deep Learner";
  _neuralNetwork->getConfiguration(_qTrainingExperiment["Solver"]["Neural Network"]);

  _qTrainingExperiment["Solver"]["Optimizer"] = _weightOptimizer;
  _qTrainingExperiment["Solver"]["Steps Per Generation"] = 1;

  _qTrainingExperiment["Console Output"]["Frequency"] = 0;
  _qTrainingExperiment["Console Output"]["Verbosity"] = "Silent";
  _qTrainingExperiment["File Output"]["Enabled"] = false;
  _qTrainingExperiment["Random Seed"] = _k->_randomSeed++;

  // Initializing experiment with an initial zero set
  for (size_t i = 0; i < _miniBatchSize; i++)
  {
   _qTrainingExperiment["Problem"]["Training"]["Solution"][i][0] = 0.0;

   for (size_t j = 0; j < _k->_variables.size(); j++)
    _qTrainingExperiment["Problem"]["Training"]["Input"][i][j] = 0.0;
  }

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_qTrainingExperiment);

  // Replicating experiment for training experiment
  knlohmann::json qConfig;
  _qTrainingExperiment.getConfiguration(qConfig);
  _qInferenceExperiment._js.getJson() = qConfig;
  _engine.initialize(_qInferenceExperiment);

  // Getting learner pointers
  _qTrainingProblem = dynamic_cast<problem::SupervisedLearning *>(_qTrainingExperiment._problem);
  _qTrainingLearner = dynamic_cast<solver::Learner *>(_qTrainingExperiment._solver);
  _qInferenceLearner = dynamic_cast<solver::Learner *>(_qInferenceExperiment._solver);

  /*********************************************************************
  * Creating Q-Maximizing argmax(Action) Experiment
  *********************************************************************/

  // Creating evaluation lambda function for optimization
  auto fc = [this](Sample &sample) { this->evaluateAction(sample, true); };

  _argMaxActionExperiment["Problem"]["Type"] = "Optimization";
  _argMaxActionExperiment["Problem"]["Objective Function"] = fc;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   _argMaxActionExperiment["Variables"][i]["Name"] = _k->_variables[varIdx]->_name;
   _argMaxActionExperiment["Variables"][i]["Lower Bound"] = _k->_variables[varIdx]->_lowerBound;
   _argMaxActionExperiment["Variables"][i]["Upper Bound"] = _k->_variables[varIdx]->_upperBound;

   double initialGuess = (_k->_variables[varIdx]->_upperBound + _k->_variables[varIdx]->_lowerBound) * 0.5;
   double initialStdDev = abs((_k->_variables[varIdx]->_upperBound - _k->_variables[varIdx]->_lowerBound));

   _argMaxActionExperiment["Variables"][i]["Initial Value"] = initialGuess;
   _argMaxActionExperiment["Variables"][i]["Initial Mean"] = initialGuess;
   _argMaxActionExperiment["Variables"][i]["Initial Standard Deviation"] = initialStdDev;
   _argMaxActionExperiment["Variables"][i]["Values"] = _k->_variables[varIdx]->_values;
  }

  _argMaxActionExperiment["Solver"] = _actionOptimizer;

  _argMaxActionExperiment["Console Output"]["Frequency"] = 0;
  _argMaxActionExperiment["Console Output"]["Verbosity"] = "Silent";
  _argMaxActionExperiment["File Output"]["Enabled"] = false;
  _argMaxActionExperiment["Random Seed"] = _k->_randomSeed++;

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_argMaxActionExperiment);

  // If initial generation, set initial DQN configuration
  if (_k->_currentGeneration == 0)
  {
   _currentEpisode = 0;
   _epsilonCurrentValue = _epsilonInitialValue;
   _suboptimalStepCounter = 0;

    // Initialize best reward
   _bestAverageReward = -korali::Inf;

    // Get the initial set of hyperparameters
   _hyperparameters =  _qTrainingLearner->getHyperparameters();
   _bestHyperparameters = _hyperparameters;
  }

  // Assigning training hyperparameters to inference learner
  _qInferenceLearner->setHyperparameters(_hyperparameters);
}

void DQN::runGeneration()
{
  // Making sure we don't create more agents than remaining initial states
  size_t curEpisodeCount = _episodesPerGeneration;
  if (curEpisodeCount + _currentEpisode > _maxEpisodes) if (_maxEpisodes > 0)
   curEpisodeCount = _maxEpisodes - _currentEpisode;

  // Creating storage for agents
  std::vector<Sample> agents(curEpisodeCount);

  // Setting global configuration for workers to use
  knlohmann::json globals;

  // Getting q training hyperparameters to broadcast to workers
  _hyperparameters =  _qTrainingLearner->getHyperparameters();
  globals["Hyperparameters"] = _hyperparameters;
  _qInferenceLearner->setHyperparameters(_hyperparameters);

  // If the minimum number of experiences haven been reached, do not update NN
  // and use epsilon = 1.0 to use only random decisions.
  if (_stateHistory.size() < _replayMemoryStartSize)
   globals["Epsilon"] = 1.0;
  else
   globals["Epsilon"] = _epsilonCurrentValue;

  // Broadcasting updated globals for all workers to have
  _conduit->broadcastGlobals(globals);

  // Initializing the agents and their environments
  for (size_t i = 0; i < curEpisodeCount; i++)
  {
    // Configuring Agent
    agents[i]["Sample Id"] = i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";

    // Launching agent initialization
    _conduit->start(agents[i]);
  }

  _conduit->waitAll(agents);

  /*********************************************************************
   * Storing new experiences to the history
   *********************************************************************/

  _totalExperienceCount = 0;
  _maxExperienceCount = 0;
  _minExperienceCount = std::numeric_limits<size_t>::max();

  // Calculating the cumulative reward of this round of experiences
  double cumulativeReward = 0.0;

  for (size_t i = 0; i < curEpisodeCount; i++)
  {
   size_t experienceSize = agents[i]["Experience"]["States"].size();

   // Storing statistics
   _totalExperienceCount += experienceSize;
   if (experienceSize > _maxExperienceCount) _maxExperienceCount = experienceSize;
   if (experienceSize < _minExperienceCount) _minExperienceCount = experienceSize;

   size_t startExpId = experienceSize > _agentHistorySize ? experienceSize - _agentHistorySize : 0;

   for (size_t j = startExpId; j < experienceSize; j++)
   {
    // Storing action and state experience
    _stateHistory.push_back(agents[i]["Experience"]["States"][j].get<std::vector<double>>());
    _actionHistory.push_back(agents[i]["Experience"]["Actions"][j].get<std::vector<double>>());

    // Storing reward
    double reward = agents[i]["Experience"]["Rewards"][j].get<double>();
    _rewardHistory.push_back(reward);
    cumulativeReward += reward;

    // If not a terminal state, store the next state
    if (j < agents[i]["Experience"]["States"].size() - 1)
     _nextStateHistory.push_back(agents[i]["Experience"]["States"][j+1].get<std::vector<double>>());
    else // Otherwise, store an empty next state
     _nextStateHistory.push_back({ });
   }
  }

  // Updating best reward so far, it the cumulative reward of this round exceeds it
  // and store the best hyperparameters used for that
  _averageReward = cumulativeReward / (double) curEpisodeCount;
  if (_averageReward > _bestAverageReward)
  {
   _bestAverageReward = _averageReward;
   _bestHyperparameters = _hyperparameters;
   _suboptimalStepCounter = 0;
  }
  else
  {
   // If not better, then increase the suboptimal step counter
   _suboptimalStepCounter++;
  }

  // Updating average experience count
  _averageExperienceCount = ((double)_totalExperienceCount) / ((double)curEpisodeCount);

  // If the maximum number of experiences have been reached, start forgetting excess experiences
  if (_stateHistory.size() > _replayMemoryMaximumSize)
  {
   size_t excess = _stateHistory.size() - _replayMemoryMaximumSize;

   if (_replayMemoryReplacementPolicy == "Least Recently Added")
   {
    _stateHistory.erase(_stateHistory.begin(), _stateHistory.begin()+excess);
    _actionHistory.erase(_actionHistory.begin(), _actionHistory.begin()+excess);
    _rewardHistory.erase(_rewardHistory.begin(), _rewardHistory.begin()+excess);
    _nextStateHistory.erase(_nextStateHistory.begin(), _nextStateHistory.begin()+excess);
   }

   if (_replayMemoryReplacementPolicy == "Uniform")
   {
    for (size_t i = 0; i < excess; i++)
    {
     double x = _uniformGenerator->getRandomNumber();
     size_t expId = floor(x * _stateHistory.size());

     _stateHistory.erase(_stateHistory.begin() + expId);
     _actionHistory.erase(_actionHistory.begin() + expId);
     _rewardHistory.erase(_rewardHistory.begin() + expId);
     _nextStateHistory.erase(_nextStateHistory.begin() + expId);
    }
   }

  }

  // If the minimum number of experiences have been reached, do not do any training
  if (_stateHistory.size() < _replayMemoryStartSize) return;

  /***********************************************************************************
   * Randomly selecting experiences for the mini-batch and calculating their target Q
   ***********************************************************************************/

  _cumulativeQStar = 0.0;

  for (size_t step = 0; step < _optimizationStepsPerUpdate; step++)
  {
   // Calculating target Q value (solution) for Qnew on selected batch
   std::vector<double> qValue(_miniBatchSize);

   for (size_t i = 0; i < _miniBatchSize; i++)
   {
    double x = _uniformGenerator->getRandomNumber();
    size_t expId = floor(x * _stateHistory.size());

    // Qnew = max_a(q) with s' fixed
    // Q* = r + y*Qnew -- If not terminal state
    // Q* = r -- If terminal state

    qValue[i] = _rewardHistory[expId];

    // If state is not terminal (next state is filled) then add Qnew to the Q value.
    if (_nextStateHistory[expId].size() > 0)
    {
     // Updating current state
     _currentState = _nextStateHistory[expId];

     // Running optimization experiment to get best estimated action
     _engine.run(_argMaxActionExperiment);

     // Getting optimal action, based on the NN evaluation
     auto action = _argMaxActionExperiment["Results"]["Best Sample"]["Parameters"].get<std::vector<double>>();

     // Getting optimal Qnew, based on the NN evaluation
     auto qNew = _argMaxActionExperiment["Results"]["Best Sample"]["F(x)"].get<double>();

     // We add the expected remaining reward based on the optimization
     qValue[i] += _discountFactor * qNew;

     // Debug only, print new experience values
     //printf("State: %f %f %f %f \n", _stateHistory[expId][0], _stateHistory[expId][1], _stateHistory[expId][2], _stateHistory[expId][3]);
     //printf("Action: %f\n", _actionHistory[expId][0]);
     //printf("Reward: %f\n", _rewardHistory[expId]);
     //printf("New State: %f %f %f %f\n", _currentState[0], _currentState[1], _currentState[2], _currentState[3]);
     //printf("New Action: %f\n", action[0]);
     //printf("QNew: %f\n", qNew);
     //printf("Q* %f\n", qValue[i]);
    }

    // Updating inputs to training learner
    for (size_t j = 0; j < _problem->_stateVectorSize; j++) _qTrainingProblem->_trainingInput[i][j] = _stateHistory[expId][j];
    for (size_t j = 0; j < _problem->_actionVectorSize; j++) _qTrainingProblem->_trainingInput[i][_problem->_stateVectorSize + j] = _actionHistory[expId][j];
    _qTrainingProblem->_trainingSolution[i][0] = qValue[i];

    // Keeping statistics
    _cumulativeQStar += qValue[i];
   }

   // Running one generation of the optimization method with the given mini-batch
   _qTrainingExperiment["Solver"]["Termination Criteria"]["Max Generations"] = _qTrainingExperiment._currentGeneration + 1;
   _qTrainingExperiment._solver->initialize();
   _engine.resume(_qTrainingExperiment);
  }

  // Keeping statistics
  _averageQStar = (double)_cumulativeQStar / (double)(_optimizationStepsPerUpdate * _miniBatchSize);

  /*********************************************************************
  * Updating the value of epsilon
  *********************************************************************/

  _epsilonCurrentValue = _epsilonCurrentValue - _epsilonDecreaseRate;
  if (_epsilonCurrentValue < _epsilonTargetValue) _epsilonCurrentValue = _epsilonTargetValue;

}

void DQN::getAction(Sample &sample)
{
  // Getting p = U[0,1] for the epsilon strategy
  double p = _uniformGenerator->getRandomNumber();

  // Getting the value of epsilon. If we haven't reached the start the minimum history, always use random decisions
  auto epsilon = sample.globals()["Epsilon"].get<double>();

  // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
  // If no states have been registered yet, use the random option in any case
  if (p < epsilon)
  {
   for (size_t i = 0; i < _problem->_actionVectorSize; i++)
   {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    double x = _uniformGenerator->getRandomNumber();

    // If discrete value vector was not provided, use lower and upper bounds
    if (_k->_variables[varIdx]->_values.size() == 0)
    {
     double lowerBound = _k->_variables[varIdx]->_lowerBound;
     double upperBound = _k->_variables[varIdx]->_upperBound;
     sample["Action"][i] = lowerBound + x*(upperBound - lowerBound);
    }
    else
    {
     // Randomly select one of the actions provided in the value vector
     size_t valIdx = floor(x * _k->_variables[varIdx]->_values.size());
     sample["Action"][i] = _k->_variables[varIdx]->_values[valIdx];
    }
   }

   return;
  }

 // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_qInferenceExperiment._solver);

  // Updating learner with the latest hyperparameters
  learner->setHyperparameters(sample.globals()["Hyperparameters"]);

  // Updating current state
  _currentState = KORALI_GET(std::vector<double>, sample, "State");

  // Running optimization experiment to get best estimated action
  korali::Engine engine;
  engine.run(_argMaxActionExperiment);

  // Getting optimal action, based on the NN evaluation
  sample["Action"] = _argMaxActionExperiment["Results"]["Best Sample"]["Parameters"];
}

void DQN::evaluateAction(Sample &sample, bool getGradients)
{
  // Getting learner's pointer
  auto learner = dynamic_cast<solver::Learner *>(_qInferenceExperiment._solver);

  // Setting weights and biases
  auto action = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Creating input batch
  std::vector<std::vector<double>> inputBatch(1);

  // Putting together state and action
  inputBatch[0].resize( _currentState.size() + action.size() );
  size_t pos = 0;
  for (size_t i = 0; i < _currentState.size(); i++) inputBatch[0][pos++]  = _currentState[i];
  for (size_t i = 0; i < action.size(); i++) inputBatch[0][pos++] = action[i];

  auto evaluation = learner->evaluate(inputBatch);
  auto gradients = learner->getGradients(evaluation);

  sample["F(x)"] = evaluation[0][0];

  for (size_t i = 0; i < action.size(); i++)
  {
    size_t startIdx = _currentState.size();
    sample["Gradient"][i] = gradients[0][startIdx + i];
  }

}

void DQN::printGenerationAfter()
{
 // Printing results so far
 _k->_logger->logInfo("Normal", "Experience Memory Size:       %lu/%lu\n", _stateHistory.size(), _replayMemoryMaximumSize);
 _k->_logger->logInfo("Normal", "Epsilon:                      %f\n", _epsilonCurrentValue);
 _k->_logger->logInfo("Normal", "Total Experience Count:       %lu\n", _totalExperienceCount);
 _k->_logger->logInfo("Normal", "Max Experience Count:         %lu\n", _maxExperienceCount);
 _k->_logger->logInfo("Normal", "Avg Experience Count:         %.0f\n", _averageExperienceCount);
 _k->_logger->logInfo("Normal", "Min Experience Count:         %lu\n", _minExperienceCount);
 _k->_logger->logInfo("Normal", "Average Reward:               %f\n", _averageReward);

 if (_targetAverageReward > -korali::Inf)
 _k->_logger->logInfo("Normal", "Best Average Reward:          %f/%f\n", _bestAverageReward, _targetAverageReward);
 else
 _k->_logger->logInfo("Normal", "Best Average Reward:          %f\n", _bestAverageReward);

 if (_maxSuboptimalSteps > 0)
 _k->_logger->logInfo("Normal", "Steps with Suboptimal Reward: %lu/%lu\n", _suboptimalStepCounter, _maxSuboptimalSteps);
 else
 _k->_logger->logInfo("Normal", "Steps with Suboptimal Reward: %lu\n", _suboptimalStepCounter);

 _k->_logger->logInfo("Normal", "Cumulative Expected Q-Value:  %f\n", _cumulativeQStar);
 _k->_logger->logInfo("Normal", "Average Expected Q-Value:     %f\n", _averageQStar);

 _k->_logger->logInfo("Normal", "Learner Information:\n");
 _qTrainingExperiment._logger->_verbosityLevel = _k->_logger->_verbosityLevel;
 _qTrainingExperiment._solver->printGenerationAfter();
 _qTrainingExperiment._logger->setVerbosityLevel("Silent");
}

} // namespace solver

} // namespace korali

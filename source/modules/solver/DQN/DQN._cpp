#include "modules/conduit/conduit.hpp"
#include "modules/solver/DQN/DQN.hpp"

namespace korali
{
namespace solver
{

void DQN::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Checking inputs

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   if (std::isfinite(_k->_variables[varIdx]->_lowerBound) == false)
    KORALI_LOG_ERROR("Lower bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_lowerBound);

   if (std::isfinite(_k->_variables[varIdx]->_upperBound) == false)
    KORALI_LOG_ERROR("Upper bound for action variable %lu must be finite, but is %f instead.\n", i, _k->_variables[varIdx]->_upperBound);

   if (_k->_variables[varIdx]->_lowerBound >= _k->_variables[varIdx]->_upperBound)
    KORALI_LOG_ERROR("Lower Bound (%f) for action variable %lu must be strictly smaller than its upper bound (%f).\n", _k->_variables[varIdx]->_lowerBound, i, _k->_variables[varIdx]->_upperBound);
  }

  /*********************************************************************
  * Creating Q-Maximizing Action Experiment
  *********************************************************************/

  // Creating evaluation lambda function for optimization
  auto fc = [this, s = &_currentState](Sample &sample) { this->evaluateAction(sample, *s, true); };

  _AExperiment["Problem"]["Type"] = "Optimization";
  _AExperiment["Problem"]["Objective Function"] = fc;

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
   size_t varIdx = _problem->_actionVectorIndexes[i];

   _AExperiment["Variables"][i]["Name"] = _k->_variables[varIdx]->_name;
   _AExperiment["Variables"][i]["Lower Bound"] = _k->_variables[varIdx]->_lowerBound;
   _AExperiment["Variables"][i]["Upper Bound"] = _k->_variables[varIdx]->_upperBound;

   double initialGuess = (_k->_variables[varIdx]->_upperBound + _k->_variables[varIdx]->_lowerBound) * 0.5;
   double initialStdDev = abs((_k->_variables[varIdx]->_upperBound - _k->_variables[varIdx]->_lowerBound)) * 0.1;
   _AExperiment["Variables"][i]["Initial Value"] = initialGuess;
   _AExperiment["Variables"][i]["Initial Mean"] = initialGuess;
   _AExperiment["Variables"][i]["Initial Standard Deviation"] = initialStdDev;
  }

  _AExperiment["Solver"] = _actionOptimizer;

  _AExperiment["Console Output"]["Frequency"] = 0;
  _AExperiment["File Output"]["Enabled"] = false;
  _AExperiment["Console Output"]["Verbosity"] = "Silent";

  _AExperiment["Random Seed"] = _k->_randomSeed++;

  // Running initialization to verify that the configuration is correct
  _engine.initialize(_AExperiment);

  // Setting termination criterion
  if (_maxEpisodes > _problem->_initialStateCount || _maxEpisodes == 0)
   _maxEpisodes = _problem->_initialStateCount;
}

void DQN::setInitialConfiguration()
{
 // Starting to process initial states, from the first one
 _currentEpisode = 0;

 // Keeps track of how many states have been processed so far
 _stateCount = 0;
}

void DQN::runGeneration()
{
 // If initial generation, set initial DQN configuration
  if (_k->_currentGeneration == 1)
   setInitialConfiguration();

  // Creating as many agents as initial states defined.
  size_t agentCount = 1;

  // Making sure we don't create more agents than remaining initial states
  if (agentCount + _currentEpisode > _maxEpisodes)
   agentCount = _maxEpisodes - _currentEpisode;

  // Creating storage for agents
  std::vector<Sample> agents(agentCount);

  // Index for current worker Id that helps making a balanced distribution of work
  size_t currentWorker = 0;

  // Storage for agent state/action information
  std::vector<std::vector<double>> actions(agentCount);
  std::vector<std::vector<double>> states(agentCount);
  for (size_t i = 0; i < agentCount; i++) actions[i].resize(_problem->_actionVectorSize);
  for (size_t i = 0; i < agentCount; i++) states[i].resize(_problem->_stateVectorSize);

  // Storage for agent reward information
  std::vector<double> rewards(agentCount);

  // Initializing Finished count
  size_t finishedCount = 0;

  // Setting initial states and actions
  for (size_t i = 0; i < agentCount; i++)
  {
   states[i] = _problem->_initialStates[_currentEpisode++];
   getAction(actions[i], states[i]);
   printf("ActionSize: %lu\n", actions[i].size());
  }

  // Initializing the agents and their environments
  for (size_t i = 0; i < agentCount; i++)
  {
    // Configuring Agent
    agents[i]["Worker"] = 0;
    agents[i]["Sample Id"] = i;
    agents[i]["Module"] = "Problem";
    agents[i]["Operation"] = "Run Environment";
    agents[i]["State"] = states[i];
    agents[i]["Action"] = actions[i];

    // Launching agent initialization
    _conduit->start(agents[i]);

    // Advancing to the next worker (if reached the last, start from beginning)
    currentWorker++;
    if (currentWorker == _conduit->maxConcurrency()) currentWorker = 0;
  }

  // Running main iteration loop until all agents have finished
  while (finishedCount < agentCount)
  {
    // Waiting for any of the pending agents
    size_t i = _conduit->waitAny(agents);

    // Updating reward
    rewards[i] = KORALI_GET(double, agents[i], "Reward");

    // Add state to current policy history
    _actionHistory.push_back(actions[i]);
    _stateHistory.push_back(states[i]);
    _rewardHistory.push_back(rewards[i]);

    // Storing agent's new state
    auto newState = KORALI_GET(std::vector<double>, agents[i], "State");

    // The new state becomes the current state
    states[i] = newState;

    // Checking for agent termination
    bool hasTerminated = KORALI_GET(bool, agents[i], "Finished");

    // If it finished, then add to the count
    if (hasTerminated)
    {
      // Incrementing finished agent count
      finishedCount++;
    }
    else // Else, get new action and run next episode
    {
      // Getting new action to perform
      getAction(actions[i], states[i]);
      agents[i]["Action"] = actions[i];

      // Run Episode
      _conduit->start(agents[i]);
    }
  }

  /*********************************************************************
  * Creating and running Q-Approximation Experiment
  *********************************************************************/

  Experiment QExperiment;

  QExperiment["Problem"]["Type"] = "Supervised Learning";
  QExperiment["Solver"]["Type"] = "Deep Learner";
  _neuralNetwork->getConfiguration(QExperiment["Solver"]["Neural Network"]);

  QExperiment["Solver"]["Optimizer"] = _weightOptimizer;

  QExperiment["Console Output"]["Frequency"] = 50;
  //QExperiment["Console Output"]["Verbosity"] = "Silent";
  QExperiment["File Output"]["Enabled"] = false;

   // Setting dummy configuration
  QExperiment["Problem"]["Training"]["Input"][0][0] = 0;
  QExperiment["Problem"]["Validation"]["Input"][0][0] = 0;
  QExperiment["Problem"]["Training"]["Solution"][0][0] = 0;
  QExperiment["Problem"]["Validation"]["Solution"][0][0] = 0;

  _engine.initialize(QExperiment);

  for (size_t i = _stateCount; i < _stateHistory.size(); i++)
  {
   size_t expVar = 0;
   for (size_t j = 0; j < _problem->_stateVectorSize; j++)
   {
    QExperiment["Problem"]["Training"]["Input"][i][expVar] = _stateHistory[i][j];
    QExperiment["Problem"]["Validation"]["Input"][i][expVar] = _stateHistory[i][j];
    expVar++;
   }

   for (size_t j = 0; j < _problem->_actionVectorSize; j++)
   {
    QExperiment["Problem"]["Training"]["Input"][i][expVar] = _actionHistory[i][j];
    QExperiment["Problem"]["Validation"]["Input"][i][expVar] = _actionHistory[i][j];
    expVar++;
   }

   QExperiment["Problem"]["Training"]["Solution"][i][0] = _rewardHistory[i];
   QExperiment["Problem"]["Validation"]["Solution"][i][0] = _rewardHistory[i];
  }

  _engine.run(QExperiment);

  // Broadcasting configuration
  knlohmann::json js;
  QExperiment.getConfiguration(js["Q Experiment"]);
  _conduit->broadcastGlobals(js);

  // Increasing state count
  _stateCount = _stateHistory.size();
}

void DQN::getAction(std::vector<double>& action, std::vector<double>& state)
{
 // Getting p = U[0,1] for the epsilon strategy
  double p = _uniformGenerator->getRandomNumber();

  // If p < e, then we choose the action randomly, with a uniform probability, among all possible actions.
  // If no states have been registered yet, use the random option in any case
  if (p < _epsilon || _stateCount == 0)
  {
   for (size_t i = 0; i < action.size(); i++)
   {
    size_t varIdx = _problem->_actionVectorIndexes[i];
    double lowerBound = _k->_variables[varIdx]->_lowerBound;
    double upperBound = _k->_variables[varIdx]->_upperBound;
    double x = _uniformGenerator->getRandomNumber();
    action[i] = lowerBound + x*(upperBound - lowerBound);
   }

   return;
  }

  // Updating current state
  _currentState = &state;

  // Running optimization experiment to get best estimated action
  _engine.run(_AExperiment);

  // Getting optimal action, based on the NN evaluation
  action = _AExperiment["Results"]["Best Sample"]["Parameters"].get<std::vector<double>>();
}

void DQN::evaluateAction(Sample &sample, std::vector<double>* state, bool getGradients)
{
  printf("Config: %s\n", sample["Globals"]["Q Experiment"].dump(2).c_str());

  // Getting updated Q Experiment configuration
  Experiment QExperiment;
  QExperiment.setConfiguration(sample["Globals"]["Q Experiment"]);
  QExperiment.initialize();

  // Setting weights and biases
  auto action = KORALI_GET(std::vector<double>, sample, "Parameters");

  // Creating input batch
  std::vector<std::vector<double>> inputBatch(1);

  // Putting together state and action
  inputBatch[0].resize( state->size() + action.size() );
  size_t pos = 0;
  for (size_t i = 0; i < state->size(); i++) inputBatch[0][pos++] = (*state)[i];
  for (size_t i = 0; i < action.size(); i++) inputBatch[0][pos++] = action[i];

  // Getting function valuation
  auto evaluation = QExperiment.evaluate(inputBatch);
  auto gradients = QExperiment.getGradients(evaluation);

  sample["F(x)"] = evaluation[0][0];

  for (size_t i = 0; i < action.size(); i++)
  {
    size_t startIdx = state->size();
    sample["Gradient"][i] = gradients[0][startIdx + i];
  }
}

} // namespace solver

} // namespace korali

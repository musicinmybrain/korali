{
 "Configuration Settings":
 [
  {
   "Name": [ "Discount Factor" ],
   "Type": "double",
   "Description": "Discount Factor for future states."
  },
  {
    "Name": [ "Neural Network" ],
    "Type": "korali::NeuralNetwork*",
    "Description": "Indicates the configuration of the underlying neural networks to use."
  },
  {
   "Name": [ "Epsilon", "Initial Value" ],
   "Type": "double",
   "Description": "Specifies the initial value for epsilon, the probability of not choosing the best possible action (exploitation) and using a random selection instead (exploration). e = 0 represents a pure greedy strategy, 0 < e < 1 represents the epsilon-greedy strategy, and e = 1 represents a purely random strategy."
  },
  {
   "Name": [ "Epsilon", "Decrease Rate" ],
   "Type": "double",
   "Description": "Specifies the how much the value of epsilon should be decreased as generations progress. A rate d(e) > 0.0 represents the Epsilon-decreasing strategy."
  },
  {
   "Name": [ "Epsilon", "Target Value" ],
   "Type": "double",
   "Description": "Specifies the last value of epsilon after which it will not be reduced any further."
  },
  {
    "Name": [ "Action Optimizer" ],
    "Type": "knlohmann::json",
    "Description": "Represents the state and configuration of the solver algorithm for argmax_action(Q)."
  },
  {
    "Name": [ "Weight Optimizer" ],
    "Type": "knlohmann::json",
    "Description": "Represents the state and configuration of the solver algorithm for the weights and biases of the NN."
  },
  {
    "Name": [ "Validation Ratio" ],
    "Type": "double",
    "Description": "Represents the ratio of validation episodes vs training episodes to use during training."
  },
  {
    "Name": [ "Episodes Per Generation" ],
    "Type": "size_t",
    "Description": "The number of episodes from which to draw experiences at each generation."
  },
  {
    "Name": [ "Mini Batch Size" ],
    "Type": "size_t",
    "Description": "The number of experiences to randomly select to train the neural network with."
  },
  {
    "Name": [ "Replay Memory", "Start Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to gather before learning starts."
  },
  {
    "Name": [ "Replay Memory", "Maximum Size" ],
    "Type": "size_t",
    "Description": "The minimum number of experiences to accumulate before starting to forget."
  },
  {
    "Name": [ "Replay Memory", "Replacement Policy" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Least Recently Added", "Description": "New experiences will replace those added least recently." },
                { "Value": "Uniform", "Description": "After adding the new experiences to the replay memory, randomly select experiences to evict with a uniform distribution." }
               ],
    "Description": "Specified the replacement policy to be used when receiving new experiences."
   },
  {
    "Name": [ "Optimization Steps Per Update" ],
    "Type": "size_t",
    "Description": "The number of optimization steps on the training Q estimator before updating the actual Q estimator."
  },
  {
    "Name": [ "Agent History Size" ],
    "Type": "size_t",
    "Description": "The number of most recent experiences returned by the actor."
  }
 ],
 
 "Results":
 [
 
 ],
 
  "Termination Criteria":
 [
  {
    "Name": [ "Max Suboptimal Steps" ],
    "Type": "size_t",
    "Criteria": "(_maxSuboptimalSteps > 0) && (_suboptimalStepCounter >= _maxSuboptimalSteps)",
    "Description": "The solver will stop when the given number of optimization steps have been applied without an improvement between learner updates."
  },
  {
    "Name": [ "Max Episodes" ],
    "Type": "size_t",
    "Criteria": "(_maxEpisodes > 0) && (_currentEpisode >= _maxEpisodes)",
    "Description": "The solver will stop when the given number of environments have been fully executed."
  },
  {
    "Name": [ "Max Experiences" ],
    "Type": "size_t",
    "Criteria": "(_maxExperiences > 0) && (_stateHistory.size() >= _maxExperiences)",
    "Description": "The solver will stop when the given number of experiences have been gathered."
  },
  {
    "Name": [ "Target Average Reward" ],
    "Type": "double",
    "Criteria": "(_targetAverageReward > -korali::Inf) && (_bestAverageReward >= _targetAverageReward)",
    "Description": "The solver will stop when the given best average per-episode reward has been reached among the experiences between two learner updates."
  }
 ],

 "Variables Configuration":
 [
  {
   "Name": [ "Lower Bound" ],
   "Type": "double",
   "Description": "Lower bound for the variable's value."
  },
  {
   "Name": [ "Upper Bound" ],
   "Type": "double",
   "Description": "Upper bound for the variable's value."
  },
  {
    "Name": [ "Values" ],
    "Type": "std::vector<double>",
    "Description": "[Hint] Locations to evaluate the Objective Function."
  }
 ],

 "Internal Settings":
 [
  {
    "Name": [ "Current Episode" ],
    "Type": "size_t",
    "Description": "Indicates the current episode being processed."
  },
  {
    "Name": [ "State History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of states."
  },
  {
    "Name": [ "Action History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of actions."
  },
  {
    "Name": [ "Reward History" ],
    "Type": "std::vector<double>",
    "Description": "Stores the whole history of rewards."
  },
  {
    "Name": [ "Next State History" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the whole history of next states (the ones that came after the action from the previous state)."
  },
  {
    "Name": [ "Uniform Generator" ],
    "Type": "korali::distribution::univariate::Uniform*",
    "Description": "Uniform random number generator for the epsilon-greedy strategy."
  },
  {
   "Name": [ "Epsilon", "Current Value" ],
   "Type": "double",
   "Description": "Specifies the current value of epsilon."
  },
  {
   "Name": [ "Total Experience Count" ],
   "Type": "size_t",
   "Description": "Total number of experiences in the generation."
  },
  {
   "Name": [ "Min Experience Count" ],
   "Type": "size_t",
   "Description": "Minimum number of experiences among the actors."
  },
  {
   "Name": [ "Max Experience Count" ],
   "Type": "size_t",
   "Description": "Maximum number of experiences among the actors."
  },
  {
   "Name": [ "Average Experience Count" ],
   "Type": "double",
   "Description": "Average number of experiences among the actors."
  },
  {
   "Name": [ "Cumulative Q Star" ],
   "Type": "double",
   "Description": "Sum of E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Average Q Star" ],
   "Type": "double",
   "Description": "Average E(Q) among the experiences from all minibatches in this generation."
  },
  {
   "Name": [ "Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the state of the training learner."
  },
  {
    "Name": [ "Suboptimal Step Counter" ],
    "Type": "size_t",
    "Description": "Remembers the number of minibatch optimization steps have been performed without a perceived improvement in rewards."
  },
  {
    "Name": [ "Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found on this round of experiences."
  },
  {
    "Name": [ "Best Average Reward" ],
    "Type": "double",
    "Description": "Remembers the cumulative average episode reward found so far from the experiences in between learner updates."
  },
  {
   "Name": [ "Best Hyperparameters" ],
   "Type": "knlohmann::json",
   "Description": "Stores the state of the best training learner (the one that got the best reward)."
  }
 ],
 
 "Module Defaults":
 {
  "Neural Network":   { "Type": "Neural Network" },
  "Termination Criteria":
   {
    "Max Episodes": 0,
    "Max Experiences": 0,
    "Max Suboptimal Steps": 0,
    "Target Average Reward": -Infinity 
   },
   
  "Replay Memory":
   {
    "Maximum Size": 10000,
    "Start Size": 1000,
    "Replacement Policy": "Least Recently Added"
   },
  
  "Discount Factor": 0.9,
  "Epsilon":
  {
   "Initial Value": 0.9,
   "Decrease Rate": 0.001,
   "Target Value": 0.1
  },
  
  "Uniform Generator":
  {
   "Type": "Univariate/Uniform",
   "Minimum": 0.0,
   "Maximum": 1.0
  },
  
  "Validation Ratio": 0.2,
  "Episodes Per Generation": 32,
  "Mini Batch Size": 32,
  "Optimization Steps Per Update": 10,
  "Agent History Size": 4
 },
 
 "Variable Defaults":
 {
  "Lower Bound": -Infinity,
  "Upper Bound": Infinity,
  "Values": [ ],
  "Hyperparameters": { }
 }
}

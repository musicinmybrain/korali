#include "modules/solver/QLearning/QLearning.hpp"
#include "modules/conduit/conduit.hpp"
#include "math.h"

void korali::solver::QLearning::setInitialConfiguration()
{
 for (size_t i = 0; i < _k->_variables.size(); i++)
 {
//  printf("Var: %lu - type: %s\n", i, _k->_variables[i]->_type.c_str());
  if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
  if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
 }

 auto actionVectorSize = _actionVectorIndexes.size();
 auto stateVectorSize = _stateVectorIndexes.size();

 if (actionVectorSize == 0) _k->_logger->logError("No action variables have been defined.\n");
 if (stateVectorSize == 0) _k->_logger->logError("No state variables have been defined.\n");

// printf("actionSize: %lu, stateSize: %lu\n", actionVectorSize, stateVectorSize);

 _stateCount = 1;
 for (size_t i = 0; i < stateVectorSize; i++)
 {
  size_t index = _stateVectorIndexes[i];
  _stateCount *= _k->_variables[index]->_parameterVector.size();
 }

 _actionCount = 1;
  for (size_t i = 0; i < actionVectorSize; i++)
  {
   size_t index = _actionVectorIndexes[i];
   _actionCount *= _k->_variables[index]->_parameterVector.size();
  }

 _q.resize(_actionCount * _stateCount);

 for (size_t i = 0; i < _q.size(); i++)
  _q[i] = 50;
}

void korali::solver::QLearning::runGeneration()
{
 if (_k->_currentGeneration == 1) setInitialConfiguration();

 korali::Sample sample;

 // Initializing Environments
 sample["Worker"] = 0;
 sample["Sample Id"] = 0;
 sample["Module"] = "Problem";
 sample["Operation"] = "Run Environment";

 // Initializing Environment
 _conduit->start(sample);
 _conduit->wait(sample);

 std::vector<double> state = sample["State"];
 size_t stateIdx = interpolateStateIndex(state);
 size_t actionIdx = 0, newActionIdx = 0;

 std::vector<size_t> actionIdxVector(_actionVectorIndexes.size());
 auto qIdx = selectBestAction(stateIdx, actionIdxVector, actionIdx);
 double reward = 0;
// printf("QIndex: %lu\n", qIdx);

 _convergenceRate = 0.0;

 std::vector<std::vector<double>> currentPolicy;

 while (sample["Finished"] == false)
 {
//  printf("stateIdx: %lu\n", stateIdx);

  std::vector<double> currentAction;
  for (size_t i = 0; i < actionIdxVector.size(); i++)
  {
   size_t varIndex = _actionVectorIndexes[i];
   double actionVal = _k->_variables[varIndex]->_parameterVector[actionIdxVector[i]];
//   printf("action[%lu] = %f\n", i, actionVal);
   currentAction.push_back(actionVal);
  }

  sample["Action"] = currentAction;
  currentPolicy.push_back(currentAction);

  _conduit->start(sample);
  _conduit->wait(sample);

  reward = sample["Reward"];
//  printf("New Reward: %f\n", reward);

  std::vector<double> newState = sample["State"];
  auto newStateIdx = interpolateStateIndex(newState);
//  printf("New State: %f\n", newState[0]);

  std::vector<size_t> newActionIdxVector(_actionVectorIndexes.size());
  auto argMaxNextQIdx = selectBestAction(newStateIdx, newActionIdxVector, newActionIdx);

  auto curQValue = _q[qIdx];
  auto newQValue = (1 - _learningRate) * _q[qIdx] + _learningRate * (reward + _discountFactor * _q[argMaxNextQIdx]);
  auto QValDiff = abs(newQValue - curQValue);
//  printf("Cur %f - New %f - QValDiff = %f\n", curQValue, newQValue, QValDiff);
  _q[qIdx] = newQValue;
  _convergenceRate += QValDiff;

  actionIdxVector = newActionIdxVector;
  state = newState;
  stateIdx = newStateIdx;
  actionIdx = newActionIdx;
  qIdx = argMaxNextQIdx;
 }

 if (reward > _bestReward)
 {
  _bestReward = reward;
  _bestPolicy = currentPolicy;

  _k->_js["Results"]["Optimal Policy"] = _bestPolicy;
  _k->_js["Results"]["Optimal Reward"] = _bestReward;
 }
}


size_t korali::solver::QLearning::selectBestAction(size_t stateId, std::vector<size_t>& actionIdxVector, size_t& actionIdx)
{
 size_t startPos = stateId * _actionCount;
 size_t finalPos = startPos + _actionCount;
 double qVal = -korali::Inf;
 size_t qIdx = 0;

 for (size_t pos = startPos; pos < finalPos; pos++)
  if (_q[pos] > qVal) { qVal = _q[pos]; qIdx = pos; }

 actionIdx =  qIdx - startPos;
 auto tempIdx = actionIdx;
 for (size_t i = 0; i < _actionVectorIndexes.size(); i++)
 {
  size_t index = _actionVectorIndexes[i];
  size_t actionVectorSize = _k->_variables[index]->_parameterVector.size();
  actionIdxVector[i] = tempIdx % actionVectorSize;
  tempIdx = tempIdx / actionVectorSize;
//  printf("Action[%lu] Index: %lu\n", i, actionIdxVector[i]);
 }

 return qIdx;
}

size_t korali::solver::QLearning::interpolateStateIndex(const std::vector<double>& state)
{
 size_t pos = 0;
 size_t multiplier = 1;

 for (size_t i = 0; i < state.size(); i++)
 {
  double value = state[i];
  size_t index = _stateVectorIndexes[i];

  double minDiff = korali::Inf;
  size_t valPos = 0;
  for (size_t j = 0; j < _k->_variables[index]->_parameterVector.size(); j++)
  {
   double diff = abs(value - _k->_variables[index]->_parameterVector[j]);
   if (diff < minDiff) { minDiff = diff; valPos = j; }
  }

  pos += valPos * multiplier;
  multiplier *= _k->_variables[index]->_parameterVector.size();
 }

 return pos;
}

void korali::solver::QLearning::printGenerationBefore()
{
}

void korali::solver::QLearning::printGenerationAfter()
{
 _k->_logger->logInfo("Normal", "Best Reward: %f\n", _bestReward);
 _k->_logger->logInfo("Normal", "Convergence Rate: %f\n", _convergenceRate);
}


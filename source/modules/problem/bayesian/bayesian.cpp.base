#include "modules/problem/bayesian/bayesian.hpp"
#include "sample/sample.hpp"

__startNamespace__;

void __className__::initialize()
{
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    bool foundDistribution = false;

    for (size_t j = 0; j < _k->_distributions.size(); j++)
    {
      _k->_distributions[j]->updateDistribution();
      if (_k->_variables[i]->_priorDistribution == _k->_distributions[j]->_name)
      {
        foundDistribution = true;
        _k->_variables[i]->_distributionIndex = j;
      }
    }

    if (foundDistribution == false)
      KORALI_LOG_ERROR("Did not find distribution %s, specified by variable %s\n", _k->_variables[i]->_priorDistribution.c_str(), _k->_variables[i]->_name.c_str());
  }
}

void __className__::evaluateLogPrior(Sample &sample)
{
  double logPrior = 0.0;
  const auto params = KORALI_GET(std::vector<double>, sample, "Parameters");

  for (size_t i = 0; i < params.size(); i++)
    logPrior += _k->_distributions[_k->_variables[i]->_distributionIndex]->getLogDensity(params[i]);

  sample["logPrior"] = logPrior;
}

void __className__::evaluateLogPriorGradient(Sample &sample)
{
  const auto params = KORALI_GET(std::vector<double>, sample, "Parameters");
  std::vector<double> logPriorGradient(params.size(), 0.);

  for (size_t i = 0; i < params.size(); i++)
    logPriorGradient[i] = _k->_distributions[_k->_variables[i]->_distributionIndex]->getLogDensityGradient(params[i]);

  sample["logPrior Gradient"] = logPriorGradient;
}

void __className__::evaluateLogPriorHessian(Sample &sample)
{
  const auto params = KORALI_GET(std::vector<double>, sample, "Parameters");
  const size_t numParam = params.size();
  std::vector<double> logPriorHessian(numParam * numParam, 0.);

  for (size_t i = 0; i < numParam; i++)
    logPriorHessian[i * numParam + i] = _k->_distributions[_k->_variables[i]->_distributionIndex]->getLogDensityHessian(params[i]);

  sample["logPrior Hessian"] = logPriorHessian;
}

void __className__::evaluateLogPosterior(Sample &sample)
{
  const int sampleId = sample["Sample Id"];
  evaluateLogPrior(sample);

  const double logPrior = KORALI_GET(double, sample, "logPrior");

  if (logPrior == -Inf)
  {
    sample["logLikelihood"] = -Inf;
    sample["logPosterior"] = -Inf;
  }
  else
  {
    evaluateLoglikelihood(sample);
    const double logLikelihood = KORALI_GET(double, sample, "logLikelihood");
    const double logPosterior = logPrior + logLikelihood;

    if (std::isnan(logLikelihood) == true) KORALI_LOG_ERROR("Sample %d returned NaN logLikelihood evaluation.\n", sampleId);

    sample["logPosterior"] = logPosterior;
  }
}

void __className__::evaluate(Sample &sample)
{
  evaluateLogPosterior(sample);
  sample["F(x)"] = sample["logPosterior"];
  sample["logP(x)"] = sample["logPosterior"];
}

void __className__::evaluateGradient(Sample &sample)
{
  evaluateLogPriorGradient(sample);
  evaluateLoglikelihoodGradient(sample);
  const auto logPriorGrad = KORALI_GET(std::vector<double>, sample, "logPrior Gradient");
  auto logLikGrad = KORALI_GET(std::vector<double>, sample, "logLikelihood Gradient");

  for (size_t i = 0; i < logPriorGrad.size(); ++i)
    logLikGrad[i] += logPriorGrad[i];
  sample["grad(logP(x))"] = logLikGrad;
}

void __className__::evaluateHessian(Sample &sample)
{
  evaluateLogPriorHessian(sample);
  evaluateLogLikelihoodHessian(sample);
  const auto logPriorHessian = KORALI_GET(std::vector<double>, sample, "logPrior Hessian");
  auto logLikHessian = KORALI_GET(std::vector<double>, sample, "logLikelihood Hessian");

  for (size_t i = 0; i < logPriorHessian.size(); i++)
    logLikHessian[i] += logPriorHessian[i];

  sample["H(logP(x))"] = logLikHessian;
}

__moduleAutoCode__;

__endNamespace__;

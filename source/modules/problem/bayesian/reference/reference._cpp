#include "modules/conduit/conduit.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/problem/bayesian/reference/reference.hpp"
#include "sample/sample.hpp"

#include <gsl/gsl_blas.h>
#include <gsl/gsl_cdf.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_sf_gamma.h>
#include <gsl/gsl_sf_psi.h>
#include <gsl/gsl_randist.h>


@startNamespace

void @className::initialize()
{
  Bayesian::initialize();

  if (_referenceData.size() == 0) KORALI_LOG_ERROR("Bayesian (%s) problems require defining reference data.\n", _likelihoodModel.c_str());

  if (_k->_variables.size() < 1) KORALI_LOG_ERROR("Bayesian (%s) inference problems require at least one variable.\n", _likelihoodModel.c_str());
}

void @className::evaluateLoglikelihood(Sample &sample)
{
  sample.run(_computationalModel);

  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized result array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  if (_likelihoodModel == "Normal")
    loglikelihoodNormal(sample);
  else if (_likelihoodModel == "Positive Normal")
    loglikelihoodPositiveNormal(sample);
  else if (_likelihoodModel == "StudentT")
    loglikelihoodStudentT(sample);
  else if (_likelihoodModel == "Positive StudentT")
    loglikelihoodPositiveStudentT(sample);
  else if (_likelihoodModel == "Poisson")
    loglikelihoodPoisson(sample);
  else if (_likelihoodModel == "Geometric")
    loglikelihoodGeometric(sample);
  else if (_likelihoodModel == "Negative Binomial")
    loglikelihoodNegativeBinomial(sample);
  else
    KORALI_LOG_ERROR("Bayesian problem (%s) not recognized.\n", _likelihoodModel.c_str());
}

double @className::compute_normalized_sse(std::vector<double> f, std::vector<double> g, std::vector<double> y)
{
  double sse = 0.;
  for (size_t i = 0; i < y.size(); i++)
  {
    double diff = (y[i] - f[i]) / g[i];
    sse += diff * diff;
  }
  return sse;
}

void @className::loglikelihoodNormal(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double sse = -Inf;
  sse = compute_normalized_sse(refEvals, stdDevs, _referenceData);

  double loglike = 0.;
  for (size_t i = 0; i < stdDevs.size(); i++)
  {
    if (stdDevs[i] <= 0.0) KORALI_LOG_ERROR("Negative or zero value (%lf) detected for the Standard Deviation.\n", stdDevs[i]);
    loglike -= log(stdDevs[i]);
  }

  loglike -= 0.5 * (_referenceData.size() * _log2pi + sse);
  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodPositiveNormal(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double loglike = 0.;
  for (size_t i = 0; i < stdDevs.size(); i++)
  {
    double m = refEvals[i];
    double s = stdDevs[i];

    if (s <= 0.0) KORALI_LOG_ERROR("Negative or zero value (%lf) detected for the Standard Deviation.\n", s);
    if (m < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Evaluation.\n", m);
    if (_referenceData[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Data.\n", _referenceData[i]);

    double z = (_referenceData[i] - m) / s;

    loglike -= 0.5 * (_log2pi + z * z);
    loglike -= log(s);
    loglike -= log(1. - gsl_cdf_gaussian_P(-m / s, 1.0));
  }

  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodStudentT(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto nus = KORALI_GET(std::vector<double>, sample, "Degrees Of Freedom");

  if (nus.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Degrees Of Freedom array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), nus.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double loglike = 0.;
  for (size_t i = 0; i < nus.size(); i++)
  {
    if (nus[i] <= 0.0) KORALI_LOG_ERROR("Negative or zero value (%lf) detected for the Degrees Of Freedom.\n", nus[i]);
    loglike += log(gsl_ran_tdist_pdf(_referenceData[i] - refEvals[i], nus[i]));
  }

  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodPositiveStudentT(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto nus = KORALI_GET(std::vector<double>, sample, "Degrees Of Freedom");

  if (nus.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Degrees Of Freedom array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), nus.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double loglike = 0.;
  for (size_t i = 0; i < _referenceData.size(); i++)
  {
    if (nus[i] <= 0.0) KORALI_LOG_ERROR("Negative or zero value (%lf) detected for the Degrees Of Freedom.\n", nus[i]);
    if (refEvals[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Evaluation.\n", refEvals[i]);
    if (_referenceData[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Data.\n", _referenceData[i]);

    loglike += log(gsl_ran_tdist_pdf(_referenceData[i] - refEvals[i], nus[i]));
    loglike -= log(1.0 - gsl_cdf_tdist_P(-refEvals[i], nus[i]));
  }

  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodPoisson(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double loglike = 0.;
  for (size_t i = 0; i < _referenceData.size(); i++)
  {
    if (refEvals[i] <= 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Evaluation.\n", refEvals[i]);
    if (_referenceData[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Data.\n", _referenceData[i]);
    loglike += log(gsl_ran_poisson_pdf(_referenceData[i], refEvals[i]));
  }

  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodGeometric(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  double loglike = 0.;
  for (size_t i = 0; i < _referenceData.size(); i++)
  {
    if (refEvals[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Evaluation.\n", refEvals[i]);
    if (_referenceData[i] < 0.0) KORALI_LOG_ERROR("Negative value (%lf) detected in Reference Data.\n", _referenceData[i]);
    loglike += log(gsl_ran_geometric_pdf(_referenceData[i] + 1.0, 1.0 / (1.0 + refEvals[i])));
  }

  sample["logLikelihood"] = loglike;
}

void @className::loglikelihoodNegativeBinomial(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto dispersion = KORALI_GET(std::vector<double>, sample, "Dispersion");

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  if (dispersion.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Dispersion array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), dispersion.size());

  size_t N = _referenceData.size();
  double loglike = 0.0;

  for (size_t i = 0; i < N; i++)
  {
    double y = _referenceData[i];
    loglike -= gsl_sf_lngamma(y + 1.);

    double m = refEvals[i];

    if (y < 0)
    {
      KORALI_LOG_ERROR("Negative Binomial Likelihood not defined for negative Reference Data (provided %lf.\n", y);
    }

    if (m <= 0)
    {
      sample["logLikelihood"] = -Inf;
      return;
    }

    double r = dispersion[i];

    double p = m / (m + r);

    loglike += gsl_sf_lngamma(y + r);
    loglike -= gsl_sf_lngamma(r);
    loglike += r * log(1 - p);
    loglike += y * log(p);
  }

  sample["logLikelihood"] = loglike;
}

void @className::evaluateLoglikelihoodGradient(Sample &sample)
{
  double eval = sample["F(x)"];
  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")
      gradientLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")
      gradientLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial")
      gradientLoglikelihoodNegativeBinomial(sample);
    else
      KORALI_LOG_ERROR("Gradient not yet implemented for selected bayesian problem and log likelihood model.");
  }
  else
  {
    sample["logLikelihood Gradient"] = std::vector<double>(_k->_variables.size(), 0.0);
  }
}

void @className::gradientLoglikelihoodNormal(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");
  auto gradientF = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Mean");
  auto gradientG = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  if (gradientF.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());
  if (gradientG.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) KORALI_LOG_ERROR("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) KORALI_LOG_ERROR("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double invStdDev = 1.0 / stdDevs[i];
    double invStdDev2 = invStdDev * invStdDev;
    double invStdDev3 = invStdDev2 * invStdDev;

    double dif = _referenceData[i] - refEvals[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      double tmpGrad = -invStdDev * gradientG[i][d] + invStdDev2 * dif * gradientF[i][d] + invStdDev3 * dif * dif * gradientG[i][d];
      llkgradient[d] += tmpGrad;
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void @className::gradientLoglikelihoodPositiveNormal(Sample &sample)
{
  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");
  auto gradientF = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Mean");
  auto gradientG = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  if (gradientF.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());
  if (gradientG.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());

  std::vector<double> llkgradient(_k->_variables.size(), 0.0);
  for (size_t i = 0; i < _referenceData.size(); ++i)
  {
    if (gradientF[i].size() != _k->_variables.size()) KORALI_LOG_ERROR("Bayesian Reference Mean gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientF[i].size());
    if (gradientG[i].size() != _k->_variables.size()) KORALI_LOG_ERROR("Bayesian Reference Standard Deviation gradient calculation requires gradients of size %zu (provided size %zu)\n", _k->_variables.size(), gradientG[i].size());

    double mu = refEvals[i];
    double sig = stdDevs[i];

    double invsig = 1.0 / sig;
    double invsig2 = invsig * invsig;
    double invsig3 = invsig2 * invsig;

    double Z = 1.0 - gsl_cdf_gaussian_P(-mu / sig, 1.0);
    double invZ = 1.0 / Z;

    double phims = gsl_ran_gaussian_pdf(-mu / sig, 1.0);

    double dif = _referenceData[i] - refEvals[i];

    for (size_t d = 0; d < _k->_variables.size(); ++d)
    {
      if (!isfinite(gradientF[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Mean.\n");
      if (!isfinite(gradientG[i][d])) _k->_logger->logWarning("Normal", "Non-finite value detected in Gradient Standard Deviation.\n");
      llkgradient[d] += (-invsig * gradientG[i][d] + invsig2 * dif * gradientF[i][d] + invsig3 * dif * dif * gradientG[i][d]);
      llkgradient[d] += invZ * phims * (-1.0 * invsig * gradientF[i][d] + invsig2 * mu * gradientG[i][d]);
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void @className::gradientLoglikelihoodNegativeBinomial(Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto dispersion = KORALI_GET(std::vector<double>, sample, "Dispersion");
  auto gradientMean = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Mean");
  auto gradientDispersion = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Dispersion");

  if (refEvals.size() != Nd)
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), Nd, refEvals.size());

  if (dispersion.size() != Nd)
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Dispersion array. Provided: %lu.\n", _likelihoodModel.c_str(), Nd, dispersion.size());

  if (gradientMean.size() != Nd) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientMean.size(), Nd);

  if (gradientDispersion.size() != Nd) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Dispersion for each reference evaluation (provided %zu required %zu).", gradientMean.size(), Nd);

  std::vector<double> llkgradient(Nth, 0.0);

  double r, m, k, tmpsum;
  for (size_t i = 0; i < Nd; i++)
  {
    r = dispersion[i];
    m = refEvals[i];
    k = _referenceData[i];

    tmpsum = r + m;

    for (size_t d = 0; d < Nth; ++d)
    {
      llkgradient[d] += r * (k - m) * gradientMean[i][d] / (m * tmpsum);
      llkgradient[d] += (gsl_sf_psi(r + k) - gsl_sf_psi(r)) * gradientDispersion[i][d];
      llkgradient[d] += gradientDispersion[i][d] * (m * log(r / tmpsum) + r * log(r / tmpsum) + m - k) / tmpsum;
    }
  }

  sample["logLikelihood Gradient"] = llkgradient;
}

void @className::evaluateFisherInformation(Sample &sample)
{
  auto eval = KORALI_GET(double, sample, "F(x)");

  if (isfinite(eval))
  {
    if (_likelihoodModel == "Normal")
      fisherInformationLoglikelihoodNormal(sample);
    else if (_likelihoodModel == "Positive Normal")
      fisherInformationLoglikelihoodPositiveNormal(sample);
    else if (_likelihoodModel == "Negative Binomial")
      fisherInformationLoglikelihoodNegativeBinomial(sample);
    else
      KORALI_LOG_ERROR("Fisher Information not yet implemented for selected Bayesian problem and log likelihood model.");
  }
  else
  {
    sample["Fisher Information"] = std::vector<double>(_k->_variables.size() * _k->_variables.size(), 0.0);
  }
}

void @className::fisherInformationLoglikelihoodNormal(Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");
  auto gradientF = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Mean");
  auto gradientG = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (gradientF.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());
  if (gradientG.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  {
    double var = stdDevs[i] * stdDevs[i];
    double varinv = 1. / var;

    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = varinv * gradientF[i][k] * gradientF[i][l] + 2. * varinv * gradientG[i][k] * gradientG[i][l];
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] += (varinv * gradientF[i][k] * gradientF[i][k] + 2. * varinv * gradientG[i][k] * gradientG[i][k]);
    }
  }
  sample["Fisher Information"] = FIM;
}

void @className::fisherInformationLoglikelihoodPositiveNormal(Sample &sample)
{
  size_t Nd = _referenceData.size();
  size_t Nth = _k->_variables.size();

  auto refEvals = KORALI_GET(std::vector<double>, sample, "Reference Evaluations");
  auto stdDevs = KORALI_GET(std::vector<double>, sample, "Standard Deviation");
  auto gradientF = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Mean");
  auto gradientG = KORALI_GET(std::vector<std::vector<double>>, sample, "Gradient Standard Deviation");

  if (stdDevs.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Standard Deviation array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), stdDevs.size());

  if (refEvals.size() != _referenceData.size())
    KORALI_LOG_ERROR("This Bayesian (%s) problem requires a %lu-sized Reference Evaluations array. Provided: %lu.\n", _likelihoodModel.c_str(), _referenceData.size(), refEvals.size());

  if (gradientF.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Mean for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());
  if (gradientG.size() != _referenceData.size()) KORALI_LOG_ERROR("Bayesian problem requires a gradient of the Standard Deviation for each reference evaluation (provided %zu required %zu).", gradientF.size(), _referenceData.size());

  std::vector<double> FIM(Nth * Nth, 0.0);
  for (size_t i = 0; i < Nd; ++i)
  {
    double mu = refEvals[i];
    double sig = stdDevs[i];
    double var = sig * sig;

    double phims = gsl_ran_ugaussian_pdf(mu / sig);
    double phims2 = phims * phims;

    double Z = 1.0 - gsl_cdf_ugaussian_P(-mu / sig);
    double invZ = 1.0 / Z;
    double invZ2 = invZ * invZ;

    double invvar = 1. / var;
    double invsig3 = invvar / sig;
    double invsig4 = invvar * invvar;
    double invsig5 = invvar * invsig3;

    double Imu = invvar - invZ2 * invvar * phims2 - invZ * mu * invsig3 * phims;
    double Isig = 2. * invvar - 5. * invZ * mu * invsig3 * phims - invZ2 * mu * mu * invsig4 * phims2 - invZ * mu * mu * mu * invsig5 * phims;
    double Ims = invZ * (var + mu * mu) * invsig4 * phims + invZ2 * mu * invsig3 * phims2;

    double tmp;
    for (size_t k = 0; k < Nth; ++k)
    {
      for (size_t l = 0; l < k; ++l)
      {
        tmp = gradientF[i][k] * gradientF[i][l] * Imu + (gradientF[i][k] * 2 * sig * gradientG[i][l] + gradientF[i][l] * 2 * sig * gradientG[i][k]) * Ims + 4 * var * gradientG[i][k] * gradientG[i][l] * Isig;
        FIM[k * Nth + l] += tmp;
        FIM[l * Nth + k] += tmp;
      }
      FIM[k * Nth + k] += (gradientF[i][k] * gradientF[i][k] * Imu + (4 * sig * gradientF[i][k] * gradientG[i][k]) * Ims + 4 * var * gradientG[i][k] * gradientG[i][k] * Isig);
    }
  }
  sample["Fisher Information"] = FIM;
}

void @className::fisherInformationLoglikelihoodNegativeBinomial(Sample &sample)
{
  // TODO
}

@moduleAutoCode

@endNamespace

#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace problem
{
void ReinforcementLearning::initialize()
{
  // Processing state/action variable configuration

  _stateVectorIndexes.clear();
  _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _actionVectorSize = _actionVectorIndexes.size();
  _stateVectorSize = _stateVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");

  if (_actionRepeat == 0) KORALI_LOG_ERROR("Action repeat cannot be zero.\n");
}

/**
 * @brief Pointer to the current sample, it is immediately copied as to avoid concurrency problems
 */
Sample *__currentSample;

/**
 * @brief Identifier of the current environment function Id.
 */
size_t __envFunctionId;

/**
 * @brief Thread wrapper to run an environment
 */
void __environmentWrapper()
{
  Sample *sample = __currentSample;

  sample->run(__envFunctionId);

  (*sample)["Finished"] = true;

  co_switch(sample->_workerThread);

  KORALI_LOG_ERROR("Resuming a finished sample\n");
}

void ReinforcementLearning::runTrainingSample(Sample &sample) { runEnvironment(sample, true); }
void ReinforcementLearning::runTestingSample(Sample &sample) { runEnvironment(sample, false); }
void ReinforcementLearning::runEnvironment(Sample &sample, bool useTrainingAction)
{
  // Getting RL-compatible solver
  auto agent = dynamic_cast<solver::Agent *>(_k->_solver);

  // First, we update the policy's hyperparameters
  agent->updateHyperparameters(sample.globals()["Hyperparameters"]);

  sample["Finished"] = false;

  // Creating sample coroutine
  __currentSample = &sample;
  __envFunctionId = _environmentFunction;
  sample._workerThread = co_active();
  auto envThread = co_create(1 << 28, __environmentWrapper);

  // Getting first state
  co_switch(envThread);

  // Action Repeat Counter
  size_t actionLastUpdated = 0;

  // Initializing episode's experience counter
  size_t expId = 0;

  // A counter for the cumulative reward
  double cumulativeReward = 0.0;

  // Storage for the action
  std::vector<double> action(_actionVectorSize);

  // Saving experiences
  while (sample["Finished"] == false)
  {
    // Retrieving state
    auto state = sample["State"].get<std::vector<double>>();

    // Requesting new action, if enough steps have passed, given by the action repeat parameter
    if (actionLastUpdated == 0)
    {
      if (useTrainingAction == true)
        action = agent->getTrainingAction(state);
      else
        action = agent->getInferenceAction(state);

      actionLastUpdated = _actionRepeat;
    }
    actionLastUpdated--;

    // Setting current action
    sample["Action"] = action;

    // Storing the current state
    sample["Experience"]["States"][expId] = state;

    // Storing the current action
    sample["Experience"]["Actions"][expId] = action;

    // Calculating and storing probability density of current state/action
    sample["Experience"]["Densities"][expId] = agent->getStateActionProbabilityDensity(state, action);

    // Jumping back into the agent's environment
    co_switch(envThread);

    // Returning from the agent's environment with the reward
    double reward = sample["Reward"].get<double>();
    sample["Experience"]["Rewards"][expId] = reward;

    // Adding this experience's reward to total reward
    cumulativeReward += reward;

    // Advancing to next experience
    expId++;
  }

  sample["Cumulative Reward"] = cumulativeReward;

  co_delete(envThread);
}

} // namespace problem

} // namespace korali

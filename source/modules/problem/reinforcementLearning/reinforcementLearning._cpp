#include "engine.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <gsl/gsl_multifit.h>

namespace korali
{
namespace problem
{
void ReinforcementLearning::initialize()
{
  // Processing state/action variable configuration
  _stateVectorIndexes.clear();
  _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _actionVectorSize = _actionVectorIndexes.size();
  _stateVectorSize = _stateVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");
  if (_testingFrequency == 0) _testingFrequency = _k->_solver->_maxGenerations;

  // Validating observations
  _numberObservedTrajectories = _observationsStates.size();
  _featureVectorSize = _observationsFeatures[0][0].size();
  if (_featureVectorSize == 0) KORALI_LOG_ERROR("No features have been defined.\n");

  if (_observationsActions.size() != _numberObservedTrajectories)
    KORALI_LOG_ERROR("Number of trajectories mismatch between observed states and observed actions.\n");

  if (_observationsFeatures.size() != _numberObservedTrajectories)
    KORALI_LOG_ERROR("Number of trajectories mismacht between observed states and observed features.\n");

  size_t totalObservedPairs = 0;
  for (size_t t = 0; t < _numberObservedTrajectories; ++t)
  {
    size_t trajectoryLength = _observationsStates[t].size();
    totalObservedPairs += trajectoryLength;

    for (size_t i = 0; i < trajectoryLength; ++i)
    {
      if (_observationsStates[t][i].size() != _stateVectorSize)
        KORALI_LOG_ERROR("Dimension of observed state (trajectory %zu index %zu) does not agree with problem configuration.\n", t, i);
    }

    if (_observationsActions[t].size() != trajectoryLength)
      KORALI_LOG_ERROR("Trajectory (%zu) length of observed actions (%zu) does not agree with trajectory length of observed states (%zu)\n", t, _observationsActions[t].size(), trajectoryLength);

    for (size_t i = 0; i < trajectoryLength; ++i)
    {
      if (_observationsActions[t][i].size() != _actionVectorSize)
        KORALI_LOG_ERROR("Dimension of observed action (%zu) does not agree with problem configuration (trajectory %zu index %zu).\n", _observationsActions[t][i].size(), t, i);
    }

    if (_observationsFeatures[t].size() != trajectoryLength)
      KORALI_LOG_ERROR("Trajectory length of observed features (trajectory %zu) does not agree with observed states\n", t);

    for (size_t i = 0; i < trajectoryLength; ++i)
    {
      if (_observationsFeatures[t][i].size() != _featureVectorSize)
        KORALI_LOG_ERROR("Dimension (%zu) of observed features (trajectory %zu index %zu) does not agree with problem configuration.\n", _observationsFeatures[t][i].size(), t, i, _featureVectorSize);
    }
  }

  // Allocate memory for linear controller
  _observationsApproximatorWeights.resize(_stateVectorSize + 1, std::vector<float>(_actionVectorSize));
  _observationsApproximatorSigmas.resize(_actionVectorSize);

  // Building linear controller for observed state action pairs
  gsl_matrix *X = gsl_matrix_alloc (totalObservedPairs, _stateVectorSize + 1);
  gsl_matrix *Y = gsl_matrix_alloc (totalObservedPairs, _actionVectorSize);
  size_t idx = 0;
  for (size_t t = 0; t < _numberObservedTrajectories; ++t)
  {
    size_t trajectoryLength = _observationsStates[t].size();
    for (size_t i = 0; i < trajectoryLength; ++i)
    {
        gsl_matrix_set(X, idx, 0, 1.0); // intercept
        for (size_t j = 0; j < _stateVectorSize; ++j)
        {
            gsl_matrix_set(X, idx, j+1, (double) _observationsStates[t][i][j]);
            printf("x %zu %zu %f\n", idx, j, _observationsStates[t][i][j]);
        }
 
        for (size_t j = 0; j < _actionVectorSize; ++j)
        {
            gsl_matrix_set(Y, idx, j, (double) _observationsActions[t][i][j]);
            printf("Y %zu %zu %f\n", idx, j, _observationsActions[t][i][j]);
        }

        idx++;
    }
  }

  // Do regression over actions
  for(size_t i = 0; i < _actionVectorSize; ++i)
  {
    double chisq;
    gsl_vector *c = gsl_vector_alloc (_stateVectorSize + 1);
    gsl_matrix *cov = gsl_matrix_alloc (_stateVectorSize + 1, _stateVectorSize + 1);
    gsl_multifit_linear_workspace * work = gsl_multifit_linear_alloc (totalObservedPairs, _stateVectorSize + 1);

    gsl_vector_view y = gsl_matrix_column(Y, i);
    gsl_multifit_linear (X, &y.vector, c, cov, &chisq, work);
    
    for(size_t j = 0; j < _stateVectorSize + 1; ++j)
    {
      _observationsApproximatorWeights[i][j] = gsl_vector_get(c,j);
      printf("w: %f\n", _observationsApproximatorWeights[i][j]);
    }
  
    gsl_multifit_linear_free(work);
    gsl_vector_free(c);
    gsl_matrix_free(cov);
   }

  // Calculate standard errors
  std::vector<float> squaredErrors(_actionVectorSize);
  for(size_t t = 0; t < _numberObservedTrajectories; ++t)
     for(size_t i = 0; i < _observationsStates[t].size(); ++i)
     {
         for(size_t k = 0; k < _actionVectorSize; ++k)
         {
              float approx = _observationsApproximatorWeights[0][k]; // intercept
              for(size_t j = 0; j < _stateVectorSize; ++j)
              {
                  approx += _observationsStates[t][i][j] * _observationsApproximatorWeights[j+1][k];
              }
              squaredErrors[k] += std::pow(_observationsActions[t][i][k] - approx, 2.);
         }
     }

  // Set MLE sigma estimates
  for(size_t k = 0; k < _actionVectorSize; ++k)
  {
    _observationsApproximatorSigmas[k] = std::sqrt(squaredErrors[k]/(float) totalObservedPairs);
    printf("approximator sigma %zu %f\n", k, _observationsApproximatorSigmas[k]);
  }

  gsl_matrix_free(X);
  gsl_matrix_free(Y);
  
  KORALI_LOG_ERROR("STOP TEST");
}

/**
 * @brief Pointer to the current agent, it is immediately copied as to avoid concurrency problems
 */
Sample *__currentSample;

/**
 * @brief Identifier of the current environment function Id.
 */
size_t __envFunctionId;

/**
 * @brief Pointer to the agent (Korali solver module)
 */
solver::Agent *_agent;

/**
  * @brief Stores the environment thread (coroutine).
  */
cothread_t _envThread;

/**
 * @brief Thread wrapper to run an environment
 */
void __environmentWrapper()
{
  Sample *agent = __currentSample;

  agent->run(__envFunctionId);

  if ((*agent)["Termination"] == "Non Terminal")
    KORALI_LOG_ERROR("Environment function terminated, but agent termination status (success or truncated) was not set.\n");

  bool terminationRecognized = false;
  if ((*agent)["Termination"] == "Terminal") terminationRecognized = true;
  if ((*agent)["Termination"] == "Truncated") terminationRecognized = true;

  if (terminationRecognized == false)
    KORALI_LOG_ERROR("Environment function terminated, but agent termination status (%s) is neither 'Terminal' nor 'Truncated'.\n", (*agent)["Termination"].get<std::string>().c_str());

  co_switch(agent->_workerThread);

  KORALI_LOG_ERROR("Resuming a finished agent\n");
}

void ReinforcementLearning::runTrainingEpisode(Sample &agent)
{
  // Profiling information - Computation and communication time taken by the agent
  double agentPolicyEvaluationTime = 0.0;
  double agentComputationTime = 0.0;
  double agentCommunicationTime = 0.0;

  // Initializing environment configuration
  initializeEnvironment(agent);

  // Define state rescaling variables
  auto stateRescalingMeans = agent["State Rescaling"]["Means"].get<std::vector<float>>();
  auto stateRescalingSdevs = agent["State Rescaling"]["Standard Deviations"].get<std::vector<float>>();

  // Counter for the total number of actions taken
  size_t actionCount = 0;

  // Setting mode to traing to add exploratory noise or random actions
  agent["Mode"] = "Training";

  // Reserving message storage for sending back the episode
  knlohmann::json episode;

  // Storage to keep track of cumulative reward
  float trainingReward = 0.0;

  // Setting termination status of initial state (and the following ones) to non terminal.
  // The environment will change this at the last state, indicating whether the episode was
  // "Success" or "Truncated".
  agent["Termination"] = "Non Terminal";

  // Getting first state
  runEnvironment();

  // Saving experiences
  while (agent["Termination"] == "Non Terminal")
  {
    // Sanity checks for state
    for (size_t i = 0; i < _stateVectorSize; i++)
      if (std::isfinite(agent["State"][i].get<float>()) == false)
        KORALI_LOG_ERROR("Environment state variable %lu returned an invalid value: %f\n", agent["State"][i].get<float>());

    // Retrieve the current state
    auto state = agent["State"].get<std::vector<float>>();

    // Scale the state
    for (size_t d = 0; d < _stateVectorSize; ++d)
      state[d] = (state[d] - stateRescalingMeans[d]) / stateRescalingSdevs[d];

    // Re-storing state into agent
    agent["State"] = state;

    // Generating new action from policy
    auto beginTime0 = std::chrono::steady_clock::now(); // Profiling
    _agent->getAction(agent);
    auto endTime0 = std::chrono::steady_clock::now();                                                                 // Profiling
    agentPolicyEvaluationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime0 - beginTime0).count(); // Profiling

    // Sanity checks for action
    for (size_t i = 0; i < _actionVectorSize; i++)
      if (std::isfinite(agent["Action"][i].get<float>()) == false)
        KORALI_LOG_ERROR("Agent agent variable %lu returned an invalid value: %f\n", agent["Action"][i].get<float>());

    // Store the current state in the experience
    episode["Experiences"][actionCount]["State"] = state;

    // Storing the current action
    episode["Experiences"][actionCount]["Action"] = agent["Action"];

    // Storing the experience's policy
    episode["Experiences"][actionCount]["Policy"] = agent["Policy"];

    // Jumping back into the _agent's environment
    auto beginTime1 = std::chrono::steady_clock::now(); // Profiling
    runEnvironment();
    auto endTime1 = std::chrono::steady_clock::now();                                                            // Profiling
    agentComputationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime1 - beginTime1).count(); // Profiling

    // Storing features of the reward function
    episode["Experiences"][actionCount]["Features"] = agent["Features"];

    // Sanity checks for the features
    for (size_t i = 0; i < _featureVectorSize; i++)
      if (std::isfinite(agent["Features"][i].get<float>()) == false)
        KORALI_LOG_ERROR("Feature variable %lu returned an invalid value: %f\n", agent["Features"][i].get<float>());

    // Storing experience's reward
    episode["Experiences"][actionCount]["Reward"] = _agent->calculateReward(agent["Features"].get<std::vector<float>>());

    // Sanity checks for reward
    if (std::isfinite(episode["Experiences"][actionCount]["Reward"].get<float>()) == false)
      KORALI_LOG_ERROR("Calculated reward returned an invalid value: %f\n", episode[actionCount]["Reward"].get<float>());

    // Storing termination status
    episode["Experiences"][actionCount]["Termination"] = agent["Termination"];

    // If the episode was truncated, then save the terminal state
    if (agent["Termination"] == "Truncated")
    {
      episode["Experiences"][actionCount]["Truncated State"] = agent["State"];

      // Sanity checks for truncated state
      for (size_t i = 0; i < _stateVectorSize; i++)
        if (std::isfinite(agent["State"][i].get<float>()) == false)
          KORALI_LOG_ERROR("Environment truncatedstate variable %lu returned an invalid value: %f\n", agent["State"][i].get<float>());
    }

    // Adding to cumulative training reward
    trainingReward += episode["Experiences"][actionCount]["Reward"].get<float>();

    auto beginTime2 = std::chrono::steady_clock::now(); // Profiling

    // Checking if we requested the given number of actions in between policy updates and it is not a terminal state
    actionCount++;
    bool requestNewPolicy = (_actionsBetweenPolicyUpdates > 0) &&
                            (agent["Termination"] == "Non Terminal") &&
                            (actionCount % _actionsBetweenPolicyUpdates == 0);

    if (requestNewPolicy)
    {
      // Reserving message storage for requesting new policy
      knlohmann::json message;

      // Sending request to engine
      message["Sample Id"] = agent["Sample Id"];
      message["Action"] = "Request New Policy";
      KORALI_SEND_MSG_TO_ENGINE(message);

      // If requested new policy, wait for incoming message containing new hyperparameters
      agent["Policy Hyperparameters"] = KORALI_RECV_MSG_FROM_ENGINE();
      _agent->setAgentPolicy(agent["Policy Hyperparameters"]);
    }

    auto endTime2 = std::chrono::steady_clock::now();                                                              // Profiling
    agentCommunicationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime2 - beginTime2).count(); // Profiling
  }

  // Setting cumulative reward
  agent["Training Reward"] = trainingReward;

  // Finalizing Environment
  finalizeEnvironment();

  // Setting tested policy flag to false, unless we do testing
  agent["Tested Policy"] = false;

  // If the training reward exceeds the threshold, then also run testing on it
  if (trainingReward > _trainingRewardThreshold || _k->_currentGeneration % _testingFrequency == 0)
  {
    float averageTestingReward = 0.0;
    float stdevTestingReward = 0.0;
    float bestTestingReward = -Inf;
    float worstTestingReward = +Inf;

    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
      runTestingEpisode(agent);

      // Getting current testing reward
      float currentTestingReward = agent["Testing Reward"];

      // Adding current testing reward to the average and keeping statistics
      averageTestingReward += currentTestingReward;
      stdevTestingReward += currentTestingReward * currentTestingReward;
      if (currentTestingReward > bestTestingReward) bestTestingReward = currentTestingReward;
      if (currentTestingReward < worstTestingReward) worstTestingReward = currentTestingReward;
    }

    // Normalizing average
    averageTestingReward /= (float)_policyTestingEpisodes;
    stdevTestingReward = std::sqrt(stdevTestingReward / (float)_policyTestingEpisodes - averageTestingReward * averageTestingReward);

    // Storing testing information
    agent["Average Testing Reward"] = averageTestingReward;
    agent["Stdev Testing Reward"] = stdevTestingReward;
    agent["Best Testing Reward"] = bestTestingReward;
    agent["Worst Testing Reward"] = worstTestingReward;

    // Indicate that the agent has been tested
    agent["Tested Policy"] = true;
  }

  // Sending last experience last (after testing)
  // This is important to prevent the engine for block-waiting for the return of the sample
  // while the testing runs are being performed.
  knlohmann::json message;
  message["Action"] = "Send Episode";
  message["Sample Id"] = agent["Sample Id"];
  message["Experiences"] = episode["Experiences"];
  KORALI_SEND_MSG_TO_ENGINE(message);

  // Adding profiling information to agent
  agent["Computation Time"] = agentComputationTime;
  agent["Communication Time"] = agentCommunicationTime;
  agent["Policy Evaluation Time"] = agentPolicyEvaluationTime;
}

void ReinforcementLearning::runTestingEpisode(Sample &agent)
{
  float testingReward = 0.0;

  // Initializing Environment
  initializeEnvironment(agent);

  // Setting mode to testing to prevent the addition of noise or random actions
  agent["Mode"] = "Testing";

  // Setting initial non terminal state
  agent["Termination"] = "Non Terminal";

  // Getting first state
  runEnvironment();

  // Running environment using the last policy only
  while (agent["Termination"] == "Non Terminal")
  {
    _agent->getAction(agent);
    runEnvironment();

    auto features = agent["Features"];

    testingReward += _agent->calculateReward(agent["Features"].get<std::vector<float>>());
  }

  // Storing the cumulative reward of the testing episode
  agent["Testing Reward"] = testingReward;

  // Finalizing Environment
  finalizeEnvironment();
}

void ReinforcementLearning::initializeEnvironment(Sample &agent)
{
  // Getting RL-compatible solver
  _agent = dynamic_cast<solver::Agent *>(_k->_solver);

  // First, we update the initial policy's hyperparameters
  _agent->setAgentPolicy(agent["Policy Hyperparameters"]);

  // Then, we reset the state sequence for time-dependent learners
  _agent->resetTimeSequence();

  // Appending any user-defined settings
  agent["Custom Settings"] = _customSettings;

  // Creating agent coroutine
  __currentSample = &agent;
  __envFunctionId = _environmentFunction;
  agent._workerThread = co_active();

  // Creating coroutine
  _envThread = co_create(1 << 28, __environmentWrapper);
}

void ReinforcementLearning::finalizeEnvironment()
{
  // Freeing training co-routine memory
  co_delete(_envThread);
}

void ReinforcementLearning::runEnvironment()
{
  // Switching back to the environment's thread
  co_switch(_envThread);
}

} // namespace problem

} // namespace korali

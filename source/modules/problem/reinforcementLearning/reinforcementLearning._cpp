#include "engine.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace problem
{
void ReinforcementLearning::initialize()
{
  // Processing state/action variable configuration
  _stateVectorIndexes.clear();
  _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _actionVectorSize = _actionVectorIndexes.size();
  _stateVectorSize = _stateVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");

  if (_actionRepeat == 0) KORALI_LOG_ERROR("Action repeat cannot be zero.\n");
}

/**
 * @brief Pointer to the current sample, it is immediately copied as to avoid concurrency problems
 */
Sample *__currentSample;

/**
 * @brief Identifier of the current environment function Id.
 */
size_t __envFunctionId;

/**
 * @brief Thread wrapper to run an environment
 */
void __environmentWrapper()
{
  Sample *sample = __currentSample;

  sample->run(__envFunctionId);

  if ((*sample)["Termination"] == "Non Terminal")
   KORALI_LOG_ERROR("Environment function terminated, but sample termination status (success or truncated) was not set.\n");

  co_switch(sample->_workerThread);

  KORALI_LOG_ERROR("Resuming a finished sample\n");
}

void ReinforcementLearning::runEpisode(Sample &sample)
{
  /***************************************************************
   * Environment Configuration section
   ***************************************************************/

  // Getting RL-compatible solver
  auto _agent = dynamic_cast<solver::Agent *>(_k->_solver);

  // First, we update the initial policy's hyperparameters
  _agent->updateAgentPolicy(sample.globals()["Hyperparameters"]);

  // Creating sample coroutine
  __currentSample = &sample;
  __envFunctionId = _environmentFunction;
  sample._workerThread = co_active();
  auto envThread = co_create(1 << 28, __environmentWrapper);

  // Action Repeat Counter
  size_t actionLastUpdated = 0;

  // Counter for the total number of actions taken
  size_t actionCount = 0;

  // Storage for the action
  std::vector<float> action(_actionVectorSize);

  // Reserving storage for sending back experiences
  knlohmann::json experience;
  experience["Sample Id"] = sample["Sample Id"];

  // Storage to keep track of cumulative reward
  float cumulativeReward = 0.0;

 /***************************************************************
  * Environment Execution section
  ***************************************************************/

  // Setting termination status of initial state (and the following ones) to non terminal.
  // The environment will change this at the last state, indicating whether the episode was
  // "Success" or "Truncated".
  sample["Termination"] = "Non Terminal";

  // Getting first state
  co_switch(envThread);

  // Saving experiences
  while (sample["Termination"] == "Non Terminal")
  {
    // Requesting new action, if enough steps have passed, given by the action repeat parameter
    if (actionLastUpdated == 0)
    {
      // Getting new action
      _agent->getAction(sample);

      // Resetting action repeat counter
      actionLastUpdated = _actionRepeat;
      actionCount++;
    }
    actionLastUpdated--;

    // Storing the current state
    experience["State"] = sample["State"];

    // Storing the current action
    experience["Action"] = sample["Action"];

    // Storing the experience's policy
    experience["Policy"] = sample["Policy"];

    // Jumping back into the _agent's environment
    co_switch(envThread);

    // Storing experience's reward
    experience["Reward"] = sample["Reward"];

    // Storing termination status
    experience["Termination"] = sample["Termination"];

    // If the episode was truncated, then save the terminal state
    if (sample["Termination"] == "Truncated") experience["Truncated State"] = sample["State"];

    // Adding to cumulative reward
    cumulativeReward += sample["Reward"].get<float>();

    // If training, updating engine with the current experience
    if (sample["Mode"] == "Training")
    {
      // Checking if we requested the given number of actions in between policy updates
      bool requestNewPolicy = actionCount % _actionsBetweenPolicyUpdates == 0;

      experience["Request New Policy"] = false;
      if (requestNewPolicy) experience["Request New Policy"] = true;

      // Sending experience to engine
      KORALI_SEND_MSG_TO_ENGINE(experience);

      // If requested new policy, wait for incoming message containing new hyperparameters
      if (requestNewPolicy)
      {
        auto hyperparameters = KORALI_RECV_MSG_FROM_ENGINE();
        _agent->updateAgentPolicy(hyperparameters);
      }
    }
  }

  // Setting cumulative reward
  sample["Cumulative Reward"] = cumulativeReward;

  // Freeing co-routine memory
  co_delete(envThread);
}

} // namespace problem

} // namespace korali

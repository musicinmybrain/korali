#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"

namespace korali
{
namespace problem
{
void ReinforcementLearning::initialize()
{
  // Processing state/action variable configuration

 _stateVectorIndexes.clear();
 _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _actionVectorSize = _actionVectorIndexes.size();
  _stateVectorSize = _stateVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");
}

Sample* __currentSample;
size_t __envFunctionId;

void __environmentWrapper()
{
 Sample* sample = __currentSample;

 sample->run(__envFunctionId);

 (*sample)["Finished"] = true;

 co_switch(sample->_workerThread);

 KORALI_LOG_ERROR("Resuming a finished sample\n");
}

void ReinforcementLearning::runEnvironment(Sample &sample)
{
 // Getting RL-compatible solver
 auto agent = dynamic_cast<solver::Agent *>(_k->_solver);

 sample["Finished"] = false;

 // Creating sample coroutine
 __currentSample = &sample;
 __envFunctionId = _environmentFunction;
 sample._workerThread = co_active();
 auto envThread = co_create(1 << 28, __environmentWrapper);

 // Getting first state
 co_switch(envThread);

 // Saving experiences
 size_t actionId = 0;
 while (sample["Finished"] == false)
 {
  sample["Experience"]["States"][actionId] = sample["State"];
  agent->getAction(sample);
  sample["Experience"]["Actions"][actionId] = sample["Action"];

  // Jumping back into the agent's environment
  co_switch(envThread);

  // Returning from the agent's environment with the reward
  sample["Experience"]["Rewards"][actionId] = sample["Reward"];
  actionId++;
 }

 co_delete(envThread);
}

} // namespace problem

} // namespace korali

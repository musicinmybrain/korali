#include "engine.hpp"
#include "modules/problem/reinforcementLearning/reinforcementLearning.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"

namespace korali
{
namespace problem
{
void ReinforcementLearning::initialize()
{
  // Processing state/action variable configuration
  _stateVectorIndexes.clear();
  _actionVectorIndexes.clear();
  for (size_t i = 0; i < _k->_variables.size(); i++)
  {
    if (_k->_variables[i]->_type == "State") _stateVectorIndexes.push_back(i);
    if (_k->_variables[i]->_type == "Action") _actionVectorIndexes.push_back(i);
  }

  _actionVectorSize = _actionVectorIndexes.size();
  _stateVectorSize = _stateVectorIndexes.size();

  if (_actionVectorSize == 0) KORALI_LOG_ERROR("No action variables have been defined.\n");
  if (_stateVectorSize == 0) KORALI_LOG_ERROR("No state variables have been defined.\n");
}

/**
 * @brief Pointer to the current agent, it is immediately copied as to avoid concurrency problems
 */
Sample *__currentSample;

/**
 * @brief Identifier of the current environment function Id.
 */
size_t __envFunctionId;

/**
 * @brief Thread wrapper to run an environment
 */
void __environmentWrapper()
{
  Sample *agent = __currentSample;

  agent->run(__envFunctionId);

  if ((*agent)["Termination"] == "Non Terminal")
      KORALI_LOG_ERROR("Environment function terminated, but agent termination status (success or truncated) was not set.\n");

  bool terminationRecognized = false;
  if ((*agent)["Termination"] == "Terminal") terminationRecognized = true;
  if ((*agent)["Termination"] == "Truncated") terminationRecognized = true;

  if (terminationRecognized == false)
   KORALI_LOG_ERROR("Environment function terminated, but agent termination status (%s) is neither 'Terminal' nor 'Truncated'.\n", (*agent)["Termination"].get<std::string>().c_str());

  co_switch(agent->_workerThread);

  KORALI_LOG_ERROR("Resuming a finished agent\n");
}

void ReinforcementLearning::runEpisode(Sample &agent)
{
  /***************************************************************
   * Environment Configuration section
   ***************************************************************/

  // Getting RL-compatible solver
  auto _agent = dynamic_cast<solver::Agent *>(_k->_solver);

  // First, we update the initial policy's hyperparameters
  _agent->updateAgentPolicy(agent["Hyperparameters"]);

  // Creating agent coroutine
  __currentSample = &agent;
  __envFunctionId = _environmentFunction;
  agent._workerThread = co_active();
  auto envThread = co_create(1 << 28, __environmentWrapper);

  // Counter for the total number of actions taken
  size_t actionCount = 0;

  // Reserving storage for sending back experiences
  knlohmann::json experience;
  experience["Sample Id"] = agent["Sample Id"];

  // Storage to keep track of cumulative reward
  float trainingReward = 0.0;

  /***************************************************************
  * Environment Execution section
  ***************************************************************/

  // Setting termination status of initial state (and the following ones) to non terminal.
  // The environment will change this at the last state, indicating whether the episode was
  // "Success" or "Truncated".
  agent["Termination"] = "Non Terminal";

  // Using testing mode, where noise is added for exploration
  agent["Mode"] = "Training";

  // Getting first state
  co_switch(envThread);

  // Saving experiences
  while (agent["Termination"] == "Non Terminal")
  {
    // Generating new action from policy
    _agent->getAction(agent);
    actionCount++;

    // Storing the current state
    experience["State"] = agent["State"];

    // Storing the current action
    experience["Action"] = agent["Action"];

    // Storing the experience's policy
    experience["Policy"] = agent["Policy"];

    // Jumping back into the _agent's environment
    co_switch(envThread);

    // Storing experience's reward
    experience["Reward"] = agent["Reward"];

    // Storing termination status
    experience["Termination"] = agent["Termination"];

    // If the episode was truncated, then save the terminal state
    if (agent["Termination"] == "Truncated") experience["Truncated State"] = agent["State"];

    // Adding to cumulative trainingreward
    trainingReward += agent["Reward"].get<float>();

    // If training, updating engine with the current experience
    if (agent["Mode"] == "Training")
    {
      // Checking if we requested the given number of actions in between policy updates and it is not a terminal state
      bool requestNewPolicy = (agent["Termination"] == "Non Terminal") &&
                              (actionCount % _actionsBetweenPolicyUpdates == 0);

      experience["Request New Policy"] = requestNewPolicy;

      // Sending experience to engine
      KORALI_SEND_MSG_TO_ENGINE(experience);

      // If requested new policy, wait for incoming message containing new hyperparameters
      if (requestNewPolicy)
      {
        agent["Hyperparameters"] = KORALI_RECV_MSG_FROM_ENGINE();
        _agent->updateAgentPolicy(agent["Hyperparameters"]);
      }
    }
  }

  // Freeing training co-routine memory
  co_delete(envThread);

  // Setting cumulative reward
  agent["Training Reward"] = trainingReward;

  // Specifying whether the agent performed policy testing
  agent["Tested Policy"] = false;

  // If the training reward exceeded the threshold, then test the last policy
  if (trainingReward > _trainingRewardThreshold)
  {
    float averageTestingReward = 0.0;
    float bestTestingReward = -Inf;
    float worstTestingReward = +Inf;

    for (size_t i = 0; i < _policyTestingEpisodes; i++)
    {
     float currentTestingReward = 0.0;

     // Re-create environment
     envThread = co_create(1 << 28, __environmentWrapper);

     // Using testing mode, where no noise is given to the action
     agent["Mode"] = "Testing";

     // Setting initial non terminal state
     agent["Termination"] = "Non Terminal";

     // Getting first state
     co_switch(envThread);

     // Running environment using the last policy only
     while (agent["Termination"] == "Non Terminal")
     {
      _agent->getAction(agent);
      co_switch(envThread);
      currentTestingReward += agent["Reward"].get<float>();
     }

     // Adding current testing reward to the average and keeping statistics
     averageTestingReward += currentTestingReward;
     if (currentTestingReward > bestTestingReward) bestTestingReward = currentTestingReward;
     if (currentTestingReward < worstTestingReward) worstTestingReward = currentTestingReward;

     // Freeing training co-routine memory
     co_delete(envThread);
    }

    // Calculating average
    averageTestingReward = averageTestingReward / (float)_policyTestingEpisodes;

    // Storing testing information
    agent["Tested Policy"] = true;
    agent["Average Testing Reward"] = averageTestingReward;
    agent["Best Testing Reward"] = bestTestingReward;
    agent["Worst Testing Reward"] = worstTestingReward;
  }

}

} // namespace problem

} // namespace korali

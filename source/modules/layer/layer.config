{
 "Configuration Settings":
 [
   { 
    "Name": [ "Node Count" ],
    "Type": "size_t",
    "Description": "Indicates the node count of the current layer."
   },
   { 
    "Name": [ "Weights" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Indicates the weights to apply for the inner product operation. It must be of size KxN, where K is the number of nodes in the previous layer, and N is the number of nodes of the current layer."
   },
   { 
    "Name": [ "Bias" ],
    "Type": "std::vector<double>",
    "Description": "A collection of bias for each of the layer nodes."
   },
   { 
    "Name": [ "Activation Function", "Type" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Identity", "Description": "Forwards the output values values unaltered." },
                { "Value": "Linear", "Description": "Transforms (s) input element-wise to alpha*s." },
                { "Value": "Tanh", "Description": "Applies the element-wise tanh function." },
                { "Value": "ReLU", "Description": "Applies an element-wise rectifier linear unit function." },
                { "Value": "Logistic", "Description": "Applies an element-wise logistic (sigmoid) function." },
                { "Value": "Clip", "Description": "Clips the input (s) to be clipped into the alpha < s < beta range." },
                { "Value": "Log", "Description": "Applies the element-wise log function." }
               ],
    "Description": "Indicates the activation function for the weighted inputs to the current layer."
   },
   { 
    "Name": [ "Activation Function", "Alpha" ],
    "Type": "double",
    "Description": "First (alpha) argument to the activation function."
   },
   { 
    "Name": [ "Activation Function", "Beta" ],
    "Type": "double",
    "Description": "Second (beta) argument to the activation function."
   }
 ],
 
  "Internal Settings":
 [

 ],
 
 "Module Defaults":
 {
  "Node Count": 0,
  "Weights": [ [ ] ],
  "Bias": [ ],
  "Node Values": [ ],
  "Activation Function":
  {
   "Alpha": 0.0,
   "Beta": 0.0
  }
 }
}

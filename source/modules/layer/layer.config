{
 "Configuration Settings":
 [
   { 
    "Name": [ "Node Count" ],
    "Type": "size_t",
    "Description": "Indicates the node count of the current layer."
   },
   { 
    "Name": [ "Weights" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Indicates the weights to apply for the inner product operation. It must be of size KxN, where K is the number of nodes in the previous layer, and N is the number of nodes of the current layer."
   },
   { 
    "Name": [ "Bias" ],
    "Type": "std::vector<double>",
    "Description": "A collection of bias for each of the layer nodes."
   },
   { 
    "Name": [ "Batch Normalization", "Shift" ],
    "Type": "std::vector<double>",
    "Description": "The value by which the node values are shifted during normalization."
   },
   { 
    "Name": [ "Batch Normalization", "Scale" ],
    "Type": "std::vector<double>",
    "Description": "The value by which the node values are scaled during normalization."
   },
   { 
    "Name": [ "Activation Function", "Type" ],
    "Type": "std::string",
    "Options": [
                { "Value": "Identity", "Description": "Forwards the output values values unaltered." },
                { "Value": "Linear", "Description": "Transforms (s) input element-wise to alpha*s." },
                { "Value": "Tanh", "Description": "Applies the element-wise tanh function." },
                { "Value": "ReLU", "Description": "Applies an element-wise rectifier linear unit function." },
                { "Value": "Logistic", "Description": "Applies an element-wise logistic (sigmoid) function." },
                { "Value": "Clip", "Description": "Clips the input (s) to be clipped into the alpha < s < beta range." },
                { "Value": "Log", "Description": "Applies the element-wise log function." }
               ],
    "Description": "Indicates the activation function for the weighted inputs to the current layer."
   },
   { 
    "Name": [ "Activation Function", "Alpha" ],
    "Type": "double",
    "Description": "First (alpha) argument to the activation function."
   },
   { 
    "Name": [ "Activation Function", "Beta" ],
    "Type": "double",
    "Description": "Second (beta) argument to the activation function."
   }
 ],
 
  "Internal Settings":
 [
   { 
    "Name": [ "Weight Gradient" ],
    "Type": "std::vector<std::vector<double>>",
    "Description": "Stores the gradient of the inner product (Wx+b) operation with respect to W."
   },
   { 
    "Name": [ "Bias Gradient" ],
    "Type": "std::vector<double>",
    "Description": "Stores the gradient of the inner product (Wx+b) operation with respect to b."
   },
   { 
    "Name": [ "Batch Normalization", "Scale Gradient" ],
    "Type": "std::vector<double>",
    "Description": "Stores the gradient of the normalization operation with respect to scale."
   },
   { 
    "Name": [ "Batch Normalization", "Shift Gradient" ],
    "Type": "std::vector<double>",
    "Description": "Stores the gradient of the normalization operation with respect to shift."
   },
   { 
    "Name": [ "Batch Normalization", "Means" ],
    "Type": "std::vector<double>",
    "Description": "If given, the each node in the layer will be normalized to the specified mean. Otherwise, this value will be given by the normalization operation."
   },
   { 
    "Name": [ "Batch Normalization", "Variances" ],
    "Type": "std::vector<double>",
    "Description": "If given, the each node in the layer will be normalized to the specified mean. Otherwise, this value will be given by the normalization operation."
   }
 ],
 
 "Module Defaults":
 {
  "Node Count": 0,
  "Weights": [ [ ] ],
  "Bias": [ ],
  "Activation Function":
  {
   "Alpha": 0.0,
   "Beta": 0.0
  },
  
  "Batch Normalization":
  {
   "Shift": [],
   "Scale": []
  }
 }
}

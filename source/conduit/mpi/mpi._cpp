#include "conduit/mpi/mpi.hpp"
#include "engine/engine.hpp"
#include "problems.hpp"
#include "solvers.hpp"


#ifdef _KORALI_USE_MPI

#define MPI_TAG_MODELID 1
#define MPI_TAG_SAMPLEID 2
#define MPI_TAG_SAMPLESIZE 2
#define MPI_TAG_SAMPLEDATA 3
#define MPI_TAG_RESULTSIZE 4
#define MPI_TAG_RESULTDATA 5

MPI_Comm __KoraliTeamComm;
MPI_Comm getKoraliMPIComm() { return __KoraliTeamComm; }
long int getKoraliMPICommPointer() { return (long int)(&__KoraliTeamComm); }
#endif

void korali::conduit::MPI::initialize()
{
 #ifdef _KORALI_USE_MPI
 _rankCount = 1;
 _rankId = 0;

 int isInitialized;
 MPI_Initialized(&isInitialized);
 if (isInitialized == false)  MPI_Init(nullptr, nullptr);

 MPI_Comm_size(MPI_COMM_WORLD, &_rankCount);
 MPI_Comm_rank(MPI_COMM_WORLD, &_rankId);
 #endif

 #ifndef _KORALI_USE_MPI
  korali::logError("Running an MPI-based Korali application, but Korali was installed without support for MPI.\n");
 #endif

 #ifdef _KORALI_USE_MPI
 _continueEvaluations = true;

 if (_rankCount == 1) korali::logError("Korali MPI applications require at least 2 MPI ranks to run.\n");

 _teamCount = (_rankCount-1) / _ranksPerTeam;
 _teamId = -1;
 _localRankId = -1;

 int currentRank = 0;
 for (int i = 0; i < _teamCount; i++)
 {
  _teamQueue.push(i);
  for (int j = 0; j < _ranksPerTeam; j++)
  {
   if (currentRank == _rankId)
   {
    _teamId = i;
    _localRankId = j;
   }
   _teamWorkers[i].push_back(currentRank++);
  }
 }

 MPI_Comm_split(MPI_COMM_WORLD, _teamId, _rankId, &__KoraliTeamComm);

 int mpiSize = -1;
 MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);

 if(isRoot()) if (_rankCount < _ranksPerTeam + 1)
 korali::logError("You are running Korali with %d ranks. However, you need at least %d ranks to have at least one worker team. \n", _rankCount, _ranksPerTeam + 1 );

 MPI_Barrier(MPI_COMM_WORLD);

 if (!isRoot()) workerThread();
 #endif
}


void korali::conduit::MPI::finalize()
{
 #ifdef _KORALI_USE_MPI
 if (isRoot())
  {
  int stopFlag = -1;
  for (int i = 0; i < _teamCount; i++)
   for (int j = 0; j < _ranksPerTeam; j++)
    MPI_Send(&stopFlag, 1, MPI_INT, _teamWorkers[i][j], MPI_TAG_MODELID, MPI_COMM_WORLD);
 }
 #endif
}

void korali::conduit::MPI::workerThread()
{
 #ifdef _KORALI_USE_MPI
 if (_teamId == -1) return;

 int modelId = 0;
 while (modelId != -1)
 {
   MPI_Recv(&modelId, 1, MPI_LONG, getRootRank(), MPI_TAG_MODELID, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

   if (modelId != -1)
   {
    size_t sampleId;
    size_t sampleSize;

    MPI_Recv(&sampleId, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_TAG_SAMPLEID, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(&sampleSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_TAG_SAMPLESIZE, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    std::vector<double> sampleData(sampleSize);

    MPI_Recv(sampleData.data(), sampleSize, MPI_DOUBLE, getRootRank(), MPI_TAG_SAMPLEDATA, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

    auto sample = korali::Sample(sampleData);
    sample.setSampleId(sampleId);
    sample._k = _k;
    _k->_models[modelId](sample);

    if (_localRankId == 0)
    {
     size_t resultSize = sample.getResult().size();
     MPI_Send(&resultSize, 1, MPI_UNSIGNED_LONG, getRootRank(), MPI_TAG_RESULTSIZE, MPI_COMM_WORLD);
     MPI_Send(sample.getResult().data(), resultSize, MPI_DOUBLE, getRootRank(), MPI_TAG_RESULTDATA, MPI_COMM_WORLD);
    }

    MPI_Barrier(__KoraliTeamComm);
   }
 }
 #endif
}

void korali::conduit::MPI::runModel(size_t modelId, korali::Sample& sample)
{
 _modelEvaluationCount++;

 #ifdef _KORALI_USE_MPI
 while (_teamQueue.empty()) sample.yield();

 int teamId = _teamQueue.front(); _teamQueue.pop();

 for (int i = 0; i < _ranksPerTeam; i++)
 {
  size_t sampleSize = sample.getSampleData().size();
  size_t sampleId = sample.getSampleId();
  int workerId = _teamWorkers[teamId][i];

  MPI_Send(&modelId, 1, MPI_LONG, workerId, MPI_TAG_MODELID, MPI_COMM_WORLD);
  MPI_Send(&sampleId, 1, MPI_UNSIGNED_LONG, workerId, MPI_TAG_SAMPLEID, MPI_COMM_WORLD);
  MPI_Send(&sampleSize, 1, MPI_UNSIGNED_LONG, workerId, MPI_TAG_SAMPLESIZE, MPI_COMM_WORLD);
  MPI_Send(sample.getSampleData().data(), sampleSize, MPI_DOUBLE, workerId, MPI_TAG_SAMPLEDATA, MPI_COMM_WORLD);
 }

 size_t resultSize;
 MPI_Request resultRequest;
 MPI_Irecv(&resultSize, 1, MPI_UNSIGNED_LONG, _teamWorkers[teamId][0], MPI_TAG_RESULTSIZE, MPI_COMM_WORLD, &resultRequest);

 int flag = 0;
 while(flag == 0)
 {
  MPI_Test(&resultRequest, &flag, MPI_STATUS_IGNORE);
  if (flag)
  {
    std::vector<double> resultVector(resultSize);
    MPI_Recv(resultVector.data(), resultSize, MPI_DOUBLE, _teamWorkers[teamId][0], MPI_TAG_RESULTDATA, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    sample.setResult(resultVector);
    _teamQueue.push(teamId);
  }
  else sample.yield();
 }

 #endif
}

int korali::conduit::MPI::getRootRank()
{
 #ifdef _KORALI_USE_MPI
 return _rankCount-1;
 #endif

 return 0;
}

bool korali::conduit::MPI::isRoot()
{
 #ifdef _KORALI_USE_MPI
 return _rankId == getRootRank();
 #endif

 return true;
}

void korali::conduit::MPI::abort()
{
 #ifdef _KORALI_USE_MPI
 MPI_Abort(MPI_COMM_WORLD, -1);
 #endif
}
